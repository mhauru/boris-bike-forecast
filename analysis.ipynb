{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other analyses of the same data:\n",
    "\n",
    "https://github.com/charlie1347/TfL_bikes\n",
    "\n",
    "https://medium.com/@AJOhrn/data-footprint-of-bike-sharing-in-london-be9e11425248"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import statsmodels.api as sm\n",
    "from sklearn import linear_model, svm, neighbors, naive_bayes, tree\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from IPython.display import set_matplotlib_formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For pretty and exportable matplotlib plots.\n",
    "# If you are running this yourself and want interactivity,\n",
    "# try `%matplotlib widget` instead.\n",
    "# set_matplotlib_formats(\"svg\")\n",
    "# %matplotlib inline\n",
    "%matplotlib widget\n",
    "# Set a consistent plotting style across the notebook using Seaborn.\n",
    "sns.set_style(\"darkgrid\")\n",
    "sns.set_context(\"notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing and cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikefolder = \"./data/bikes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_station_names(station_names, df, namecolumn, idcolumn):\n",
    "    namemaps = (\n",
    "        df[[idcolumn, namecolumn]]\n",
    "        .groupby(idcolumn)\n",
    "        .aggregate(lambda x: x.unique())\n",
    "    )\n",
    "    for number, names in namemaps.iterrows():\n",
    "        current_names = station_names.get(number, set())\n",
    "        # The following two lines are a stupid dance around the annoying fact that pd.unique sometimes returns a single value,\n",
    "        # sometimes a numpy array of values, but since the single value is a string, it too is an iterable.\n",
    "        vals = names[0]\n",
    "        new_names = set([vals]) if type(vals) == str else set(vals)\n",
    "        current_names.update(new_names)\n",
    "        station_names[number] = current_names\n",
    "\n",
    "\n",
    "def clean_datetime_column(df, colname, roundto=\"H\"):\n",
    "    # A bit of a hacky way to use the first entry to figure out which date format this file uses.\n",
    "    # Not super robust, but works. TODO Improve this.\n",
    "    if len(df[colname].iloc[0]) > 16:\n",
    "        format = \"%d/%m/%Y %H:%M:%S\"\n",
    "    else:\n",
    "        format = \"%d/%m/%Y %H:%M\"\n",
    "    df[colname] = pd.to_datetime(df[colname], format=format)\n",
    "    df[colname] = df[colname].dt.round(roundto)\n",
    "    early_cutoff = pd.datetime(2010, 7, 30)  # When the program started.\n",
    "    late_cutoff = pd.datetime(2020, 1, 1)  # Approximately now.\n",
    "    df = df[(late_cutoff > df[colname]) & (df[colname] >= early_cutoff)]\n",
    "    return df\n",
    "\n",
    "\n",
    "def compute_single_events(df, which):\n",
    "    stationcol = \"{}Station Id\".format(which)\n",
    "    datecol = \"{} Date\".format(which)\n",
    "    events = (\n",
    "        df.rename(columns={stationcol: \"Station\", datecol: \"Date\"})\n",
    "        .groupby([\"Date\", \"Station\"])\n",
    "        .size()\n",
    "        .unstack(\"Station\")\n",
    "    )\n",
    "    return events\n",
    "\n",
    "\n",
    "def compute_both_events(df):\n",
    "    ends = compute_single_events(df, \"End\")\n",
    "    starts = compute_single_events(df, \"Start\")\n",
    "    both = (\n",
    "        pd.concat([ends, starts], keys=[\"End\", \"Start\"], axis=1)\n",
    "        .reorder_levels([1, 0], axis=1)\n",
    "        .fillna(0.0)\n",
    "    )\n",
    "    return both\n",
    "\n",
    "\n",
    "def castable_to_int(obj):\n",
    "    try:\n",
    "        int(obj)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "\n",
    "def cast_to_int(df, colname):\n",
    "    try:\n",
    "        df = df.astype({colname: np.int_}, copy=False)\n",
    "    except ValueError:\n",
    "        castable_rows = df[colname].apply(castable_to_int)\n",
    "        df = df[castable_rows]\n",
    "        df = df.astype({colname: np.int_}, copy=False)\n",
    "    return df\n",
    "\n",
    "\n",
    "events_by_station_path = Path(\"./events_by_station.p\")\n",
    "if events_by_station_path.exists():\n",
    "    events_by_station = pd.read_pickle(events_by_station_path)\n",
    "else:\n",
    "    datafiles = sorted(os.listdir(bikefolder))\n",
    "    folderpath = Path(bikefolder)\n",
    "    datapaths = [folderpath / Path(file) for file in datafiles]\n",
    "    datapaths = [p for p in datapaths if p.suffix == \".csv\"]\n",
    "\n",
    "    station_allnames = {}\n",
    "\n",
    "    pieces = []\n",
    "    #     datapaths = [\n",
    "    #         folderpath / Path(file)\n",
    "    #         for file in [\n",
    "    #             \"21JourneyDataExtract31Aug2016-06Sep2016.csv\",\n",
    "    #             \"15JourneyDataExtract20Jul2016-26Jul2016.csv\",\n",
    "    #             \"13b. Journey Data Extract 22Dec14-03Jan15.csv\",\n",
    "    #             \"16JourneyDataExtract27Jul2016-02Aug2016.csv\",\n",
    "    #             \"10b. Journey Data Extract 28Sep14-11Oct14.csv\",\n",
    "    #             \"6. Journey Data Extract_27May-23Jun12.csv\",\n",
    "    #             \"6. Journey Data Extract_27May-23Jun12.csv\",\n",
    "    #         ]\n",
    "    #     ]\n",
    "    cols = [\n",
    "        \"Duration\",\n",
    "        \"End Date\",\n",
    "        \"EndStation Id\",\n",
    "        \"EndStation Name\",\n",
    "        \"Start Date\",\n",
    "        \"StartStation Id\",\n",
    "        \"StartStation Name\",\n",
    "    ]\n",
    "    problem_paths = []\n",
    "    for path in datapaths:\n",
    "        print(path)\n",
    "        try:\n",
    "            df = pd.read_csv(path, usecols=cols, encoding=\"ISO-8859-2\")\n",
    "        except ValueError as e:\n",
    "            # Some files have missing or abnormaly named columns. We'll deal with them later.\n",
    "            problem_paths.append(path)\n",
    "            continue\n",
    "        # Drop any rows that have missing values.\n",
    "        df = df[~df.isna().any(axis=1)]\n",
    "        # Drop any anomalously short trips. Probably somebody just taking a bike and putting\n",
    "        # it right back in.\n",
    "        df = df[df[\"Duration\"] > 60]\n",
    "        # Cast the columns to the right types. This is easier ones NAs have been dropped.\n",
    "        df = cast_to_int(df, \"EndStation Id\")\n",
    "        df = cast_to_int(df, \"StartStation Id\")\n",
    "        # Turn the date columns from strings into datetime objects rounded to the hour.\n",
    "        df = clean_datetime_column(df, \"End Date\")\n",
    "        df = clean_datetime_column(df, \"Start Date\")\n",
    "        events = compute_both_events(df)\n",
    "        pieces.append(events)\n",
    "\n",
    "        add_station_names(\n",
    "            station_allnames, df, \"EndStation Name\", \"EndStation Id\"\n",
    "        )\n",
    "        add_station_names(\n",
    "            station_allnames, df, \"StartStation Name\", \"StartStation Id\"\n",
    "        )\n",
    "\n",
    "    station_ids = {}\n",
    "    station_names = {}\n",
    "    for k, v in station_allnames.items():\n",
    "        v = sorted(v)\n",
    "        station_names[k] = v[0]\n",
    "        for name in v:\n",
    "            station_ids[name] = k\n",
    "\n",
    "    def get_station_id(name):\n",
    "        try:\n",
    "            return station_ids[name]\n",
    "        except KeyError:\n",
    "            return np.nan\n",
    "\n",
    "    print(\"Doing the problem cases ({} of them).\".format(len(problem_paths)))\n",
    "    safe_cols = [\n",
    "        \"Duration\",\n",
    "        \"End Date\",\n",
    "        \"EndStation Name\",\n",
    "        \"Start Date\",\n",
    "        \"StartStation Name\",\n",
    "    ]\n",
    "    for path in problem_paths:\n",
    "        print(path)\n",
    "        df = pd.read_csv(path, usecols=safe_cols, encoding=\"ISO-8859-2\")\n",
    "        # Drop any rows that have missing values.\n",
    "        df = df[~df.isna().any(axis=1)]\n",
    "        # Drop any anomalously short trips. Probably somebody just taking a bike and putting\n",
    "        # it right back in.\n",
    "        df = df[df[\"Duration\"] > 60]\n",
    "        # Add a column of station ids, based on names.\n",
    "        df[\"EndStation Id\"] = df[\"EndStation Name\"].apply(get_station_id)\n",
    "        df[\"StartStation Id\"] = df[\"StartStation Name\"].apply(get_station_id)\n",
    "        # Turn the date columns from strings into datetime objects rounded to the hour.\n",
    "        clean_datetime_column(df, \"End Date\")\n",
    "        clean_datetime_column(df, \"Start Date\")\n",
    "        events = compute_both_events(df)\n",
    "        pieces.append(events)\n",
    "\n",
    "    events_by_station = pd.concat(pieces).fillna(0.0)\n",
    "    # Several files may have contained entries for the same hour, which means that\n",
    "    # events_by_station has duplicate entries in the index. Get rid of them by summing.\n",
    "    events_by_station = events_by_station.groupby(\"Date\").sum().sort_index()\n",
    "    # Finally rename the columns according to the chosen names for stations.\n",
    "    events_by_station = events_by_station.rename(\n",
    "        mapper=station_names, axis=1, level=0\n",
    "    )\n",
    "\n",
    "    events_by_station.to_pickle(events_by_station_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Station\n",
       "Electrical Workshop PS                                      1757.661290\n",
       "PENTON STREET COMMS TEST TERMINAL _ CONTACT MATT McNULTY    1615.675676\n",
       "tabletop1                                                   1066.166667\n",
       "Contact Centre, Southbury House                              780.694737\n",
       "6                                                            567.974684\n",
       "Pop Up Dock 1                                                567.806009\n",
       "Mechanical Workshop Penton                                   223.818810\n",
       "South Quay East, Canary Wharf                                133.732653\n",
       "Westfield Eastern Access Road, Shepherd's Bush                88.865062\n",
       "Thornfield House, Poplar                                      83.572294\n",
       "Fore Street Avenue: Guildhall                                 72.241404\n",
       "Upper Grosvenor Street, Mayfair                               71.496163\n",
       "Courland Grove , Wandsworth Road                              57.658476\n",
       "Manfred Road, East Putney                                     54.785485\n",
       "Aberfeldy Street, Poplar                                      54.169256\n",
       "Grant Road Central, Clapham Junction                          53.220326\n",
       "Castalia Square :Cubitt Town                                  53.208849\n",
       "Stebondale Street, Cubitt Town                                53.143344\n",
       "Here East South, Queen Elizabeth Olympic Park                 49.890323\n",
       "Teviot Street, Poplar                                         47.461839\n",
       "Stewart's Road, Nine Elms                                     47.116626\n",
       "Abbotsbury Road, Holland Park                                 46.465510\n",
       "Clarkson Street :Bethnal Green                                46.455175\n",
       "Malmesbury Road :Bow                                          45.637274\n",
       "Cantrell Road, Bow                                            45.279041\n",
       "Here East North, Queen Elizabeth Olympic Park                 45.240314\n",
       "Bevington Road, North Kensington                              45.214481\n",
       "Lord's, St. John's Wood                                       44.309497\n",
       "Birkenhead Street, King's Cross                               44.122969\n",
       "Knightsbridge, Hyde Park                                      43.761308\n",
       "dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mean_within_window(s):\n",
    "    starttime = s.ne(0).idxmax()\n",
    "    endtime = s[::-1].ne(0).idxmax()\n",
    "    return s[starttime:endtime].mean()\n",
    "\n",
    "\n",
    "a = events_by_station.sum(axis=1, level=0)\n",
    "(a.max() / a.aggregate(mean_within_window)).sort_values(ascending=False)[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "improper_stations = [\n",
    "    \"Electrical Workshop PS\",\n",
    "    \"PENTON STREET COMMS TEST TERMINAL _ CONTACT MATT McNULTY\",\n",
    "    \"tabletop1\",\n",
    "    \"Pop Up Dock 1\",\n",
    "    \"6\",\n",
    "    \"Mechanical Workshop Penton\",\n",
    "]\n",
    "events_by_station = events_by_station.drop(improper_stations, axis=1, level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>End</th>\n",
       "      <th>Start</th>\n",
       "      <th>End</th>\n",
       "      <th>Start</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012-01-04 00:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-04 01:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-04 02:00:00</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-04 03:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-04 04:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-16 22:00:00</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-16 23:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-17 00:00:00</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-17 01:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-17 02:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>46879 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     End  Start  End  Start\n",
       "Date                                       \n",
       "2012-01-04 00:00:00  0.0    0.0  0.0    0.0\n",
       "2012-01-04 01:00:00  0.0    0.0  0.0    0.0\n",
       "2012-01-04 02:00:00  2.0    0.0  0.0    0.0\n",
       "2012-01-04 03:00:00  0.0    0.0  0.0    0.0\n",
       "2012-01-04 04:00:00  0.0    0.0  0.0    0.0\n",
       "...                  ...    ...  ...    ...\n",
       "2017-05-16 22:00:00  1.0    1.0  0.0    0.0\n",
       "2017-05-16 23:00:00  0.0    0.0  0.0    0.0\n",
       "2017-05-17 00:00:00  1.0    0.0  0.0    0.0\n",
       "2017-05-17 01:00:00  0.0    0.0  0.0    0.0\n",
       "2017-05-17 02:00:00  0.0    0.0  0.0    0.0\n",
       "\n",
       "[46879 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO What is up with this?\n",
    "events_by_station[\"Exhibition Road Museums, Knightsbridge\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations = events_by_station.columns.get_level_values(0).unique()\n",
    "events_by_time = events_by_station.sum(axis=1, level=1)\n",
    "totals_by_station = events_by_station.sum(axis=0)\n",
    "times = events_by_station.index.to_series()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_stations = [\n",
    "    \"Waterloo Station 3, Waterloo\",\n",
    "    \"Hyde Park Corner, Hyde Park\",\n",
    "    \"Wenlock Road , Hoxton\",\n",
    "    \"Stonecutter Street, Holborn\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dada05a7f800487ab8da85d16616a039",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO Give the plots widths from variance or something like [25%, 75%] limits (what are these called again?).\n",
    "example_means_over_week = (\n",
    "    events_by_station[test_stations]\n",
    "    .groupby([times.dt.weekday, times.dt.hour])\n",
    "    .sum()\n",
    ")\n",
    "example_means_over_week.index.rename([\"Day\", \"Hour\"], inplace=True)\n",
    "example_means_over_week = (\n",
    "    example_means_over_week.stack(level=[0, 1])\n",
    "    .reset_index()\n",
    "    .rename(columns={\"level_3\": \"End/Start\", 0: \"Count\"})\n",
    ")\n",
    "example_means_over_week[\"Weekday\"] = example_means_over_week.apply(\n",
    "    lambda x: x[\"Day\"] + x[\"Hour\"] / 24, axis=1,\n",
    ")\n",
    "g = sns.FacetGrid(\n",
    "    example_means_over_week,\n",
    "    col_wrap=2,\n",
    "    col=\"Station\",\n",
    "    hue=\"End/Start\",\n",
    "    sharey=False,\n",
    "    sharex=True,\n",
    ")\n",
    "g.map(plt.plot, \"Weekday\", \"Count\").set_titles(\"{col_name}\")\n",
    "g.add_legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd259d320aee45b7a4e5bad340f75520",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO Give the plots widths from variance or something like [25%, 75%] limits (what are these called again?).\n",
    "example_means_over_year = (\n",
    "    events_by_station[test_stations].groupby(times.dt.week).sum()\n",
    ")\n",
    "# Leave out the first and last weeks, since they are usually shorter and thus the data isn't comparable.\n",
    "example_means_over_year = example_means_over_year.iloc[1:51]\n",
    "example_means_over_year.index.rename(\"Week\", inplace=True)\n",
    "example_means_over_year = (\n",
    "    example_means_over_year.stack(level=[0, 1])\n",
    "    .reset_index()\n",
    "    .rename(columns={\"level_2\": \"End/Start\", 0: \"Count\"})\n",
    ")\n",
    "g = sns.FacetGrid(\n",
    "    example_means_over_year,\n",
    "    col_wrap=2,\n",
    "    col=\"Station\",\n",
    "    hue=\"End/Start\",\n",
    "    sharey=False,\n",
    "    sharex=True,\n",
    ")\n",
    "g.map(plt.plot, \"Week\", \"Count\").set_titles(\"{col_name}\")\n",
    "g.add_legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08a39f0d195a41d7a0ab453dc229a0d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/markus/.local/lib/python3.7/site-packages/pandas/plotting/_matplotlib/converter.py:103: FutureWarning: Using an implicitly registered datetime converter for a matplotlib plotting method. The converter was registered by pandas on import. Future versions of pandas will require you to explicitly register matplotlib converters.\n",
      "\n",
      "To register the converters:\n",
      "\t>>> from pandas.plotting import register_matplotlib_converters\n",
      "\t>>> register_matplotlib_converters()\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "yearly_rolling = (\n",
    "    events_by_station.loc[:, test_stations]\n",
    "    .rolling(\"365d\", min_periods=24 * 90)\n",
    "    .mean()\n",
    ")\n",
    "yearly_rolling = (\n",
    "    yearly_rolling.stack(level=[0, 1])\n",
    "    .reset_index()\n",
    "    .rename(columns={\"level_2\": \"End/Start\", 0: \"Count\"})\n",
    ")\n",
    "g = sns.FacetGrid(\n",
    "    yearly_rolling,\n",
    "    col_wrap=2,\n",
    "    col=\"Station\",\n",
    "    hue=\"End/Start\",\n",
    "    sharey=False,\n",
    "    sharex=True,\n",
    ")\n",
    "g.map(plt.plot, \"Date\", \"Count\").set_titles(\"{col_name}\")\n",
    "g.add_legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the extremely strong weekly pattern every station seems to have, one we thing we do straight away is instead of trying to predict the data itself, try to predict the change compared to a week ago. This makes fitting models much easier since they don't have to concentrate on trying to get the shape of the weekly trend right. The data also naturally becomes roughly mean-zero, which is useful for some models. Moreover, the long term trends, which may be entirely unpredictable given the data we have, don't ruin our results once we try to predict week-on-week changes. The only downside is that we have to discard the first week of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RollingValidatorSingleStation:\n",
    "    def __init__(\n",
    "        self, data, predictors, min_training_time, prediction_time,\n",
    "    ):\n",
    "        cv_batches = []\n",
    "        first_time = data.index.min()\n",
    "        last_time = data.index.max()\n",
    "        test_end_time = last_time\n",
    "        cutoff = test_end_time - prediction_time\n",
    "        while cutoff > first_time + min_training_time:\n",
    "            training_data = data[:cutoff]\n",
    "            training_predictors = predictors[:cutoff]\n",
    "            test_data = data[cutoff:test_end_time]\n",
    "            test_predictors = predictors[cutoff:test_end_time]\n",
    "            cv_batches.append(\n",
    "                (\n",
    "                    training_data,\n",
    "                    training_predictors,\n",
    "                    test_data,\n",
    "                    test_predictors,\n",
    "                )\n",
    "            )\n",
    "            test_end_time = cutoff\n",
    "            cutoff = test_end_time - prediction_time\n",
    "        self.cv_batches = cv_batches\n",
    "        self.models = {}\n",
    "        print(\n",
    "            \"Created a RollingValidator with {} cross-validation batches.\".format(\n",
    "                len(cv_batches)\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def test_modelclass(self, modelclass, print_progress=False):\n",
    "        test_errors = []\n",
    "        training_errors = []\n",
    "        test_predictions = []\n",
    "        training_predictions = []\n",
    "        for (i, cv_batch,) in enumerate(self.cv_batches):\n",
    "            (\n",
    "                training_data,\n",
    "                training_predictors,\n",
    "                test_data,\n",
    "                test_predictors,\n",
    "            ) = cv_batch\n",
    "            if print_progress:\n",
    "                print(\"Training for batch {}.\".format(i))\n",
    "            model = modelclass()\n",
    "            model.train(\n",
    "                training_data, training_predictors,\n",
    "            )\n",
    "            if print_progress:\n",
    "                print(\"Predicting for batch {}.\".format(i))\n",
    "            test_prediction = model.predict(test_predictors)\n",
    "            training_prediction = model.predict(training_predictors)\n",
    "            test_error = test_prediction - test_data\n",
    "            training_error = training_prediction - training_data\n",
    "            test_errors.append(test_error)\n",
    "            training_errors.append(training_error)\n",
    "            test_predictions.append(test_prediction)\n",
    "            training_predictions.append(training_prediction)\n",
    "        test_mae = pd.concat(test_errors).abs().mean()\n",
    "        training_mae = pd.concat(training_errors).abs().mean()\n",
    "        self.models[modelclass.classname] = {\n",
    "            \"test_mae\": test_mae,\n",
    "            \"training_mae\": training_mae,\n",
    "            \"test_errors\": test_errors,\n",
    "            \"training_errors\": training_errors,\n",
    "            \"test_predictions\": test_predictions,\n",
    "            \"training_predictions\": training_predictions,\n",
    "        }\n",
    "        return test_mae.sum()\n",
    "\n",
    "\n",
    "class RollingValidator:\n",
    "    def __init__(\n",
    "        self,\n",
    "        data,\n",
    "        common_predictors,\n",
    "        specific_predictors,\n",
    "        min_training_time,\n",
    "        prediction_time,\n",
    "    ):\n",
    "        self.models = {}\n",
    "        self.subrvs = {}\n",
    "        for c in data.columns:\n",
    "            data_c = pd.DataFrame(data[c])\n",
    "            if specific_predictors is not None and c in specific_predictors:\n",
    "                predictors_c = pd.concat(\n",
    "                    [common_predictors, specific_predictors[c]], axis=1\n",
    "                )\n",
    "            else:\n",
    "                predictors_c = common_predictors\n",
    "            subrv = RollingValidatorSingleStation(\n",
    "                data_c, predictors_c, min_training_time, prediction_time\n",
    "            )\n",
    "            self.subrvs[c] = subrv\n",
    "\n",
    "    def test_modelclass(self, modelclass, print_progress=False):\n",
    "        for c, subrv in self.subrvs.items():\n",
    "            if print_progress:\n",
    "                print(\"Running RV on {}.\".format(c))\n",
    "            subrv.test_modelclass(modelclass, print_progress=print_progress)\n",
    "        classname = modelclass.classname\n",
    "        self.models[classname] = {}\n",
    "        for k in (\"test_mae\", \"training_mae\"):\n",
    "            self.models[classname][k] = pd.concat(\n",
    "                [subrv.models[classname][k] for subrv in self.subrvs.values()]\n",
    "            )\n",
    "        num_batches = len(next(iter(self.subrvs.values())).cv_batches)\n",
    "        for k in (\n",
    "            \"test_errors\",\n",
    "            \"training_errors\",\n",
    "            \"test_predictions\",\n",
    "            \"training_predictions\",\n",
    "        ):\n",
    "            self.models[classname][k] = [\n",
    "                pd.concat(\n",
    "                    [\n",
    "                        subrv.models[classname][k][i]\n",
    "                        for subrv in self.subrvs.values()\n",
    "                    ],\n",
    "                    axis=1,\n",
    "                )\n",
    "                for i in range(num_batches)\n",
    "            ]\n",
    "        test_mae = self.models[classname][\"test_mae\"].sum()\n",
    "        return test_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekday_dummies = pd.get_dummies(times.dt.weekday_name)\n",
    "week_dummies = pd.get_dummies(times.dt.week)\n",
    "hour_dummies = pd.get_dummies(times.dt.hour)\n",
    "hour_dummies = hour_dummies.rename(\n",
    "    columns={c: \"Hour {}\".format(c) for c in hour_dummies.columns}\n",
    ")\n",
    "week_dummies = week_dummies.rename(\n",
    "    columns={c: \"Week {}\".format(c) for c in week_dummies.columns}\n",
    ")\n",
    "predictors_dum = pd.concat(\n",
    "    [week_dummies, hour_dummies, weekday_dummies,], axis=1,\n",
    ")\n",
    "\n",
    "day_angles = 2 * np.pi * times.dt.hour / 24\n",
    "year_angles = (\n",
    "    2 * np.pi * times.dt.week / 52\n",
    ")  # TODO Should we do this by day or week or month?\n",
    "predictors_trig = pd.concat(\n",
    "    [\n",
    "        pd.DataFrame(\n",
    "            {\n",
    "                \"Year sine\": np.sin(year_angles),\n",
    "                \"Year cosine\": np.cos(year_angles),\n",
    "            }\n",
    "        ),\n",
    "        pd.DataFrame(\n",
    "            {\"Day sine\": np.sin(day_angles), \"Day cosine\": np.cos(day_angles)}\n",
    "        ),\n",
    "        weekday_dummies,\n",
    "    ],\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_columns = [\n",
    "    (\"Waterloo Station 3, Waterloo\", \"Start\"),\n",
    "    (\"Hyde Park Corner, Hyde Park\", \"End\"),\n",
    "    (\"Wenlock Road , Hoxton\", \"End\"),\n",
    "]\n",
    "# test_columns = sum(\n",
    "#     [[(s, es) for s in test_stations] for es in (\"End\", \"Start\")], []\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weekly_rolling = changes.rolling(\"7d\").mean()\n",
    "# weekly_rolling = weekly_rolling - weekly_rolling.mean()\n",
    "# weekly_rolling = weekly_rolling / weekly_rolling.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekoffset = events_by_station.shift(freq=pd.Timedelta(\"7d\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_time = weekoffset.index.min()\n",
    "last_time = events_by_station.index.max()\n",
    "data = events_by_station.loc[first_time:last_time, test_columns]\n",
    "weekoffset = weekoffset.reindex(\n",
    "    data.index, method=\"nearest\"\n",
    ")  # TODO Is \"nearest\" good?\n",
    "# weekly_rolling = weekly_rolling[first_time:last_time]\n",
    "predictors_trig = predictors_trig.loc[first_time:last_time, :]\n",
    "predictors_dum = predictors_dum.loc[first_time:last_time, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created a RollingValidator with 6 cross-validation batches.\n",
      "Created a RollingValidator with 6 cross-validation batches.\n",
      "Created a RollingValidator with 6 cross-validation batches.\n",
      "Created a RollingValidator with 6 cross-validation batches.\n",
      "Created a RollingValidator with 6 cross-validation batches.\n",
      "Created a RollingValidator with 6 cross-validation batches.\n"
     ]
    }
   ],
   "source": [
    "specific_predictors = weekoffset[test_columns]\n",
    "min_training_time = 2 * pd.Timedelta(\"365d\")\n",
    "prediction_time = 0.5 * pd.Timedelta(\"365d\")\n",
    "rv_trig = RollingValidator(\n",
    "    data,\n",
    "    predictors_trig,\n",
    "    specific_predictors,\n",
    "    min_training_time,\n",
    "    prediction_time,\n",
    ")\n",
    "rv_dum = RollingValidator(\n",
    "    data,\n",
    "    predictors_dum,\n",
    "    specific_predictors,\n",
    "    min_training_time,\n",
    "    prediction_time,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For debugging\n",
    "# dbg_timerange = slice(\"2017-01-01\", None)\n",
    "# min_training_time = 2 * pd.Timedelta(\"30d\")\n",
    "# prediction_time = 0.5 * pd.Timedelta(\"30d\")\n",
    "# rv_trig = RollingValidator(\n",
    "#     data[dbg_timerange],\n",
    "#     predictors_trig[dbg_timerange],\n",
    "#     specific_predictors[dbg_timerange],\n",
    "#     min_training_time,\n",
    "#     prediction_time,\n",
    "# )\n",
    "# rv_dum = RollingValidator(\n",
    "#     data[dbg_timerange],\n",
    "#     predictors_dum[dbg_timerange],\n",
    "#     specific_predictors[dbg_timerange],\n",
    "#     min_training_time,\n",
    "#     prediction_time,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMean:\n",
    "    classname = \"SimpleMean\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "\n",
    "    def train(self, data, predictors):\n",
    "        mean = pd.DataFrame(data.mean())\n",
    "        self.model = mean\n",
    "\n",
    "    def predict(self, predictors):\n",
    "        mean = self.model\n",
    "        index = predictors.index\n",
    "        predictions = mean.T.apply(lambda x: [x[0]] * len(index)).set_index(\n",
    "            index\n",
    "        )\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LastWeek:\n",
    "    classname = \"LastWeek\"\n",
    "\n",
    "    def train(self, data, predictors):\n",
    "        pass\n",
    "\n",
    "    def predict(self, predictors):\n",
    "        # TODO Fix relying on column order\n",
    "        predictions = pd.DataFrame(predictors.iloc[:, -1])\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running RV on ('Waterloo Station 3, Waterloo', 'Start').\n",
      "Training for batch 0.\n",
      "Predicting for batch 0.\n",
      "Training for batch 1.\n",
      "Predicting for batch 1.\n",
      "Training for batch 2.\n",
      "Predicting for batch 2.\n",
      "Training for batch 3.\n",
      "Predicting for batch 3.\n",
      "Training for batch 4.\n",
      "Predicting for batch 4.\n",
      "Training for batch 5.\n",
      "Predicting for batch 5.\n",
      "Running RV on ('Hyde Park Corner, Hyde Park', 'End').\n",
      "Training for batch 0.\n",
      "Predicting for batch 0.\n",
      "Training for batch 1.\n",
      "Predicting for batch 1.\n",
      "Training for batch 2.\n",
      "Predicting for batch 2.\n",
      "Training for batch 3.\n",
      "Predicting for batch 3.\n",
      "Training for batch 4.\n",
      "Predicting for batch 4.\n",
      "Training for batch 5.\n",
      "Predicting for batch 5.\n",
      "Running RV on ('Wenlock Road , Hoxton', 'End').\n",
      "Training for batch 0.\n",
      "Predicting for batch 0.\n",
      "Training for batch 1.\n",
      "Predicting for batch 1.\n",
      "Training for batch 2.\n",
      "Predicting for batch 2.\n",
      "Training for batch 3.\n",
      "Predicting for batch 3.\n",
      "Training for batch 4.\n",
      "Predicting for batch 4.\n",
      "Training for batch 5.\n",
      "Predicting for batch 5.\n",
      "25.58690041257407\n",
      "Running RV on ('Waterloo Station 3, Waterloo', 'Start').\n",
      "Training for batch 0.\n",
      "Predicting for batch 0.\n",
      "Training for batch 1.\n",
      "Predicting for batch 1.\n",
      "Training for batch 2.\n",
      "Predicting for batch 2.\n",
      "Training for batch 3.\n",
      "Predicting for batch 3.\n",
      "Training for batch 4.\n",
      "Predicting for batch 4.\n",
      "Training for batch 5.\n",
      "Predicting for batch 5.\n",
      "Running RV on ('Hyde Park Corner, Hyde Park', 'End').\n",
      "Training for batch 0.\n",
      "Predicting for batch 0.\n",
      "Training for batch 1.\n",
      "Predicting for batch 1.\n",
      "Training for batch 2.\n",
      "Predicting for batch 2.\n",
      "Training for batch 3.\n",
      "Predicting for batch 3.\n",
      "Training for batch 4.\n",
      "Predicting for batch 4.\n",
      "Training for batch 5.\n",
      "Predicting for batch 5.\n",
      "Running RV on ('Wenlock Road , Hoxton', 'End').\n",
      "Training for batch 0.\n",
      "Predicting for batch 0.\n",
      "Training for batch 1.\n",
      "Predicting for batch 1.\n",
      "Training for batch 2.\n",
      "Predicting for batch 2.\n",
      "Training for batch 3.\n",
      "Predicting for batch 3.\n",
      "Training for batch 4.\n",
      "Predicting for batch 4.\n",
      "Training for batch 5.\n",
      "Predicting for batch 5.\n",
      "12.284625985759131\n"
     ]
    }
   ],
   "source": [
    "print(rv_trig.test_modelclass(SimpleMean, print_progress=True))\n",
    "print(rv_trig.test_modelclass(LastWeek, print_progress=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results for Linear are bad because all time variation has to be essentially sinusoidal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenericModel:\n",
    "    # Place-holders for subclasses to replace.\n",
    "    regressor = None\n",
    "    classname = None\n",
    "\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.column_name = None\n",
    "        self.std = None\n",
    "\n",
    "    def normalize_predictors(self, predictors):\n",
    "        predictors = predictors.copy()\n",
    "        self.stds = {}\n",
    "        self.means = {}\n",
    "        for c in predictors.columns:\n",
    "            try:\n",
    "                is_stationcolumn = \"Start\" in c or \"End\" in c\n",
    "            except TypeError:\n",
    "                is_stationcolumn = False\n",
    "            if is_stationcolumn:\n",
    "                col = predictors[c]\n",
    "                mean = col.mean()\n",
    "                std = col.std()\n",
    "                predictors[c] = (col - mean) / std\n",
    "                self.stds[c] = std\n",
    "                self.means[c] = mean\n",
    "        return predictors\n",
    "                \n",
    "    def renormalize_predictors(self, predictors):\n",
    "        predictors = predictors.copy()\n",
    "        for c in predictors.columns:\n",
    "            try:\n",
    "                is_stationcolumn = \"Start\" in c or \"End\" in c\n",
    "            except TypeError:\n",
    "                is_stationcolumn = False\n",
    "            if is_stationcolumn:\n",
    "                col = predictors[c]\n",
    "                mean = self.means[c]\n",
    "                std = self.stds[c]\n",
    "                predictors[c] = (col - mean) / std\n",
    "        return predictors\n",
    "        \n",
    "    def train(self, data, predictors):\n",
    "        predictors = self.normalize_predictors(predictors)\n",
    "        model = self.regressor()\n",
    "        model.fit(predictors, data)\n",
    "        self.model = model\n",
    "        self.column_name = data.columns[0]\n",
    "\n",
    "    def predict(self, predictors):\n",
    "        predictors = self.renormalize_predictors(predictors)\n",
    "        predictions = self.model.predict(predictors)\n",
    "        name = self.column_name\n",
    "        index = predictors.index\n",
    "        predictions = np.squeeze(predictions)\n",
    "        predictions = pd.DataFrame({name: predictions}, index=index)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(GenericModel):\n",
    "    classname = \"Linear\"\n",
    "    regressor = linear_model.Ridge\n",
    "\n",
    "\n",
    "class LinearSVR(GenericModel):\n",
    "    classname = \"LinearSVR\"\n",
    "    regressor = lambda x: svm.LinearSVR(max_iter=5000)\n",
    "\n",
    "\n",
    "class KNeighbors(GenericModel):\n",
    "    classname = \"KNeighbors\"\n",
    "    regressor = lambda x: neighbors.KNeighborsRegressor(\n",
    "        n_neighbors=5, weights=\"distance\"\n",
    "    )\n",
    "\n",
    "\n",
    "class SVRrbf(GenericModel):\n",
    "    classname = \"SVRrbf\"\n",
    "    regressor = lambda x: svm.SVR(kernel=\"rbf\", cache_size=500)\n",
    "\n",
    "\n",
    "class SVRpoly(GenericModel):\n",
    "    classname = \"SVRpoly\"\n",
    "    regressor = lambda x: svm.SVR(kernel=\"poly\", cache_size=500)\n",
    "\n",
    "\n",
    "class SVRsigmoid(GenericModel):\n",
    "    classname = \"SVRsigmoid\"\n",
    "    regressor = lambda x: svm.SVR(kernel=\"sigmoid\", cache_size=500)\n",
    "\n",
    "\n",
    "# class GaussianNB(GenericModel):\n",
    "#     classname = \"GaussianNB\"\n",
    "#     regressor = lambda x: naive_bayes.GaussianNB()\n",
    "\n",
    "\n",
    "class DecisionTree(GenericModel):\n",
    "    classname = \"DecisionTree\"\n",
    "    regressor = lambda x: tree.DecisionTreeRegressor(\n",
    "        criterion=\"mae\", max_depth=3\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that linear models probably fit to squared error, we measure absolute error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KernelRidge also can't handle this amount of training data with chocking on RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_diffmodelclass(modelclass):\n",
    "    class DiffModel(modelclass):\n",
    "        parent = modelclass\n",
    "        classname = \"DiffModel({})\".format(modelclass.classname)\n",
    "\n",
    "        def train(self, data, predictors):\n",
    "            data = data.copy()\n",
    "            predictors = predictors.copy()\n",
    "            self.dropped = []\n",
    "            for c in data.columns:\n",
    "                if c in predictors:\n",
    "                    data[c] = data[c] - predictors[c]\n",
    "                    predictors.drop(columns=c, inplace=True)\n",
    "                    self.dropped.append(c)\n",
    "            super().train(data, predictors)\n",
    "\n",
    "        def predict(self, predictors):\n",
    "            pruned_predictors = predictors.copy()\n",
    "            for c in self.dropped:\n",
    "                pruned_predictors.drop(columns=c, inplace=True)\n",
    "            predictions = super().predict(pruned_predictors)\n",
    "            for c in self.dropped:\n",
    "                predictions[c] = predictions[c] + predictors[c]\n",
    "            return predictions\n",
    "\n",
    "    return DiffModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "DiffSimpleMean = make_diffmodelclass(SimpleMean)\n",
    "DiffLinear = make_diffmodelclass(Linear)\n",
    "DiffKNeighbors = make_diffmodelclass(KNeighbors)\n",
    "DiffLinearSVR = make_diffmodelclass(LinearSVR)\n",
    "# DiffGaussianNB = make_diffmodelclass(GaussianNB)\n",
    "DiffDecisionTree = make_diffmodelclass(DecisionTree)\n",
    "DiffSVRrbf = make_diffmodelclass(SVRrbf)\n",
    "DiffSVRpoly = make_diffmodelclass(SVRpoly)\n",
    "DiffSVRsigmoid = make_diffmodelclass(SVRsigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running RV on ('Waterloo Station 3, Waterloo', 'Start').\n",
      "Training for batch 0.\n",
      "Predicting for batch 0.\n",
      "Training for batch 1.\n",
      "Predicting for batch 1.\n",
      "Training for batch 2.\n",
      "Predicting for batch 2.\n",
      "Training for batch 3.\n",
      "Predicting for batch 3.\n",
      "Training for batch 4.\n",
      "Predicting for batch 4.\n",
      "Training for batch 5.\n",
      "Predicting for batch 5.\n",
      "Running RV on ('Hyde Park Corner, Hyde Park', 'End').\n",
      "Training for batch 0.\n",
      "Predicting for batch 0.\n",
      "Training for batch 1.\n",
      "Predicting for batch 1.\n",
      "Training for batch 2.\n",
      "Predicting for batch 2.\n",
      "Training for batch 3.\n",
      "Predicting for batch 3.\n",
      "Training for batch 4.\n",
      "Predicting for batch 4.\n",
      "Training for batch 5.\n",
      "Predicting for batch 5.\n",
      "Running RV on ('Wenlock Road , Hoxton', 'End').\n",
      "Training for batch 0.\n",
      "Predicting for batch 0.\n",
      "Training for batch 1.\n",
      "Predicting for batch 1.\n",
      "Training for batch 2.\n",
      "Predicting for batch 2.\n",
      "Training for batch 3.\n",
      "Predicting for batch 3.\n",
      "Training for batch 4.\n",
      "Predicting for batch 4.\n",
      "Training for batch 5.\n",
      "Predicting for batch 5.\n",
      "14.32823418319169\n"
     ]
    }
   ],
   "source": [
    "# print(rv_dum.test_modelclass(DiffLinear, print_progress=True))\n",
    "print(rv_trig.test_modelclass(DiffKNeighbors, print_progress=True))\n",
    "# print(rv_trig.test_modelclass(DiffDecisionTree, print_progress=True))\n",
    "# print(rv_trig.test_modelclass(Linear, print_progress=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleMean\n",
      "trig mae: 25.587   (took 0.0 mins)\n",
      "dumm mae: 25.587   (took 0.0 mins)\n",
      "\n",
      "LastWeek\n",
      "trig mae: 12.285   (took 0.0 mins)\n",
      "dumm mae: 12.285   (took 0.0 mins)\n",
      "\n",
      "Linear\n",
      "trig mae: 12.537   (took 0.0 mins)\n",
      "dumm mae: 12.615   (took 0.0 mins)\n",
      "\n",
      "KNeighbors\n",
      "trig mae: 11.346   (took 0.2 mins)\n",
      "dumm mae: 11.491   (took 30.2 mins)\n",
      "\n",
      "LinearSVR\n",
      "trig mae: 11.496   (took 0.1 mins)\n",
      "dumm mae: 11.361   (took 0.1 mins)\n",
      "\n",
      "DecisionTree\n",
      "trig mae: 11.586   (took 5.6 mins)\n",
      "dumm mae: 11.724   (took 21.7 mins)\n",
      "\n",
      "SVRrbf\n",
      "trig mae: 10.800   (took 16.9 mins)\n",
      "dumm mae: 10.915   (took 69.0 mins)\n",
      "\n",
      "SVRpoly\n",
      "trig mae: 10.553   (took 16.9 mins)\n",
      "dumm mae: 10.686   (took 72.9 mins)\n",
      "\n",
      "DiffModel(SimpleMean)\n",
      "trig mae: 12.300   (took 0.0 mins)\n",
      "dumm mae: 12.300   (took 0.0 mins)\n",
      "\n",
      "DiffModel(Linear)\n",
      "trig mae: 12.436   (took 0.0 mins)\n",
      "dumm mae: 13.029   (took 0.0 mins)\n",
      "\n",
      "DiffModel(KNeighbors)\n",
      "trig mae: 14.328   (took 0.2 mins)\n",
      "dumm mae: 14.316   (took 31.5 mins)\n",
      "\n",
      "DiffModel(LinearSVR)\n",
      "trig mae: 12.285   (took 0.0 mins)\n",
      "dumm mae: 12.296   (took 0.0 mins)\n",
      "\n",
      "DiffModel(DecisionTree)\n",
      "trig mae: 12.294   (took 5.5 mins)\n",
      "dumm mae: 12.284   (took 26.8 mins)\n",
      "\n",
      "DiffModel(SVRrbf)\n",
      "trig mae: 12.341   (took 14.5 mins)\n",
      "dumm mae: 12.452   (took 68.2 mins)\n",
      "\n",
      "DiffModel(SVRpoly)\n",
      "trig mae: 12.341   (took 12.1 mins)\n",
      "dumm mae: 12.513   (took 66.3 mins)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from timeit import default_timer as timer\n",
    "\n",
    "\n",
    "import pickle\n",
    "\n",
    "classes = [\n",
    "    SimpleMean,\n",
    "    LastWeek,\n",
    "    Linear,\n",
    "    KNeighbors,\n",
    "    LinearSVR,\n",
    "    #     GaussianNB,\n",
    "    DecisionTree,\n",
    "    SVRrbf,\n",
    "    SVRpoly,\n",
    "    #     SVRsigmoid,\n",
    "    DiffSimpleMean,\n",
    "    DiffLinear,\n",
    "    DiffKNeighbors,\n",
    "    DiffLinearSVR,\n",
    "    #     DiffGaussianNB,\n",
    "    DiffDecisionTree,\n",
    "    DiffSVRrbf,\n",
    "    DiffSVRpoly,\n",
    "    #     DiffSVRsigmoid,\n",
    "]\n",
    "for modelclass in classes:\n",
    "    print(modelclass.classname)\n",
    "    for rv, rv_name in ((rv_trig, \"trig\"), (rv_dum, \"dumm\")):\n",
    "        start = timer()\n",
    "        try:\n",
    "            err = rv.test_modelclass(modelclass)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        stop = timer()\n",
    "        time = (stop - start) / 60\n",
    "        print(\"{} mae: {:.3f}   (took {:.1f} mins)\".format(rv_name, err, time))\n",
    "        with open(\"latest_rv_dum.p\", \"wb\") as f:\n",
    "            pickle.dump(rv_dum, f)\n",
    "        with open(\"latest_rv_trig.p\", \"wb\") as f:\n",
    "            pickle.dump(rv_trig, f)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"latest_rv_dum.p\", \"rb\") as f:\n",
    "    rv_dum = pickle.load(f)\n",
    "with open(\"latest_rv_trig.p\", \"rb\") as f:\n",
    "    rv_trig = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleMean               : 25.587   24.211\n",
      "LastWeek                 : 12.285   11.369\n",
      "Linear                   : 12.615   11.821\n",
      "KNeighbors               : 11.491   1.104\n",
      "LinearSVR                : 11.361   10.479\n",
      "DecisionTree             : 11.724   10.689\n",
      "SVRrbf                   : 10.915   9.282\n",
      "SVRpoly                  : 10.686   8.827\n",
      "DiffModel(SimpleMean)    : 12.300   11.384\n",
      "DiffModel(Linear)        : 13.029   11.942\n",
      "DiffModel(KNeighbors)    : 14.316   10.089\n",
      "DiffModel(LinearSVR)     : 12.296   11.367\n",
      "DiffModel(DecisionTree)  : 12.284   11.335\n",
      "DiffModel(SVRrbf)        : 12.452   11.083\n",
      "DiffModel(SVRpoly)       : 12.513   10.898\n"
     ]
    }
   ],
   "source": [
    "for k, v in rv_dum.models.items():\n",
    "    test_mae = v[\"test_mae\"].sum()\n",
    "    training_mae = v[\"training_mae\"].sum()\n",
    "    print(\"{:25}: {:.3f}   {:.3f}\".format(k, test_mae, training_mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04621ee759144b32be4c88fcb5d5480b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fe7ce6ffed0>]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rv = rv_trig\n",
    "plot_column = (\"Hyde Park Corner, Hyde Park\", \"End\")\n",
    "# plot_columns = [(\"Wenlock Road , Hoxton\", \"End\")]\n",
    "# plot_columns = [(\"Waterloo Station 3, Waterloo\",  \"Start\")]\n",
    "modelclass = Linear\n",
    "err = pd.concat(rv.models[modelclass.classname][\"test_errors\"]).sort_index()\n",
    "pred = pd.concat(rv.models[modelclass.classname][\"test_predictions\"]).sort_index()\n",
    "truth = pd.concat([l[3] for l in rv.cv_batches]).sort_index()\n",
    "plt.figure()\n",
    "plt.plot(truth[plot_column], pred[plot_column], ls=\"\", marker=\"*\", ms=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06984f0001af4e8bb88e32a5feb1df05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fe7c5725510>]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rv = rv_dum\n",
    "plot_columns = [(\"Hyde Park Corner, Hyde Park\", \"End\")]\n",
    "# plot_columns = [(\"Wenlock Road , Hoxton\", \"End\")]\n",
    "# plot_columns = [(\"Waterloo Station 3, Waterloo\",  \"Start\")]\n",
    "modelclass = SVRrbf\n",
    "err = pd.concat(rv.models[modelclass.classname][\"test_errors\"]).sort_index()\n",
    "pred = pd.concat(rv.models[modelclass.classname][\"test_predictions\"]).sort_index()\n",
    "truth = pd.concat([l[3] for l in rv.cv_batches]).sort_index()\n",
    "plt.figure()\n",
    "plt.plot(truth[plot_columns])\n",
    "plt.plot(pred[plot_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:1: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61020229d9df41c99caf774bc7bef1c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fe7c375ce90>]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure()\n",
    "example_morning = data[(\"Waterloo Station 3, Waterloo\",  \"Start\")][(times.dt.weekday == 1) & (times.dt.hour == 7)]\n",
    "m = example_morning.mean()\n",
    "example_morning.hist(bins=50)\n",
    "xaxis = example_morning.unique()\n",
    "xaxis.sort()\n",
    "yaxis = sp.stats.poisson.pmf(xaxis, m) * len(example_morning)\n",
    "plt.plot(xaxis, yaxis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZeroPredictor:\n",
    "\n",
    "    def predict(self, predictors):\n",
    "        prediction = pd.Series(0.0, index=predictors.index)\n",
    "        return prediction\n",
    "\n",
    "\n",
    "class PoissonGLM:\n",
    "    classname = \"PoissonGLM\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "        self.stds = {}\n",
    "        self.means= {}\n",
    "        self.column_name = None\n",
    "        \n",
    "        \n",
    "    def normalize_predictors(self, predictors):\n",
    "        predictors = predictors.copy()\n",
    "        self.stds = {}\n",
    "        self.means = {}\n",
    "        for c in predictors.columns:\n",
    "            try:\n",
    "                is_stationcolumn = \"Start\" in c or \"End\" in c\n",
    "            except TypeError:\n",
    "                is_stationcolumn = False\n",
    "            if is_stationcolumn:\n",
    "                col = predictors[c]\n",
    "                mean = col.mean()\n",
    "                std = col.std()\n",
    "                predictors[c] = (col - mean) / std\n",
    "                self.stds[c] = std\n",
    "                self.means[c] = mean\n",
    "        return predictors\n",
    "                \n",
    "    def renormalize_predictors(self, predictors):\n",
    "        predictors = predictors.copy()\n",
    "        for c in predictors.columns:\n",
    "            try:\n",
    "                is_stationcolumn = \"Start\" in c or \"End\" in c\n",
    "            except TypeError:\n",
    "                is_stationcolumn = False\n",
    "            if is_stationcolumn:\n",
    "                col = predictors[c]\n",
    "                mean = self.means[c]\n",
    "                std = self.stds[c]\n",
    "                predictors[c] = (col - mean) / std\n",
    "        return predictors\n",
    "\n",
    "    def train(self, data, predictors):\n",
    "        self.models = {}\n",
    "        predictors = self.normalize_predictors(predictors)\n",
    "\n",
    "        times = data.index.to_series()\n",
    "        groupers = [times.dt.weekday, times.dt.hour]\n",
    "        data_groups = data.groupby(groupers)\n",
    "        predictors_groups = predictors.groupby(groupers)\n",
    "        for group_label, data_group in data_groups:\n",
    "            predictors_group = predictors_groups.get_group(\n",
    "                group_label\n",
    "            )\n",
    "            glm_poisson = sm.GLM(\n",
    "                data_group,\n",
    "                predictors_group,\n",
    "                family=sm.families.Poisson(),\n",
    "            )\n",
    "            try:\n",
    "                model = glm_poisson.fit()\n",
    "            except ValueError as e:\n",
    "                # The GLM can't handle casese like data that is all zeros.\n",
    "                # In those cases, just make a predictor that always predicts zero.\n",
    "                model = ZeroPredictor()\n",
    "                if (data_group.max() > 0.0).any():\n",
    "                    # The usual reason all all-zero is not why we errored this time.\n",
    "                    # Print some diagnostics to figure out what went wrong.\n",
    "                    print(\"glm_poisson.fit() raise a ValueError.\")\n",
    "                    print(data_group.describe())\n",
    "                    print(predictors_group.describe())\n",
    "            self.models[group_label] = model\n",
    "        self.column_name = data.columns[0]\n",
    "\n",
    "    def predict(self, predictors):\n",
    "        predictors = self.renormalize_predictors(predictors)\n",
    "        name = self.column_name\n",
    "        times = predictors.index.to_series()\n",
    "        groupers = [times.dt.weekday, times.dt.hour]\n",
    "        predictors_groups = predictors.groupby(groupers)\n",
    "        predictions_groups = []\n",
    "        for (\n",
    "            group_label,\n",
    "            predictors_group,\n",
    "        ) in predictors_groups:\n",
    "            model = self.models[group_label]\n",
    "            predictions_group = model.predict(predictors_group)\n",
    "            predictions_groups.append(predictions_group)\n",
    "        predictions = pd.concat(predictions_groups, axis=0)\n",
    "        predictions = pd.DataFrame({name: predictions}, index=times)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_rolling = events_by_station[test_columns].rolling(\"7d\").mean()[first_time:last_time]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created a RollingValidator with 6 cross-validation batches.\n",
      "Created a RollingValidator with 6 cross-validation batches.\n",
      "Created a RollingValidator with 6 cross-validation batches.\n"
     ]
    }
   ],
   "source": [
    "# year_angles = 2 * np.pi * times.dt.week / 52\n",
    "# predictors_poiss = pd.DataFrame(\n",
    "#     {\"Year sine\": np.sin(year_angles), \"Year cosine\": np.cos(year_angles),}\n",
    "# )\n",
    "predictors_poiss = pd.get_dummies(times.dt.month)\n",
    "rv_poiss = RollingValidator(\n",
    "    data,\n",
    "    predictors_poiss,\n",
    "#     weekly_rolling,\n",
    "    specific_predictors,\n",
    "    min_training_time,\n",
    "    prediction_time,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running RV on ('Waterloo Station 3, Waterloo', 'Start').\n",
      "Training for batch 0.\n",
      "Predicting for batch 0.\n",
      "Training for batch 1.\n",
      "Predicting for batch 1.\n",
      "Training for batch 2.\n",
      "Predicting for batch 2.\n",
      "Training for batch 3.\n",
      "Predicting for batch 3.\n",
      "Training for batch 4.\n",
      "Predicting for batch 4.\n",
      "Training for batch 5.\n",
      "Predicting for batch 5.\n",
      "Running RV on ('Hyde Park Corner, Hyde Park', 'End').\n",
      "Training for batch 0.\n",
      "Predicting for batch 0.\n",
      "Training for batch 1.\n",
      "Predicting for batch 1.\n",
      "Training for batch 2.\n",
      "Predicting for batch 2.\n",
      "Training for batch 3.\n",
      "Predicting for batch 3.\n",
      "Training for batch 4.\n",
      "Predicting for batch 4.\n",
      "Training for batch 5.\n",
      "Predicting for batch 5.\n",
      "Running RV on ('Wenlock Road , Hoxton', 'End').\n",
      "Training for batch 0.\n",
      "Predicting for batch 0.\n",
      "Training for batch 1.\n",
      "Predicting for batch 1.\n",
      "Training for batch 2.\n",
      "Predicting for batch 2.\n",
      "Training for batch 3.\n",
      "Predicting for batch 3.\n",
      "Training for batch 4.\n",
      "Predicting for batch 4.\n",
      "Training for batch 5.\n",
      "Predicting for batch 5.\n",
      "10.874694691424459\n",
      "9.484941669837266\n"
     ]
    }
   ],
   "source": [
    "mae = rv_poiss.test_modelclass(PoissonGLM, print_progress=True)\n",
    "print(mae)\n",
    "print(rv_poiss.models[PoissonGLM.classname][\"training_mae\"].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specific_predictors.loc[\"2016-08-15\":\"2016-08-16\", :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Waterloo Station 3, Waterloo, Start)    3.699950\n",
       "(Hyde Park Corner, Hyde Park, End)       4.303425\n",
       "(Wenlock Road , Hoxton, End)             0.954040\n",
       "dtype: float64"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(rv_poiss.models.values())[0][\"test_mae\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/markus/.local/lib/python3.7/site-packages/pandas/plotting/_matplotlib/core.py:338: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig = self.plt.figure(figsize=self.figsize)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f4fb2d00e2d4786b7b5cd0dfb336a8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f16aea2a390>"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat(list(rv_poiss.models.values())[0][\"test_errors\"], axis=0).sort_index().loc[\"2016-11-01\":, :].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdf = pd.read_csv(\"./data/weather/london_weather_data.csv\", usecols=[\"DATE\", \"PRCP\", \"TAVG\"], encoding=\"ISO-8859-2\")\n",
    "wdf[\"DATE\"] = pd.to_datetime(wdf[\"DATE\"])\n",
    "wdf = wdf.set_index(\"DATE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdf = wdf[first_time:last_time]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There's only 6 NaNs in this time range, so how we fill them doesn't really matter much.\n",
    "wdf[\"PRCP\"] = wdf[\"PRCP\"].fillna(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6e51550c16c4601a67702cab45eb7f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wdf.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdfn = wdf.copy()\n",
    "wdfn[\"PRCP\"] /= wdfn[\"PRCP\"].std()\n",
    "wdfn[\"TAVG\"] = (wdfn[\"TAVG\"] - wdfn[\"TAVG\"].mean())/wdfn[\"TAVG\"].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bda69e5ecd44109a8ef8407f0118f8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x7f57d2c80790>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x7f57d2f19b50>]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wdfn.hist(bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specific_predictors = weekoffset[test_columns]\n",
    "# min_training_time = 2 * pd.Timedelta(\"365d\")\n",
    "# prediction_time = 0.5 * pd.Timedelta(\"365d\")\n",
    "# rv_weather = RollingValidator(\n",
    "#     data,\n",
    "#     predictors_trig,\n",
    "#     specific_predictors,\n",
    "#     min_training_time,\n",
    "#     prediction_time,\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "08a39f0d195a41d7a0ab453dc229a0d6": {
      "model_module": "jupyter-matplotlib",
      "model_module_version": "^0.5.0",
      "model_name": "MPLCanvasModel",
      "state": {
       "layout": "IPY_MODEL_d5a102e4a29944e0abaf2fe4528892ca",
       "toolbar": "IPY_MODEL_a286d9f746ef4f00923f4327ff6b2dda",
       "toolbar_position": "left"
      }
     },
     "288c543fffc7486aa358a1449d03128f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "2fbb249fc2834c418ea527bb9778d5e2": {
      "model_module": "jupyter-matplotlib",
      "model_module_version": "^0.5.0",
      "model_name": "ToolbarModel",
      "state": {
       "layout": "IPY_MODEL_bc59892f923b4d7993a28558c394bbb2",
       "toolitems": [
        [
         "Home",
         "Reset original view",
         "home",
         "home"
        ],
        [
         "Back",
         "Back to previous view",
         "arrow-left",
         "back"
        ],
        [
         "Forward",
         "Forward to next view",
         "arrow-right",
         "forward"
        ],
        [
         "Pan",
         "Pan axes with left mouse, zoom with right",
         "arrows",
         "pan"
        ],
        [
         "Zoom",
         "Zoom to rectangle",
         "square-o",
         "zoom"
        ],
        [
         "Download",
         "Download plot",
         "floppy-o",
         "save_figure"
        ]
       ]
      }
     },
     "4ba3ee82a3dd4f598c2a32f254cf1649": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "76ec00cd01ef49afa1f1ad24f113f85a": {
      "model_module": "jupyter-matplotlib",
      "model_module_version": "^0.5.0",
      "model_name": "ToolbarModel",
      "state": {
       "layout": "IPY_MODEL_e0c770b8383c43d7b67132704f57aebe",
       "toolitems": [
        [
         "Home",
         "Reset original view",
         "home",
         "home"
        ],
        [
         "Back",
         "Back to previous view",
         "arrow-left",
         "back"
        ],
        [
         "Forward",
         "Forward to next view",
         "arrow-right",
         "forward"
        ],
        [
         "Pan",
         "Pan axes with left mouse, zoom with right",
         "arrows",
         "pan"
        ],
        [
         "Zoom",
         "Zoom to rectangle",
         "square-o",
         "zoom"
        ],
        [
         "Download",
         "Download plot",
         "floppy-o",
         "save_figure"
        ]
       ]
      }
     },
     "a286d9f746ef4f00923f4327ff6b2dda": {
      "model_module": "jupyter-matplotlib",
      "model_module_version": "^0.5.0",
      "model_name": "ToolbarModel",
      "state": {
       "layout": "IPY_MODEL_fd4ebe0d9d264297b174e01db7afc677",
       "toolitems": [
        [
         "Home",
         "Reset original view",
         "home",
         "home"
        ],
        [
         "Back",
         "Back to previous view",
         "arrow-left",
         "back"
        ],
        [
         "Forward",
         "Forward to next view",
         "arrow-right",
         "forward"
        ],
        [
         "Pan",
         "Pan axes with left mouse, zoom with right",
         "arrows",
         "pan"
        ],
        [
         "Zoom",
         "Zoom to rectangle",
         "square-o",
         "zoom"
        ],
        [
         "Download",
         "Download plot",
         "floppy-o",
         "save_figure"
        ]
       ]
      }
     },
     "bc59892f923b4d7993a28558c394bbb2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "d5a102e4a29944e0abaf2fe4528892ca": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "dada05a7f800487ab8da85d16616a039": {
      "model_module": "jupyter-matplotlib",
      "model_module_version": "^0.5.0",
      "model_name": "MPLCanvasModel",
      "state": {
       "layout": "IPY_MODEL_4ba3ee82a3dd4f598c2a32f254cf1649",
       "toolbar": "IPY_MODEL_2fbb249fc2834c418ea527bb9778d5e2",
       "toolbar_position": "left"
      }
     },
     "dd259d320aee45b7a4e5bad340f75520": {
      "model_module": "jupyter-matplotlib",
      "model_module_version": "^0.5.0",
      "model_name": "MPLCanvasModel",
      "state": {
       "layout": "IPY_MODEL_288c543fffc7486aa358a1449d03128f",
       "toolbar": "IPY_MODEL_76ec00cd01ef49afa1f1ad24f113f85a",
       "toolbar_position": "left"
      }
     },
     "e0c770b8383c43d7b67132704f57aebe": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "fd4ebe0d9d264297b174e01db7afc677": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
