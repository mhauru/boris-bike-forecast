{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Boris Bike usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since 2010, Transport for London (TfL) has been operating a city bike scheme. It was originally called Barclays Cycle Hire and is now officially known as Santander Cycles, but is more colloquially known as Boris Bikes, after then London mayor Boris Johnson. The system includes more than 10,000 bikes and hundreds of stations. With a credit card, one can go to station, rent a bike for a time, and return it to any of the other stations in London. Transport for London makes public usage data of Boris Bikes since 2012, and in this notebook I study that data. The goal is to\n",
    "* create a model for predicting the number of bikes leaving and arriving at each station, using both periodic and autoregressive features in the data and possibly weather data for London,\n",
    "* see whether the weather data is a useful predictor, by testing the hypothesis that it affects bike usage.\n",
    "\n",
    "Weather data for London is kindly provided for free by America's [National Oceanic and Atmospheric Administration](https://www.ncdc.noaa.gov/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick shout out to a couple of other interesting analyses of the same bike data that I found, that take a more exploratory tone and focus on different aspects: [Anders Ohrn](https://medium.com/@AJOhrn/data-footprint-of-bike-sharing-in-london-be9e11425248) has, among things, some cool animated maps of when and where bikes are used; [charlie1347](https://github.com/charlie1347/TfL_bikes) for instance divides stations in to two categories based on whether they see incoming trafic in the morning and outgoing in the afternoon or the other way around, and looks at the effect London tube strikes had on bike usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to hopefully giving some insight into Boris Bike usage, this project is also a vehicle for me to learn some machine learning and statistical modelling, and usage of relevant Python packages, such as scikit-learn and statsmodels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import requests\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import statsmodels.api as sm\n",
    "from sklearn import linear_model, svm, neighbors, tree\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from timeit import default_timer as timer\n",
    "from IPython.display import set_matplotlib_formats\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "try:\n",
    "    import xlrd\n",
    "except Exception as e:\n",
    "    msg = (\n",
    "        \"Please install the package xlrd: `pip install --user xlrd`\"\n",
    "        \"It's an optional requirement for pandas, and we'll be needing it.\"\n",
    "    )\n",
    "    print(msg)\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For pretty and exportable matplotlib plots.\n",
    "# If you are running this yourself and want interactivity,\n",
    "# try `%matplotlib widget` instead.\n",
    "# set_matplotlib_formats(\"svg\")\n",
    "# %matplotlib inline\n",
    "%matplotlib widget\n",
    "# Set a consistent plotting style across the notebook using Seaborn.\n",
    "sns.set_style(\"darkgrid\")\n",
    "sns.set_context(\"notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing and cleaning the bike data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before getting anywhere with it, we'll need to process the bike data quite a bit. The data comes in CSV files, each of which covers a period of time. Up first, we need to download the data from the TfL website. If you are running this code yourself, here's a script that does that. Be warned though, it's almost seven gigs of data. You can run it repeatedly though, and it won't redownload, although it will reunzip the zip files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikefolder = \"./data/bikes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already have 01aJourneyDataExtract10Jan16-23Jan16.csv\n",
      "Already have 01bJourneyDataExtract24Jan16-06Feb16.csv\n",
      "Already have 02bJourneyDataExtract21Feb16-05Mar2016.csv\n",
      "Already have 03JourneyDataExtract06Mar2016-31Mar2016.csv\n",
      "Already have 04JourneyDataExtract01Apr2016-30Apr2016.csv\n",
      "Already have 05JourneyDataExtract01May2016-17May2016.csv\n",
      "Already have 07JourneyDataExtract25May2016-31May2016.csv\n",
      "Already have 08JourneyDataExtract01Jun2016-07Jun2016.csv\n",
      "Already have 09JourneyDataExtract08Jun2016-14Jun2016.csv\n",
      "Already have 10JourneyDataExtract15Jun2016-21Jun2016.csv\n",
      "Already have 11JourneyDataExtract22Jun2016-28Jun2016.csv\n",
      "Already have 12JourneyDataExtract29Jun2016-05Jul2016.csv\n",
      "Already have 13JourneyDataExtract06Jul2016-12Jul2016.csv\n",
      "Already have 14JourneyDataExtract13Jul2016-19Jul2016.csv\n",
      "Already have 15JourneyDataExtract20Jul2016-26Jul2016.csv\n",
      "Already have 16JourneyDataExtract27Jul2016-02Aug2016.csv\n",
      "Already have 17JourneyDataExtract03Aug2016-09Aug2016.csv\n",
      "Already have 18JourneyDataExtract10Aug2016-16Aug2016.csv\n",
      "Already have 19JourneyDataExtract17Aug2016-23Aug2016.csv\n",
      "Already have 1a.JourneyDataExtract04Jan15-17Jan15.csv\n",
      "Already have 1b.JourneyDataExtract18Jan15-31Jan15.csv\n",
      "Already have 20JourneyDataExtract24Aug2016-30Aug2016.csv\n",
      "Already have 21JourneyDataExtract31Aug2016-06Sep2016.csv\n",
      "Already have 22JourneyDataExtract07Sep2016-13Sep2016.csv\n",
      "Already have 23JourneyDataExtract14Sep2016-20Sep2016.csv\n",
      "Already have 24JourneyDataExtract21Sep2016-27Sep2016.csv\n",
      "Already have 25JourneyDataExtract28Sep2016-04Oct2016.csv\n",
      "Already have 26JourneyDataExtract05Oct2016-11Oct2016.csv\n",
      "Already have 27JourneyDataExtract12Oct2016-18Oct2016.csv\n",
      "Already have 28JourneyDataExtract19Oct2016-25Oct2016.csv\n",
      "Already have 29JourneyDataExtract26Oct2016-01Nov2016.csv\n",
      "Already have 2a.JourneyDataExtract01Feb15-14Feb15.csv\n",
      "Already have 2b.JourneyDataExtract15Feb15-28Feb15.csv\n",
      "Already have 30JourneyDataExtract02Nov2016-08Nov2016.csv\n",
      "Already have 31JourneyDataExtract09Nov2016-15Nov2016.csv\n",
      "Already have 32JourneyDataExtract16Nov2016-22Nov2016.csv\n",
      "Already have 33JourneyDataExtract23Nov2016-29Nov2016.csv\n",
      "Already have 34JourneyDataExtract30Nov2016-06Dec2016.csv\n",
      "Already have 35JourneyDataExtract07Dec2016-13Dec2016.csv\n",
      "Already have 36JourneyDataExtract14Dec2016-20Dec2016.csv\n",
      "Already have 37JourneyDataExtract21Dec2016-27Dec2016.csv\n",
      "Already have 38JourneyDataExtract28Dec2016-03Jan2017.csv\n",
      "Already have 39JourneyDataExtract04Jan2017-10Jan2017.csv\n",
      "Already have 3a.JourneyDataExtract01Mar15-15Mar15.csv\n",
      "Already have 3b.JourneyDataExtract16Mar15-31Mar15.csv\n",
      "Already have 40JourneyDataExtract11Jan2017-17Jan2017.csv\n",
      "Already have 41JourneyDataExtract18Jan2017-24Jan2017.csv\n",
      "Already have 42JourneyDataExtract25Jan2017-31Jan2017.csv\n",
      "Already have 43JourneyDataExtract01Feb2017-07Feb2017.csv\n",
      "Already have 44JourneyDataExtract08Feb2017-14Feb2017.csv\n",
      "Already have 45JourneyDataExtract15Feb2017-21Feb2017.csv\n",
      "Already have 46JourneyDataExtract22Feb2017-28Feb2017.csv\n",
      "Already have 47JourneyDataExtract01Mar2017-07Mar2017.csv\n",
      "Already have 48JourneyDataExtract08Mar2017-14Mar2017.csv\n",
      "Already have 4a.JourneyDataExtract01Apr15-16Apr15.csv\n",
      "Already have 4b.JourneyDataExtract 17Apr15-02May15.csv\n",
      "Already have 50 Journey Data Extract 22Mar2017-28Mar2017.csv\n",
      "Already have 51 Journey Data Extract 29Mar2017-04Apr2017.csv\n",
      "Already have 52 Journey Data Extract 05Apr2017-11Apr2017.csv\n",
      "Already have 53JourneyDataExtract12Apr2017-18Apr2017.csv\n",
      "Already have 54JourneyDataExtract19Apr2017-25Apr2017.csv\n",
      "Already have 55JourneyData Extract26Apr2017-02May2017.csv\n",
      "Already have 56JourneyDataExtract 03May2017-09May2017.csv\n",
      "Already have 57JourneyDataExtract10May2017-16May2017.csv\n",
      "Already have 5a.JourneyDataExtract03May15-16May15.csv\n",
      "Already have 5b.JourneyDataExtract17May15-30May15.csv\n",
      "Already have 6aJourneyDataExtract31May15-12Jun15.csv\n",
      "Already have 6bJourneyDataExtract13Jun15-27Jun15.csv\n",
      "Already have 7a.JourneyDataExtract28Jun15-11Jul15.csv\n",
      "Already have 7b.JourneyDataExtract12Jul15-25Jul15.csv\n",
      "Already have 9a-Journey-Data-Extract-23Aug15-05Sep15.csv\n",
      "Already have 9b-Journey-Data-Extract-06Sep15-19Sep15.csv\n",
      "Already have cyclehireusagestats-2012.zip\n",
      "Unziping data/bikezips/cyclehireusagestats-2012.zip\n",
      "Already have cyclehireusagestats-2013.zip\n",
      "Unziping data/bikezips/cyclehireusagestats-2013.zip\n",
      "Already have cyclehireusagestats-2014.zip\n",
      "Unziping data/bikezips/cyclehireusagestats-2014.zip\n",
      "Already have 2015TripDatazip.zip\n",
      "Unziping data/bikezips/2015TripDatazip.zip\n",
      "Already have 2016TripDataZip.zip\n",
      "Unziping data/bikezips/2016TripDataZip.zip\n",
      "Already have 49JourneyDataExtract15Mar2017-21Mar2017.xlsx\n",
      "Converting .xlsx to .csv.\n"
     ]
    }
   ],
   "source": [
    "def download_file(datafolder, url):\n",
    "    \"\"\"Download the data from the given URL into the datafolder, unless it's\n",
    "    already there. Return path to downloaded file.\n",
    "    \"\"\"\n",
    "    datafolder = Path(datafolder)\n",
    "    datafolder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    a = urlparse(url)\n",
    "    filename = Path(os.path.basename(a.path))\n",
    "    filepath = datafolder / filename\n",
    "    # Don't redownload if we already have this file.\n",
    "    if filepath.exists():\n",
    "        print(\"Already have {}\".format(filename))\n",
    "    else:\n",
    "        print(\"Downloading {}\".format(filename))\n",
    "        rqst = requests.get(url)\n",
    "        with open(filepath, \"wb\") as f:\n",
    "            f.write(rqst.content)\n",
    "    return filepath\n",
    "\n",
    "\n",
    "# Most files are individual CSV files, listed in bike_data_urls.txt. Download\n",
    "# them.\n",
    "urlsfile = \"bike_data_urls.txt\"\n",
    "with open(urlsfile, \"r\") as f:\n",
    "    urls = f.read().splitlines()\n",
    "# There are a few comments in the file, marked by lines starting with #.\n",
    "# Filter them out.\n",
    "urls = [u for u in urls if u[0] != \"#\"]\n",
    "for url in urls:\n",
    "    download_file(bikefolder, url)\n",
    "\n",
    "# The early years come in zips. Download and unzip them.\n",
    "zipsfolder = Path(\"./data/bikezips\")\n",
    "bikezipurls = [\n",
    "    \"http://cycling.data.tfl.gov.uk/usage-stats/cyclehireusagestats-2012.zip\",\n",
    "    \"http://cycling.data.tfl.gov.uk/usage-stats/cyclehireusagestats-2013.zip\",\n",
    "    \"http://cycling.data.tfl.gov.uk/usage-stats/cyclehireusagestats-2014.zip\",\n",
    "    \"http://cycling.data.tfl.gov.uk/usage-stats/2015TripDatazip.zip\",\n",
    "    \"http://cycling.data.tfl.gov.uk/usage-stats/2016TripDataZip.zip\",\n",
    "]\n",
    "for url in bikezipurls:\n",
    "    zippath = download_file(zipsfolder, url)\n",
    "    print(\"Unziping {}\".format(zippath))\n",
    "    with zipfile.ZipFile(zippath, \"r\") as z:\n",
    "        z.extractall(bikefolder)\n",
    "\n",
    "# Finally, there's an odd one out: One week's data comes in as an .xlsx.\n",
    "# Download and use pandas to convert it to csv.\n",
    "xlsxurl = \"https://cycling.data.tfl.gov.uk/usage-stats/49JourneyDataExtract15Mar2017-21Mar2017.xlsx\"\n",
    "xlsxfile = download_file(bikefolder, xlsxurl)\n",
    "csvfile = xlsxfile.with_suffix(\".csv\")\n",
    "print(\"Converting .xlsx to .csv.\")\n",
    "pd.read_excel(xlsxfile).to_csv(csvfile, date_format=\"%d/%m/%Y %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data we have now lists on each line of the CSV file a single bike trip, with starting point and time, end point and time, and things like bike ID number. That's not quite what I'm interested in. What I want is a pandas DataFrame that has for each bike station a column for starts and ends, and as its index time. We choose to bin the time by hours. Each cell would then tell how many bikes left or arrived to this station at during a given hour.\n",
    "\n",
    "Doing this is complicated by the CSV files having quite inconsistent formatting. Date formats vary, stations go by different names, etc. If you care about all the details of what needs to be done to get this cleaned up, read the source code below.\n",
    "\n",
    "Finally, since processing this data takes a few minutes, we store the resulting DataFrame in a file in the current working directory. You can then run this cell again, and it'll just load that file. It takes up around 600Mb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data/bikes/01aJourneyDataExtract10Jan16-23Jan16.csv\n",
      "Processing data/bikes/01bJourneyDataExtract24Jan16-06Feb16.csv\n",
      "Processing data/bikes/02aJourneyDataExtract07Feb16-20Feb2016.csv\n",
      "Processing data/bikes/02bJourneyDataExtract21Feb16-05Mar2016.csv\n",
      "Processing data/bikes/03JourneyDataExtract06Mar2016-31Mar2016.csv\n",
      "Processing data/bikes/04JourneyDataExtract01Apr2016-30Apr2016.csv\n",
      "Processing data/bikes/05JourneyDataExtract01May2016-17May2016.csv\n",
      "Processing data/bikes/06JourneyDataExtract18May2016-24May2016.csv\n",
      "Processing data/bikes/07JourneyDataExtract25May2016-31May2016.csv\n",
      "Processing data/bikes/08JourneyDataExtract01Jun2016-07Jun2016.csv\n",
      "Processing data/bikes/09JourneyDataExtract08Jun2016-14Jun2016.csv\n",
      "Processing data/bikes/1. Journey Data Extract 01Jan-05Jan13.csv\n",
      "Processing data/bikes/1. Journey Data Extract 04Jan-31Jan 12.csv\n",
      "Processing data/bikes/1. Journey Data Extract 05Jan14-02Feb14.csv\n",
      "Processing data/bikes/10. Journey Data Extract 18Aug-13Sep13.csv\n",
      "Processing data/bikes/10. Journey Data Extract 21Aug-22 Aug12.csv\n",
      "Processing data/bikes/10JourneyDataExtract15Jun2016-21Jun2016.csv\n",
      "Processing data/bikes/10a Journey Data Extract 20Sep15-03Oct15.csv\n",
      "Processing data/bikes/10a. Journey Data Extract 14Sep14-27Sep14.csv\n",
      "Processing data/bikes/10b Journey Data Extract 04Oct15-17Oct15.csv\n",
      "Processing data/bikes/10b. Journey Data Extract 28Sep14-11Oct14.csv\n",
      "Processing data/bikes/11. Journey Data Extract 14Sep13-12Oct13.csv\n",
      "Processing data/bikes/11. Journey Data Extract 23Aug-25 Aug12.csv\n",
      "Processing data/bikes/11JourneyDataExtract22Jun2016-28Jun2016.csv\n",
      "Processing data/bikes/11a Journey Data Extract 18Oct15-31Oct15.csv\n",
      "Processing data/bikes/11a. Journey Data Extract 12Oct14-08Nov14.csv\n",
      "Processing data/bikes/11b Journey Data Extract 01Nov15-14Nov15.csv\n",
      "Processing data/bikes/11b. Journey Data Extract 12Oct14-08Nov14.csv\n",
      "Processing data/bikes/12. Journey Data Extract 13Oct13-09Nov13.csv\n",
      "Processing data/bikes/12. Journey Data Extract 26Aug-27 Aug12.csv\n",
      "Processing data/bikes/12JourneyDataExtract29Jun2016-05Jul2016.csv\n",
      "Processing data/bikes/12a Journey Data Extract 15Nov15-27Nov15.csv\n",
      "Processing data/bikes/12a. Journey Data Extract 09Nov14-06Dec14.csv\n",
      "Processing data/bikes/12b Journey Data Extract 28Nov15-12Dec15.csv\n",
      "Processing data/bikes/12b. Journey Data Extract 09Nov14-06Dec14.csv\n",
      "Processing data/bikes/13. Journey Data Extract 10Nov13-07Dec13.csv\n",
      "Processing data/bikes/13. Journey Data Extract 28Aug-29 Aug12.csv\n",
      "Processing data/bikes/13JourneyDataExtract06Jul2016-12Jul2016.csv\n",
      "Processing data/bikes/13a Journey Data Extract 13Dec15-24Dec15.csv\n",
      "Processing data/bikes/13a. Journey Data Extract 07Dec14-21Dec14.csv\n",
      "Processing data/bikes/13b Journey Data Extract 25Dec15-09Jan16.csv\n",
      "Processing data/bikes/13b. Journey Data Extract 22Dec14-03Jan15.csv\n",
      "Processing data/bikes/14. Journey Data Extract 08Dec13-04Jan14.csv\n",
      "Processing data/bikes/14. Journey Data Extract 30Aug-31 Aug12.csv\n",
      "Processing data/bikes/14JourneyDataExtract13Jul2016-19Jul2016.csv\n",
      "Processing data/bikes/15. Journey Data Extract 01Sep-30Sep12.csv\n",
      "Processing data/bikes/15JourneyDataExtract20Jul2016-26Jul2016.csv\n",
      "Processing data/bikes/16. Journey Data Extract 01Oct-31Oct12.csv\n",
      "Processing data/bikes/16JourneyDataExtract27Jul2016-02Aug2016.csv\n",
      "Processing data/bikes/17. Journey Data Extract 01Nov-30Nov12.csv\n",
      "Processing data/bikes/17JourneyDataExtract03Aug2016-09Aug2016.csv\n",
      "Processing data/bikes/18. Journey Data Extract 01Dec-31Dec12.csv\n",
      "Processing data/bikes/18JourneyDataExtract10Aug2016-16Aug2016.csv\n",
      "Processing data/bikes/19JourneyDataExtract17Aug2016-23Aug2016.csv\n",
      "Processing data/bikes/1a.JourneyDataExtract04Jan15-17Jan15.csv\n",
      "Processing data/bikes/1b.JourneyDataExtract18Jan15-31Jan15.csv\n",
      "Processing data/bikes/2. Journey Data Extract 03Feb14-01Mar14.csv\n",
      "Processing data/bikes/2. Journey Data Extract 06Jan-02Feb13.csv\n",
      "Processing data/bikes/2. Journey Data Extract_01Feb-29Feb 12.csv\n",
      "Processing data/bikes/20JourneyDataExtract24Aug2016-30Aug2016.csv\n",
      "Processing data/bikes/21JourneyDataExtract31Aug2016-06Sep2016.csv\n",
      "Processing data/bikes/22JourneyDataExtract07Sep2016-13Sep2016.csv\n",
      "Processing data/bikes/23JourneyDataExtract14Sep2016-20Sep2016.csv\n",
      "Processing data/bikes/24JourneyDataExtract21Sep2016-27Sep2016.csv\n",
      "Processing data/bikes/25JourneyDataExtract28Sep2016-04Oct2016.csv\n",
      "Processing data/bikes/26JourneyDataExtract05Oct2016-11Oct2016.csv\n",
      "Processing data/bikes/27JourneyDataExtract12Oct2016-18Oct2016.csv\n",
      "Processing data/bikes/28JourneyDataExtract19Oct2016-25Oct2016.csv\n",
      "Processing data/bikes/29JourneyDataExtract26Oct2016-01Nov2016.csv\n",
      "Processing data/bikes/2a.JourneyDataExtract01Feb15-14Feb15.csv\n",
      "Processing data/bikes/2b.JourneyDataExtract15Feb15-28Feb15.csv\n",
      "Processing data/bikes/3. Journey Data Extract 02Mar14-31Mar14.csv\n",
      "Processing data/bikes/3. Journey Data Extract 03Feb-02Mar13.csv\n",
      "Processing data/bikes/3. Journey Data Extract_01Mar-31Mar12.csv\n",
      "Processing data/bikes/30JourneyDataExtract02Nov2016-08Nov2016.csv\n",
      "Processing data/bikes/31JourneyDataExtract09Nov2016-15Nov2016.csv\n",
      "Processing data/bikes/32JourneyDataExtract16Nov2016-22Nov2016.csv\n",
      "Processing data/bikes/33JourneyDataExtract23Nov2016-29Nov2016.csv\n",
      "Processing data/bikes/34JourneyDataExtract30Nov2016-06Dec2016.csv\n",
      "Processing data/bikes/35JourneyDataExtract07Dec2016-13Dec2016.csv\n",
      "Processing data/bikes/36JourneyDataExtract14Dec2016-20Dec2016.csv\n",
      "Processing data/bikes/37JourneyDataExtract21Dec2016-27Dec2016.csv\n",
      "Processing data/bikes/38JourneyDataExtract28Dec2016-03Jan2017.csv\n",
      "Processing data/bikes/39JourneyDataExtract04Jan2017-10Jan2017.csv\n",
      "Processing data/bikes/3a.JourneyDataExtract01Mar15-15Mar15.csv\n",
      "Processing data/bikes/3b.JourneyDataExtract16Mar15-31Mar15.csv\n",
      "Processing data/bikes/4. Journey Data Extract 01Apr14-26Apr14.csv\n",
      "Processing data/bikes/4. Journey Data Extract 03Mar-31Mar13.csv\n",
      "Processing data/bikes/4. Journey Data Extract_1Apr-28Apr12.csv\n",
      "Processing data/bikes/40JourneyDataExtract11Jan2017-17Jan2017.csv\n",
      "Processing data/bikes/41JourneyDataExtract18Jan2017-24Jan2017.csv\n",
      "Processing data/bikes/42JourneyDataExtract25Jan2017-31Jan2017.csv\n",
      "Processing data/bikes/43JourneyDataExtract01Feb2017-07Feb2017.csv\n",
      "Processing data/bikes/44JourneyDataExtract08Feb2017-14Feb2017.csv\n",
      "Processing data/bikes/45JourneyDataExtract15Feb2017-21Feb2017.csv\n",
      "Processing data/bikes/46JourneyDataExtract22Feb2017-28Feb2017.csv\n",
      "Processing data/bikes/47JourneyDataExtract01Mar2017-07Mar2017.csv\n",
      "Processing data/bikes/48JourneyDataExtract08Mar2017-14Mar2017.csv\n",
      "Processing data/bikes/49JourneyDataExtract15Mar2017-21Mar2017.csv\n",
      "Processing data/bikes/4a.JourneyDataExtract01Apr15-16Apr15.csv\n",
      "Processing data/bikes/4b.JourneyDataExtract 17Apr15-02May15.csv\n",
      "Processing data/bikes/5. Journey Data Extract 01Apr-27Apr13.csv\n",
      "Processing data/bikes/5. Journey Data Extract 27Apr14-24May14.csv\n",
      "Processing data/bikes/5. Journey Data Extract_29Apr-26May12.csv\n",
      "Processing data/bikes/50 Journey Data Extract 22Mar2017-28Mar2017.csv\n",
      "Processing data/bikes/51 Journey Data Extract 29Mar2017-04Apr2017.csv\n",
      "Processing data/bikes/52 Journey Data Extract 05Apr2017-11Apr2017.csv\n",
      "Processing data/bikes/53JourneyDataExtract12Apr2017-18Apr2017.csv\n",
      "Processing data/bikes/54JourneyDataExtract19Apr2017-25Apr2017.csv\n",
      "Processing data/bikes/55JourneyData Extract26Apr2017-02May2017.csv\n",
      "Processing data/bikes/56JourneyDataExtract 03May2017-09May2017.csv\n",
      "Processing data/bikes/57JourneyDataExtract10May2017-16May2017.csv\n",
      "Processing data/bikes/5a.JourneyDataExtract03May15-16May15.csv\n",
      "Processing data/bikes/5b.JourneyDataExtract17May15-30May15.csv\n",
      "Processing data/bikes/6. Journey Data Extract 25May14-21Jun14.csv\n",
      "Processing data/bikes/6. Journey Data Extract 28Apr-25May13.csv\n",
      "Processing data/bikes/6. Journey Data Extract_27May-23Jun12.csv\n",
      "Processing data/bikes/6aJourneyDataExtract31May15-12Jun15.csv\n",
      "Processing data/bikes/6bJourneyDataExtract13Jun15-27Jun15.csv\n",
      "Processing data/bikes/7. Journey Data Extract 22Jun14-19Jul14.csv\n",
      "Processing data/bikes/7. Journey Data Extract 26May-22Jun13.csv\n",
      "Processing data/bikes/7. Journey Data Extract_24Jun-21Jul12.csv\n",
      "Processing data/bikes/7a.JourneyDataExtract28Jun15-11Jul15.csv\n",
      "Processing data/bikes/7b.JourneyDataExtract12Jul15-25Jul15.csv\n",
      "Processing data/bikes/8. Journey Data Extract 22Jul-18Aug12.csv\n",
      "Processing data/bikes/8. Journey Data Extract 23Jun-20Jul13.csv\n",
      "Processing data/bikes/8a Journey Data Extract 20Jul14-31Jul14.csv\n",
      "Processing data/bikes/8aJourneyDataExtract26Jul15-07Aug15.csv\n",
      "Processing data/bikes/8b Journey Data Extract 01Aug14-16Aug14.csv\n",
      "Processing data/bikes/8bJourneyData Extract 08Aug15-22Aug15.csv\n",
      "Processing data/bikes/9. Journey Data Extract 19Aug-20 Aug12.csv\n",
      "Processing data/bikes/9. Journey Data Extract 21Jul-17Aug13.csv\n",
      "Processing data/bikes/9a Journey Data Extract 17Aug14-31Aug14.csv\n",
      "Processing data/bikes/9a-Journey-Data-Extract-23Aug15-05Sep15.csv\n",
      "Processing data/bikes/9b Journey Data Extract 01Sep14-13Sep14.csv\n",
      "Processing data/bikes/9b-Journey-Data-Extract-06Sep15-19Sep15.csv\n",
      "Doing the problem cases (1 of them).\n",
      "data/bikes/21JourneyDataExtract31Aug2016-06Sep2016.csv\n"
     ]
    }
   ],
   "source": [
    "# Terminology for the code: An `event` is when a bike either arrives or leaves a station.\n",
    "\n",
    "\n",
    "def add_station_names(station_names, df, namecolumn, idcolumn):\n",
    "    \"\"\"Given a DataFrame df that has df[namecolumn] listing names of stations and\n",
    "    df[idcolumn] listing station ID numbers, add to the dictionary station_names\n",
    "    all the names that each ID is attached to.\n",
    "    \"\"\"\n",
    "    namemaps = (\n",
    "        df[[idcolumn, namecolumn]]\n",
    "        .groupby(idcolumn)\n",
    "        .aggregate(lambda x: x.unique())\n",
    "    )\n",
    "    for number, names in namemaps.iterrows():\n",
    "        current_names = station_names.get(number, set())\n",
    "        # The following two lines are a stupid dance around the annoying fact that pd.unique sometimes returns a single value,\n",
    "        # sometimes a numpy array of values, but since the single value is a string, it too is an iterable.\n",
    "        vals = names[0]\n",
    "        new_names = set([vals]) if type(vals) == str else set(vals)\n",
    "        current_names.update(new_names)\n",
    "        station_names[number] = current_names\n",
    "\n",
    "\n",
    "def clean_datetime_column(df, colname, roundto=\"H\"):\n",
    "    \"\"\"Parse df[colname] from strings to datetime objects, and round the\n",
    "    times to the nearest hour. Also chop off from df any rows with times before 2010-7-30 or\n",
    "    after 2020-1-1, since these are nonsense. df is partially modified in place, but the return value\n",
    "    should still be used.\n",
    "    \"\"\"\n",
    "    # A bit of a hacky way to use the first entry to figure out which date format this file uses.\n",
    "    # Not super robust, but works. TODO Improve this.\n",
    "    if len(df[colname].iloc[0]) > 16:\n",
    "        format = \"%d/%m/%Y %H:%M:%S\"\n",
    "    else:\n",
    "        format = \"%d/%m/%Y %H:%M\"\n",
    "    df[colname] = pd.to_datetime(df[colname], format=format)\n",
    "    df[colname] = df[colname].dt.round(roundto)\n",
    "    early_cutoff = pd.datetime(2010, 7, 30)  # When the program started.\n",
    "    late_cutoff = pd.datetime(2020, 1, 1)  # Approximately now.\n",
    "    df = df[(late_cutoff > df[colname]) & (df[colname] >= early_cutoff)]\n",
    "    return df\n",
    "\n",
    "\n",
    "def compute_single_events(df, which):\n",
    "    \"\"\"Read from df all the events, either starts or stops depending on `which`, and\n",
    "    collect them in a DataFrame that lists event counts per station and time.\n",
    "    \"\"\"\n",
    "    stationcol = \"{}Station Id\".format(which)\n",
    "    datecol = \"{} Date\".format(which)\n",
    "    events = (\n",
    "        df.rename(columns={stationcol: \"Station\", datecol: \"Date\"})\n",
    "        .groupby([\"Date\", \"Station\"])\n",
    "        .size()\n",
    "        .unstack(\"Station\")\n",
    "    )\n",
    "    return events\n",
    "\n",
    "\n",
    "def compute_both_events(df):\n",
    "    \"\"\"Read from df all the events, both starts and ends, and\n",
    "    collect them in a DataFrame that lists event counts per station and time.\n",
    "    \"\"\"\n",
    "    ends = compute_single_events(df, \"End\")\n",
    "    starts = compute_single_events(df, \"Start\")\n",
    "    both = (\n",
    "        pd.concat([ends, starts], keys=[\"End\", \"Start\"], axis=1)\n",
    "        .reorder_levels([1, 0], axis=1)\n",
    "        .fillna(0.0)\n",
    "    )\n",
    "    return both\n",
    "\n",
    "\n",
    "def castable_to_int(obj):\n",
    "    \"\"\"Return True if obj is castable to int, False otherwise.\"\"\"\n",
    "    try:\n",
    "        int(obj)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "\n",
    "def cast_to_int(df, colname):\n",
    "    \"\"\"Cast df[colname] to dtype int. All rows that are not castable\n",
    "    to int are dropped. df is partially modified in place, but the return\n",
    "    value should be used.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = df.astype({colname: np.int_}, copy=False)\n",
    "    except ValueError:\n",
    "        castable_rows = df[colname].apply(castable_to_int)\n",
    "        df = df[castable_rows]\n",
    "        df = df.astype({colname: np.int_}, copy=False)\n",
    "    return df\n",
    "\n",
    "\n",
    "# events_by_station is the DataFrame we are constructing. First check if it's already\n",
    "# on disk.\n",
    "events_by_station_path = Path(\"./events_by_station.p\")\n",
    "if events_by_station_path.exists():\n",
    "    events_by_station = pd.read_pickle(events_by_station_path)\n",
    "else:\n",
    "    # Collect the paths to all the CSV files.\n",
    "    datafiles = sorted(os.listdir(bikefolder))\n",
    "    folderpath = Path(bikefolder)\n",
    "    datapaths = [folderpath / Path(file) for file in datafiles]\n",
    "    datapaths = [p for p in datapaths if p.suffix == \".csv\"]\n",
    "\n",
    "    # Initialize a dictionary that will have as keys station ID numbers, and as values\n",
    "    # sets that include all the names this station has had in the files.\n",
    "    station_allnames = {}\n",
    "\n",
    "    # Each CSV file will list events in some time window. We process them one-by-one,\n",
    "    # collect all the DataFrames for individual time windows to `pieces`, and concatenate\n",
    "    # them at the end.\n",
    "    pieces = []\n",
    "    # Columns of the CSV files that we need.\n",
    "    cols = [\n",
    "        \"Duration\",\n",
    "        \"End Date\",\n",
    "        \"EndStation Id\",\n",
    "        \"EndStation Name\",\n",
    "        \"Start Date\",\n",
    "        \"StartStation Id\",\n",
    "        \"StartStation Name\",\n",
    "    ]\n",
    "    # At least one CSV file gives us trouble because it doesn't list station IDs, only station\n",
    "    # names. We'll collect the paths to those CSV files to `problem_paths` and deal with them\n",
    "    # at the end.\n",
    "    problem_paths = []\n",
    "    for path in datapaths:\n",
    "        print(\"Processing {}\".format(path))\n",
    "        try:\n",
    "            df = pd.read_csv(path, usecols=cols, encoding=\"ISO-8859-2\")\n",
    "        except ValueError as e:\n",
    "            # Some files have missing or abnormaly named columns. We'll deal with them later.\n",
    "            problem_paths.append(path)\n",
    "            continue\n",
    "        # Drop any rows that have missing values.\n",
    "        df = df[~df.isna().any(axis=1)]\n",
    "        # Drop any anomalously short trips. Probably somebody just taking a bike and putting\n",
    "        # it right back in. Durations are in seconds.\n",
    "        df = df[df[\"Duration\"] > 60]\n",
    "        # Cast the columns to the right types. This is easier ones NAs have been dropped.\n",
    "        df = cast_to_int(df, \"EndStation Id\")\n",
    "        df = cast_to_int(df, \"StartStation Id\")\n",
    "        # Turn the date columns from strings into datetime objects rounded to the hour.\n",
    "        df = clean_datetime_column(df, \"End Date\")\n",
    "        df = clean_datetime_column(df, \"Start Date\")\n",
    "        events = compute_both_events(df)\n",
    "        pieces.append(events)\n",
    "\n",
    "        # Add station names appearing in this file to our collection of names.\n",
    "        add_station_names(\n",
    "            station_allnames, df, \"EndStation Name\", \"EndStation Id\"\n",
    "        )\n",
    "        add_station_names(\n",
    "            station_allnames, df, \"StartStation Name\", \"StartStation Id\"\n",
    "        )\n",
    "\n",
    "    # Now that we've collected all the different names that the same station goes by,\n",
    "    # we'll pick one of them to be the name we'll use. We do this by just picking the\n",
    "    # one that is alphabetically first. We'll also make a dictionary that goes the other\n",
    "    # way around, for each name it gives the corresponding station ID.\n",
    "    station_ids = {}\n",
    "    station_names = {}\n",
    "    for k, v in station_allnames.items():\n",
    "        v = sorted(v)\n",
    "        station_names[k] = v[0]\n",
    "        for name in v:\n",
    "            station_ids[name] = k\n",
    "\n",
    "    def get_station_id(name):\n",
    "        try:\n",
    "            return station_ids[name]\n",
    "        except KeyError:\n",
    "            return np.nan\n",
    "\n",
    "    # Let's deal with the problem cases. They are ones that are missing station ID columns.\n",
    "    # They do have the station names though, so we'll use those to, with the above dictionary\n",
    "    # to get the IDs.\n",
    "    print(\"Doing the problem cases ({} of them).\".format(len(problem_paths)))\n",
    "    safe_cols = [\n",
    "        \"Duration\",\n",
    "        \"End Date\",\n",
    "        \"EndStation Name\",\n",
    "        \"Start Date\",\n",
    "        \"StartStation Name\",\n",
    "    ]\n",
    "    for path in problem_paths:\n",
    "        print(path)\n",
    "        df = pd.read_csv(path, usecols=safe_cols, encoding=\"ISO-8859-2\")\n",
    "        # Drop any rows that have missing values.\n",
    "        df = df[~df.isna().any(axis=1)]\n",
    "        # Drop any anomalously short trips. Probably somebody just taking a bike and putting\n",
    "        # it right back in.\n",
    "        df = df[df[\"Duration\"] > 60]\n",
    "        # Add a column of station IDs, based on names.\n",
    "        df[\"EndStation Id\"] = df[\"EndStation Name\"].apply(get_station_id)\n",
    "        df[\"StartStation Id\"] = df[\"StartStation Name\"].apply(get_station_id)\n",
    "        # Turn the date columns from strings into datetime objects rounded to the hour.\n",
    "        clean_datetime_column(df, \"End Date\")\n",
    "        clean_datetime_column(df, \"Start Date\")\n",
    "        events = compute_both_events(df)\n",
    "        pieces.append(events)\n",
    "\n",
    "    # Finally, concatenate all the data we've accumulated into a single DataFrame.\n",
    "    events_by_station = pd.concat(pieces).fillna(0.0)\n",
    "    # Several files may have contained entries for the same hour, which means that\n",
    "    # events_by_station has duplicate entries in the index. Get rid of them by summing.\n",
    "    events_by_station = events_by_station.groupby(\"Date\").sum().sort_index()\n",
    "    # Finally rename the columns according to the chosen names for stations.\n",
    "    events_by_station = events_by_station.rename(\n",
    "        mapper=station_names, axis=1, level=0\n",
    "    )\n",
    "\n",
    "    # Store the file on disk so we can read it later.\n",
    "    events_by_station.to_pickle(events_by_station_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's most of the processing done, but there's still a few nonsense stations in the data, used for testing purposes and so. Here's a handy way to find them: For each station, take the maximum number of events it has had in one hour, and divide that by the average number of events for the same station. This yields a high number for stations that have very peaked usage profiles, like these short-lived test stations do. Print some of the top stations by this metric to find their names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Station\n",
       "Electrical Workshop PS                                      1757.661290\n",
       "PENTON STREET COMMS TEST TERMINAL _ CONTACT MATT McNULTY    1615.675676\n",
       "tabletop1                                                   1066.166667\n",
       "Contact Centre, Southbury House                              785.873684\n",
       "6                                                            569.021661\n",
       "Pop Up Dock 1                                                567.806009\n",
       "Mechanical Workshop Penton                                   223.889301\n",
       "South Quay East, Canary Wharf                                133.832111\n",
       "Westfield Eastern Access Road, Shepherd's Bush                88.865062\n",
       "Thornfield House, Poplar                                      83.626863\n",
       "Fore Street Avenue: Guildhall                                 72.077569\n",
       "Upper Grosvenor Street, Mayfair                               71.536004\n",
       "Courland Grove , Wandsworth Road                              57.572555\n",
       "Manfred Road, East Putney                                     54.716094\n",
       "Aberfeldy Street, Poplar                                      54.258897\n",
       "Castalia Square :Cubitt Town                                  53.326767\n",
       "Stebondale Street, Cubitt Town                                53.173161\n",
       "Grant Road Central, Clapham Junction                          53.160678\n",
       "Here East South, Queen Elizabeth Olympic Park                 52.018072\n",
       "Teviot Street, Poplar                                         47.432214\n",
       "Stewart's Road, Nine Elms                                     47.168675\n",
       "Abbotsbury Road, Holland Park                                 46.499915\n",
       "Clarkson Street :Bethnal Green                                46.496873\n",
       "Malmesbury Road :Bow                                          45.607424\n",
       "Here East North, Queen Elizabeth Olympic Park                 45.450386\n",
       "Bevington Road, North Kensington                              45.329752\n",
       "Cantrell Road, Bow                                            45.218407\n",
       "Lord's, St. John's Wood                                       44.333229\n",
       "Birkenhead Street, King's Cross                               43.918274\n",
       "Knightsbridge, Hyde Park                                      43.819764\n",
       "dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mean_within_window(s):\n",
    "    starttime = s.ne(0).idxmax()\n",
    "    endtime = s[::-1].ne(0).idxmax()\n",
    "    return s[starttime:endtime].mean()\n",
    "\n",
    "\n",
    "a = events_by_station.sum(axis=1, level=0)\n",
    "(a.max() / a.aggregate(mean_within_window)).sort_values(ascending=False)[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I just drop the weird ones. No, I won't contact Matt McNulty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "improper_stations = [\n",
    "    \"Electrical Workshop PS\",\n",
    "    \"PENTON STREET COMMS TEST TERMINAL _ CONTACT MATT McNULTY\",\n",
    "    \"tabletop1\",\n",
    "    \"Pop Up Dock 1\",\n",
    "    \"6\",\n",
    "    \"Mechanical Workshop Penton\",\n",
    "]\n",
    "events_by_station = events_by_station.drop(improper_stations, axis=1, level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>End</th>\n",
       "      <th>Start</th>\n",
       "      <th>End</th>\n",
       "      <th>Start</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012-01-04 00:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-04 01:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-04 02:00:00</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-04 03:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-04 04:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-16 22:00:00</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-16 23:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-17 00:00:00</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-17 01:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-17 02:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>47043 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     End  Start  End  Start\n",
       "Date                                       \n",
       "2012-01-04 00:00:00  0.0    0.0  0.0    0.0\n",
       "2012-01-04 01:00:00  0.0    0.0  0.0    0.0\n",
       "2012-01-04 02:00:00  2.0    0.0  0.0    0.0\n",
       "2012-01-04 03:00:00  0.0    0.0  0.0    0.0\n",
       "2012-01-04 04:00:00  0.0    0.0  0.0    0.0\n",
       "...                  ...    ...  ...    ...\n",
       "2017-05-16 22:00:00  1.0    1.0  0.0    0.0\n",
       "2017-05-16 23:00:00  0.0    0.0  0.0    0.0\n",
       "2017-05-17 00:00:00  1.0    0.0  0.0    0.0\n",
       "2017-05-17 01:00:00  0.0    0.0  0.0    0.0\n",
       "2017-05-17 02:00:00  0.0    0.0  0.0    0.0\n",
       "\n",
       "[47043 rows x 4 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO What is up with this?\n",
    "events_by_station[\"Exhibition Road Museums, Knightsbridge\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the data set cleaned, let's explore it a bit before starting our main task of prediction. I pick a few stations as examples, and plot their usage profile, first over the hours of a week, averaged over all weeks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = events_by_station.index.to_series()\n",
    "example_stations = [\n",
    "    \"Waterloo Station 3, Waterloo\",\n",
    "    \"Hyde Park Corner, Hyde Park\",\n",
    "    \"Wenlock Road , Hoxton\",\n",
    "    \"Stonecutter Street, Holborn\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d93309648774111bd06dc08e4ed9f64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO Give the plots widths from variance or something like [25%, 75%] limits (what are these called again?).\n",
    "example_means_over_week = (\n",
    "    events_by_station[example_stations]\n",
    "    .groupby([times.dt.weekday, times.dt.hour])\n",
    "    .mean()\n",
    ")\n",
    "# Format the DataFrame into a format that seaborn likes.\n",
    "example_means_over_week.index.rename([\"Day\", \"Hour\"], inplace=True)\n",
    "example_means_over_week = (\n",
    "    example_means_over_week.stack(level=[0, 1])\n",
    "    .reset_index()\n",
    "    .rename(columns={\"level_3\": \"End/Start\", 0: \"Count\"})\n",
    ")\n",
    "example_means_over_week[\"Weekday\"] = example_means_over_week.apply(\n",
    "    lambda x: x[\"Day\"] + x[\"Hour\"] / 24, axis=1,\n",
    ")\n",
    "g = sns.FacetGrid(\n",
    "    example_means_over_week,\n",
    "    col_wrap=1,\n",
    "    aspect=2,\n",
    "    col=\"Station\",\n",
    "    hue=\"End/Start\",\n",
    "    sharey=False,\n",
    "    sharex=True,\n",
    ")\n",
    "g.map(plt.plot, \"Weekday\", \"Count\").set_titles(\"{col_name}\")\n",
    "g.add_legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The week starts from Monday in this plot.\n",
    "\n",
    "The example stations here we deliberately picked to show different kinds of trends. Stonecutter Street and Waterloo are clearly commuter stations, that see action in the morning and the afternoon on weekdays. One is the end point of commuting trips, the other the start. Hyde Park Corner is very different, with popularity peaks being in the afternoon, and especially weekend afternoons. Wenlock Road is a bitter of mixture of these features, gets much less traffic overall.\n",
    "\n",
    "Let's do the same thing over a year instead of a week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40f1d191afcf451e97289088f47d6d52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO Give the plots widths from variance or something like [25%, 75%] limits (what are these called again?).\n",
    "example_means_over_year = (\n",
    "    events_by_station[example_stations].groupby(times.dt.week).mean()\n",
    ")\n",
    "# Leave out the first and last weeks, since they are usually shorter and thus the data isn't comparable.\n",
    "example_means_over_year = example_means_over_year.iloc[1:51]\n",
    "# Format to what seaborn likes.\n",
    "example_means_over_year.index.rename(\"Week\", inplace=True)\n",
    "example_means_over_year = (\n",
    "    example_means_over_year.stack(level=[0, 1])\n",
    "    .reset_index()\n",
    "    .rename(columns={\"level_2\": \"End/Start\", 0: \"Count\"})\n",
    ")\n",
    "g = sns.FacetGrid(\n",
    "    example_means_over_year,\n",
    "    col_wrap=2,\n",
    "    col=\"Station\",\n",
    "    hue=\"End/Start\",\n",
    "    sharey=False,\n",
    "    sharex=True,\n",
    ")\n",
    "g.map(plt.plot, \"Week\", \"Count\").set_titles(\"{col_name}\")\n",
    "g.add_legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, different profiles, with Waterloo and Stonecutter, the commuter stations, being more even over the year (note different vertical scales), and Hyde Park Corner showing a huge preference for summer cycling.\n",
    "\n",
    "Finally, in addition to weekly and seasonal trends, the stations have different long term trends. Here are the yearly rolling averages over our whole data period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43dfd478e2e84d22aedb0deb566fe42a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/markus/.local/lib/python3.7/site-packages/pandas/plotting/_matplotlib/converter.py:103: FutureWarning: Using an implicitly registered datetime converter for a matplotlib plotting method. The converter was registered by pandas on import. Future versions of pandas will require you to explicitly register matplotlib converters.\n",
      "\n",
      "To register the converters:\n",
      "\t>>> from pandas.plotting import register_matplotlib_converters\n",
      "\t>>> register_matplotlib_converters()\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "yearly_rolling = (\n",
    "    events_by_station.loc[:, example_stations]\n",
    "    .rolling(\"365d\", min_periods=24 * 90)\n",
    "    .mean()\n",
    ")\n",
    "yearly_rolling = (\n",
    "    yearly_rolling.stack(level=[0, 1])\n",
    "    .reset_index()\n",
    "    .rename(columns={\"level_2\": \"End/Start\", 0: \"Count\"})\n",
    ")\n",
    "g = sns.FacetGrid(\n",
    "    yearly_rolling,\n",
    "    col_wrap=2,\n",
    "    col=\"Station\",\n",
    "    hue=\"End/Start\",\n",
    "    sharey=False,\n",
    "    sharex=True,\n",
    ")\n",
    "g.map(plt.plot, \"Date\", \"Count\").set_titles(\"{col_name}\")\n",
    "g.add_legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some rising and falling trends, and for some reason a period of time when people stopped arriving at Waterloo Station with their Boris Bikes, but didn't stop departing from there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation and features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The point of the above exploratory plots was to get a feel for the kind of trends we can expect to see, and to illustrate the differences between stations. Now that we start doing modelling and prediction, we'll always fit all our models per station and event type. I call a pair like (\"Waterloo Station 3, Waterloo\", \"Start\") a column, and so we'll be fitting to each column separately. We'll use some of the above example columns as our validation set.\n",
    "\n",
    "Before we start doing predictions, we should decide on how success is measured, and set up a framework for measuring performance of models. I choose to use a type of cross-validation often done with time series data, sometimes known as rolling cross-validation. In it you divide the data into consecutive chunks, each of which acts as a test set. For each test set, you train the model on all the data you have from before that test set. This is like the usual kind of cross-validation, except we never train on data from the future of the test set, to not accidentally create wormholes and disrupt the spacetime by having effects from the future affect our prediction.\n",
    "\n",
    "To have the validation run in a reasonable time, I choose to predict half a year at a time, and only start predictions after two years of data has been accumulated, because otherwise there isn't much to train on.\n",
    "\n",
    "Finally, we need to decide on an error metric. I choose to use the mean absolute error, aka MAE. Why MAE, and not mean squared error, MSE? Because a bike is a bike is a bike. If we predict that there will be 10 departures, and there are in fact 20, causing an error of 10 bikes, this is only 10 times worse than if we had predicted 19 and made an error of 1 bike. With MSE, this would be treated as being a 100 times worse, which just isn't fair: Imagining a case where our predicting model is used for deciding how many bikes should be kept at each station, one missing bike means one unhappy person, ten means ten, not a hundred. This is especially important when comparing predictions between different stations, some of which see much more trafic than others.\n",
    "\n",
    "Below is a class that implements the above type of cross-validation. There's one class that does this for a single column of data, and another that uses the first one to do multiple columns at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RollingValidatorSingleColumn:\n",
    "    \"\"\"A class for doing rolling cross-validation for data from a single column.\"\"\"\n",
    "    def __init__(\n",
    "        self, data, predictors, min_training_time, prediction_time,\n",
    "    ):\n",
    "        # Each validator keeps a dictionary of all the modelclasses it has tested, and the\n",
    "        # results that it got.\n",
    "        self.models = {}\n",
    "        # A cv_batch is a cross-validation batch, consisting of a set of training data,\n",
    "        # training predictors, test data, and test predictors. We slice up the data given\n",
    "        # to as many batches as we can, given the size of the test sets we want, and the\n",
    "        # minimum amount of data we need for training set to make sense. We start the slicing\n",
    "        # from the end, so even the shortest training set may be longer than the minimum\n",
    "        # required.\n",
    "        self.cv_batches = []\n",
    "        first_time = data.index.min()\n",
    "        last_time = data.index.max()\n",
    "        test_end_time = last_time\n",
    "        cutoff = test_end_time - prediction_time\n",
    "        while cutoff > first_time + min_training_time:\n",
    "            training_data = data[:cutoff]\n",
    "            training_predictors = predictors[:cutoff]\n",
    "            test_data = data[cutoff:test_end_time]\n",
    "            test_predictors = predictors[cutoff:test_end_time]\n",
    "            self.cv_batches.append(\n",
    "                (\n",
    "                    training_data,\n",
    "                    training_predictors,\n",
    "                    test_data,\n",
    "                    test_predictors,\n",
    "                )\n",
    "            )\n",
    "            test_end_time = cutoff\n",
    "            cutoff = test_end_time - prediction_time\n",
    "        msg = \"Created a RollingValidator with {} cross-validation batches.\".format(\n",
    "            len(self.cv_batches)\n",
    "        )\n",
    "        print(msg)\n",
    "\n",
    "    def test_modelclass(self, modelclass, print_progress=False):\n",
    "        \"\"\"Test a given modelclass with this RollingValidator. A modelclass should be class\n",
    "        the instances of which have the methods `train` and `predict`. `train` in training data\n",
    "        and training predictors, and predict takes in predictors and returns predictions.\n",
    "        \n",
    "        A separate instance of the model class is created for each cross-validation batch,\n",
    "        trained and asked to predict on the test set. It is also asked to predict on the \n",
    "        training set. Errors are computed for both. All predictions and errors are stored\n",
    "        in self.models[modelclass.classname]. The method returns the MAE over all the\n",
    "        cross-validation batches.\n",
    "        \"\"\"\n",
    "        # We collect the predictions and errors from different batches.\n",
    "        test_errors = []\n",
    "        training_errors = []\n",
    "        test_predictions = []\n",
    "        training_predictions = []\n",
    "        for (i, cv_batch,) in enumerate(self.cv_batches):\n",
    "            (\n",
    "                training_data,\n",
    "                training_predictors,\n",
    "                test_data,\n",
    "                test_predictors,\n",
    "            ) = cv_batch\n",
    "            if print_progress:\n",
    "                print(\"Training for batch {}.\".format(i))\n",
    "            model = modelclass()\n",
    "            model.train(\n",
    "                training_data, training_predictors,\n",
    "            )\n",
    "            if print_progress:\n",
    "                print(\"Predicting for batch {}.\".format(i))\n",
    "            test_prediction = model.predict(test_predictors)\n",
    "            training_prediction = model.predict(training_predictors)\n",
    "            test_error = test_prediction - test_data\n",
    "            training_error = training_prediction - training_data\n",
    "            test_errors.append(test_error)\n",
    "            training_errors.append(training_error)\n",
    "            test_predictions.append(test_prediction)\n",
    "            training_predictions.append(training_prediction)\n",
    "        test_mae = pd.concat(test_errors).abs().mean()\n",
    "        training_mae = pd.concat(training_errors).abs().mean()\n",
    "        self.models[modelclass.classname] = {\n",
    "            \"test_mae\": test_mae,\n",
    "            \"training_mae\": training_mae,\n",
    "            \"test_errors\": test_errors,\n",
    "            \"training_errors\": training_errors,\n",
    "            \"test_predictions\": test_predictions,\n",
    "            \"training_predictions\": training_predictions,\n",
    "        }\n",
    "        return test_mae\n",
    "\n",
    "\n",
    "class RollingValidator:\n",
    "    \"\"\"A class for doing rolling cross-validation for data from a multiple columns.\n",
    "    Each column is run individually, using RollingValidatorSingleColumn.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        data,\n",
    "        common_predictors,\n",
    "        specific_predictors,\n",
    "        min_training_time,\n",
    "        prediction_time,\n",
    "    ):\n",
    "        \"\"\"Initialization of a multicolumn RollingValidator takes, in addition\n",
    "        to data to fit to and predictors common to all stations, also an argument\n",
    "        called `specific_predictors`. This one should be a dictionary or DataFrame\n",
    "        with one entry/column for each column in data, that holds predictors specific\n",
    "        to that column only. It can also be None, in which case only the common\n",
    "        predictors are used.\"\"\"\n",
    "        self.models = {}\n",
    "        self.subrvs = {}\n",
    "        # Create a RollingValidatorSingleColumn for each column in data.\n",
    "        for c in data.columns:\n",
    "            data_c = pd.DataFrame(data[c])\n",
    "            if specific_predictors is not None and c in specific_predictors:\n",
    "                predictors_c = pd.concat(\n",
    "                    [common_predictors, specific_predictors[c]], axis=1\n",
    "                )\n",
    "            else:\n",
    "                predictors_c = common_predictors\n",
    "            subrv = RollingValidatorSingleColumn(\n",
    "                data_c, predictors_c, min_training_time, prediction_time\n",
    "            )\n",
    "            self.subrvs[c] = subrv\n",
    "\n",
    "    def test_modelclass(self, modelclass, print_progress=False):\n",
    "        \"\"\"Test a given modelclass with this RollingValidator. This runs\n",
    "        RollingValidatorSingleColumn.test_modelclass for each column individually,\n",
    "        and collects the result to a single dictionary self.models. It returns\n",
    "        the sum of the MAEs for each column.\n",
    "        \"\"\"\n",
    "        for c, subrv in self.subrvs.items():\n",
    "            if print_progress:\n",
    "                print(\"Running RV on {}.\".format(c))\n",
    "            subrv.test_modelclass(modelclass, print_progress=print_progress)\n",
    "        # Collect the results into DataFrames that have different columns for the\n",
    "        # different columns in the original data.\n",
    "        classname = modelclass.classname\n",
    "        self.models[classname] = {}\n",
    "        for k in (\"test_mae\", \"training_mae\"):\n",
    "            self.models[classname][k] = pd.concat(\n",
    "                [subrv.models[classname][k] for subrv in self.subrvs.values()]\n",
    "            )\n",
    "        # The next(iter( part just takes the first of the entries in .values().\n",
    "        num_batches = len(next(iter(self.subrvs.values())).cv_batches)\n",
    "        for k in (\n",
    "            \"test_errors\",\n",
    "            \"training_errors\",\n",
    "            \"test_predictions\",\n",
    "            \"training_predictions\",\n",
    "        ):\n",
    "            self.models[classname][k] = [\n",
    "                pd.concat(\n",
    "                    [\n",
    "                        subrv.models[classname][k][i]\n",
    "                        for subrv in self.subrvs.values()\n",
    "                    ],\n",
    "                    axis=1,\n",
    "                )\n",
    "                for i in range(num_batches)\n",
    "            ]\n",
    "        test_mae = self.models[classname][\"test_mae\"].sum()\n",
    "        return test_mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a framework for cross-validating, and we know what we want to predict, what should we give our models as predictors? There are at least two kinds of predictors we could use: Seasonal and autoregressive. By seasonal I mean things like which weekday is it, and what time it is. By autoregressive I mean past values in the same column.\n",
    "\n",
    "Let's talk about seasonal ones first. As we saw above, there are strong variations in bike usage by the hour and weekday, and some clear variation also over the year. We could encode this is several different ways. The weekdays, especially weekend vs Monday to Friday, are clearly very distinct, and since there aren't two many of them either, dummy encoding seems like a good idea: 7 predictor variables that are 1 or 0 depending on whether it's that day of the week or not. For the intraday and intrayear patterns, I try two different encodings, dummy encoding and trigonometric. In the first one, each hour of the day and each month of the year gets it's own dummy variable. We could also consider doing dummies for weeks of the year instead of months, but let's try to avoid getting a huge number of predictor variables, for performance reasons if not otherwise. For the trigonometric encoding I turn the hours of the day and weeks of a year into angles around a circle, and then create two predictor variables, the sine and the cosine of that angle. They create a simple but natural encoding of continuous cyclical variables, and combined together they don't favor any one part of the cycle over others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with all the dummy encoded predictors.\n",
    "weekday_dummies = pd.get_dummies(times.dt.weekday_name)\n",
    "month_dummies = pd.get_dummies(times.dt.month)\n",
    "hour_dummies = pd.get_dummies(times.dt.hour)\n",
    "hour_dummies = hour_dummies.rename(\n",
    "    columns={c: \"Hour {}\".format(c) for c in hour_dummies.columns}\n",
    ")\n",
    "month_dummies = month_dummies.rename(\n",
    "    columns={c: \"Month {}\".format(c) for c in month_dummies.columns}\n",
    ")\n",
    "predictors_dum = pd.concat(\n",
    "    [month_dummies, hour_dummies, weekday_dummies,], axis=1,\n",
    ")\n",
    "\n",
    "# Create a DataFrame with weekdays dummy encoded, but weeks and hours\n",
    "# trigonometricly encoded.\n",
    "day_angles = 2 * np.pi * times.dt.hour / 24\n",
    "day_trigonometrics = pd.DataFrame(\n",
    "    {\"Day sine\": np.sin(day_angles), \"Day cosine\": np.cos(day_angles)}\n",
    ")\n",
    "year_angles = 2 * np.pi * times.dt.week / 52\n",
    "year_trigonometrics = pd.DataFrame(\n",
    "    {\"Year sine\": np.sin(year_angles), \"Year cosine\": np.cos(year_angles),}\n",
    ")\n",
    "predictors_trig = pd.concat(\n",
    "    [weekday_dummies, day_trigonometrics, year_trigonometrics], axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine you are tasked to be the human model predicting bike use. You've learned that for a given station, one Mondays in January at 9am you usually get around 50 people departing. But now for the past three weeks straight, you've seen much less customers than usually. You should probably adjust your estimates then.\n",
    "\n",
    "This gets to the autoregressive features. You could come up with any number of these: a weekly rolling average, monthly rolling average, usage at the same time of the day for three previous days, etc. We shouldn't include everything though, to avoid overfitting. What I've found works quite well, is to simple give as a predictor the value from the same column exactly a week before. The intraweek patterns are the strongest, fastest moving patterns in the data, and also very reliable, and looking at the usage from exactly a week ago gives pretty good idea of what to expect this time. The only downside is that we have to discard the first week of data since we don't have preceding data for that, but we've got hundreds of weeks left still.\n",
    "\n",
    "Finally, there are more than a thousand columns in our data, and I don't have the patience to run the every model on all of them just to test it. We'll pick three example columns as our validation set of columns, that represent different types of stations as seen in the above extrapolation plots: Waterloo to represent commuter-heavy stations, Hyde Park to represent leisure rides, and Wenlock Road to represent a quieter station with a mix of users. This also gives us a natural way to separate validation from testing: We'll do model selecting using these three columns, and then at the very end test performance using a different set of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data from exactly a week before.\n",
    "events_offset = events_by_station.shift(freq=pd.Timedelta(\"7d\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discard the first week, so that all our predictors and data cover the same\n",
    "# timespan.\n",
    "first_time = events_offset.index.min()\n",
    "last_time = events_by_station.index.max()\n",
    "data = events_by_station.loc[first_time:last_time, :]\n",
    "predictors_trig = predictors_trig.loc[first_time:last_time, :]\n",
    "predictors_dum = predictors_dum.loc[first_time:last_time, :]\n",
    "times = data.index.to_series()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_columns = [\n",
    "    (\"Waterloo Station 3, Waterloo\", \"Start\"),\n",
    "    (\"Hyde Park Corner, Hyde Park\", \"End\"),\n",
    "    (\"Wenlock Road , Hoxton\", \"End\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created a RollingValidator with 6 cross-validation batches.\n",
      "Created a RollingValidator with 6 cross-validation batches.\n",
      "Created a RollingValidator with 6 cross-validation batches.\n",
      "Created a RollingValidator with 6 cross-validation batches.\n",
      "Created a RollingValidator with 6 cross-validation batches.\n",
      "Created a RollingValidator with 6 cross-validation batches.\n"
     ]
    }
   ],
   "source": [
    "specific_predictors = events_offset[test_columns]\n",
    "min_training_time = 2 * pd.Timedelta(\"365d\")\n",
    "prediction_time = 0.5 * pd.Timedelta(\"365d\")\n",
    "rv_trig = RollingValidator(\n",
    "    data[test_columns],\n",
    "    predictors_trig,\n",
    "    specific_predictors,\n",
    "    min_training_time,\n",
    "    prediction_time,\n",
    ")\n",
    "rv_dum = RollingValidator(\n",
    "    data[test_columns],\n",
    "    predictors_dum,\n",
    "    specific_predictors,\n",
    "    min_training_time,\n",
    "    prediction_time,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR DEBUGGING\n",
    "# dbg_timerange = slice(\"2017-01-01\", None)\n",
    "# min_training_time = 2 * pd.Timedelta(\"30d\")\n",
    "# prediction_time = 0.5 * pd.Timedelta(\"30d\")\n",
    "# rv_trig = RollingValidator(\n",
    "#     data[dbg_timerange],\n",
    "#     predictors_trig[dbg_timerange],\n",
    "#     specific_predictors[dbg_timerange],\n",
    "#     min_training_time,\n",
    "#     prediction_time,\n",
    "# )\n",
    "# rv_dum = RollingValidator(\n",
    "#     data[dbg_timerange],\n",
    "#     predictors_dum[dbg_timerange],\n",
    "#     specific_predictors[dbg_timerange],\n",
    "#     min_training_time,\n",
    "#     prediction_time,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With validation set up, time to make our first models. To set a baseline, here two very simple ones: `SimpleMean` just takes the mean of all the event counts in the column in question, and predicts that usage at all times will just be the mean value. `LastWeek` always predicts that usage now will be exactly the same as a week ago. We run our RollingValidator on both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMean:\n",
    "    classname = \"SimpleMean\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "\n",
    "    def train(self, data, predictors):\n",
    "        mean = pd.DataFrame(data.mean())\n",
    "        self.model = mean\n",
    "\n",
    "    def predict(self, predictors):\n",
    "        mean = self.model\n",
    "        index = predictors.index\n",
    "        predictions = mean.T.apply(lambda x: [x[0]] * len(index)).set_index(\n",
    "            index\n",
    "        )\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LastWeek:\n",
    "    classname = \"LastWeek\"\n",
    "\n",
    "    def train(self, data, predictors):\n",
    "        pass\n",
    "\n",
    "    def predict(self, predictors):\n",
    "        # TODO Fix relying on column order\n",
    "        predictions = pd.DataFrame(predictors.iloc[:, -1])\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleMean\n",
      "MAE: 25.581\n",
      "LastWeek\n",
      "MAE: 12.239\n"
     ]
    }
   ],
   "source": [
    "for modelclass in [SimpleMean, LastWeek]:\n",
    "    print(modelclass.classname)\n",
    "    err = rv_trig.test_modelclass(modelclass)\n",
    "    print(\"MAE: {:.3f}\".format(err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That gives an idea of what we can hope for. If we get a MAE of more than 25 means that something's going badly wrong. If we are above 12 or so, we aren't doing anything especially smart, a simple same-as-last-week prediction is just as good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn offers easy-to-use classes for a lot of different kinds of regression models, and they are wonderfully easy to swap between, so let's make use of that. Here's a class that can be subclassed to run practically any scikit-learn regressor.\n",
    "\n",
    "Note here that we should think about data normalization a bit. Most of our predictors come normalized already: They are dummy variables, or sinusoidal functions in the range [-1,+1]. But the data from a week before does not, and this is an issue for some regressors. Below we normalize this predictor variable by just dividing by the standard deviation. It brings it into a range comparable with the other predictors, without losing the property that the data is inherently non-negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenericModel:\n",
    "    # Place-holders for subclasses to replace.\n",
    "    # Regressor should be a function that returns a scikit-learn regressor.\n",
    "    # classname is a string to be used as a key when referring to this class.\n",
    "    regressor = None\n",
    "    classname = None\n",
    "\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.column_name = None\n",
    "        self.stds = {}\n",
    "\n",
    "    def _normalize_predictors_train(self, predictors):\n",
    "        \"\"\"Normalize given predictors. predictors shoud be a dataframe, and every column\n",
    "        that has \"Start\" or \"End\" in the name will be divided by its standard deviation.\n",
    "        This function is meant to be called before training the model, and we store the\n",
    "        standard deviations, so that the same normalization can be applied to when doing\n",
    "        prediction.\n",
    "        \"\"\"\n",
    "        predictors = predictors.copy()\n",
    "        self.stds = {}\n",
    "        for c in predictors.columns:\n",
    "            try:\n",
    "                is_stationcolumn = \"Start\" in c or \"End\" in c\n",
    "            except TypeError:\n",
    "                is_stationcolumn = False\n",
    "            if is_stationcolumn:\n",
    "                col = predictors[c]\n",
    "                std = col.std()\n",
    "                predictors[c] = col / std\n",
    "                self.stds[c] = std\n",
    "        return predictors\n",
    "\n",
    "    def _normalize_predictors_predict(self, predictors):\n",
    "        \"\"\"Normalize the given predictors the same way they were normalized before training.\n",
    "        Meant to be called before predicting.\n",
    "        \"\"\"\n",
    "        predictors = predictors.copy()\n",
    "        for c in predictors.columns:\n",
    "            try:\n",
    "                is_stationcolumn = \"Start\" in c or \"End\" in c\n",
    "            except TypeError:\n",
    "                is_stationcolumn = False\n",
    "            if is_stationcolumn:\n",
    "                col = predictors[c]\n",
    "                std = self.stds[c]\n",
    "                predictors[c] = col / std\n",
    "        return predictors\n",
    "\n",
    "    def train(self, data, predictors):\n",
    "        predictors = self._normalize_predictors_train(predictors)\n",
    "        model = self.regressor()\n",
    "        model.fit(predictors, data)\n",
    "        self.model = model\n",
    "        self.column_name = data.columns[0]\n",
    "\n",
    "    def predict(self, predictors):\n",
    "        predictors = self._normalize_predictors_predict(predictors)\n",
    "        predictions = self.model.predict(predictors)\n",
    "        name = self.column_name\n",
    "        index = predictors.index\n",
    "        predictions = np.squeeze(predictions)\n",
    "        predictions = pd.DataFrame({name: predictions}, index=index)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the above, we construct a number of model classes using different regressors scikit-learn offers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(GenericModel):\n",
    "    classname = \"Linear\"\n",
    "    regressor = linear_model.Ridge\n",
    "\n",
    "class KNeighbors(GenericModel):\n",
    "    classname = \"KNeighbors\"\n",
    "    regressor = lambda x: neighbors.KNeighborsRegressor(\n",
    "        n_neighbors=5, weights=\"distance\"\n",
    "    )\n",
    "\n",
    "class LinearSVR(GenericModel):\n",
    "    classname = \"LinearSVR\"\n",
    "    regressor = lambda x: svm.LinearSVR(max_iter=5000)\n",
    "\n",
    "class RbfSVR(GenericModel):\n",
    "    classname = \"RbfSVR\"\n",
    "    regressor = lambda x: svm.SVR(kernel=\"rbf\", cache_size=500)\n",
    "\n",
    "\n",
    "class PolySVR(GenericModel):\n",
    "    classname = \"PolySVR\"\n",
    "    regressor = lambda x: svm.SVR(kernel=\"poly\", cache_size=500)\n",
    "\n",
    "\n",
    "class DecisionTree(GenericModel):\n",
    "    classname = \"DecisionTree\"\n",
    "    regressor = lambda x: tree.DecisionTreeRegressor(\n",
    "        criterion=\"mae\", max_depth=3\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A regularized linear model, k-nearest-neighbors with k=5, three support vector machines with different kernel functions, and a depth=3 decision tree. Why these? No strong reason, but I've been testing out different regressors and these all offer something different from the others conceptually, run on our data in a reasonable amount of time, and produce decent results. I am not an expert on any of these (on the contrary I actually hadn't used anything other than linear regression before this project), and I haven't done any systematic hyperparameter tuning, so it's well possible that better results could be found with some different model or choice of parameters. I'll go with these for now though.\n",
    "\n",
    "Here's a script that runs all the above models using both the trigonometric and the dummy encoding of our predictors. This takes a good while, on my laptop a few hours. Since the Jupyter kernel has a habit of crashing and we don't want to rerun this, I store the results in two files in the current working directory, one for each predictor encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleMean\n",
      "trig mae: 25.587   (took 0.0 mins)\n",
      "dumm mae: 25.587   (took 0.0 mins)\n",
      "\n",
      "LastWeek\n",
      "trig mae: 12.285   (took 0.0 mins)\n",
      "dumm mae: 12.285   (took 0.0 mins)\n",
      "\n",
      "Linear\n",
      "trig mae: 12.537   (took 0.0 mins)\n",
      "dumm mae: 12.615   (took 0.0 mins)\n",
      "\n",
      "KNeighbors\n",
      "trig mae: 11.346   (took 0.2 mins)\n",
      "dumm mae: 11.491   (took 30.2 mins)\n",
      "\n",
      "LinearSVR\n",
      "trig mae: 11.496   (took 0.1 mins)\n",
      "dumm mae: 11.361   (took 0.1 mins)\n",
      "\n",
      "DecisionTree\n",
      "trig mae: 11.586   (took 5.6 mins)\n",
      "dumm mae: 11.724   (took 21.7 mins)\n",
      "\n",
      "SVRrbf\n",
      "trig mae: 10.800   (took 16.9 mins)\n",
      "dumm mae: 10.915   (took 69.0 mins)\n",
      "\n",
      "SVRpoly\n",
      "trig mae: 10.553   (took 16.9 mins)\n",
      "dumm mae: 10.686   (took 72.9 mins)\n",
      "\n",
      "DiffModel(SimpleMean)\n",
      "trig mae: 12.300   (took 0.0 mins)\n",
      "dumm mae: 12.300   (took 0.0 mins)\n",
      "\n",
      "DiffModel(Linear)\n",
      "trig mae: 12.436   (took 0.0 mins)\n",
      "dumm mae: 13.029   (took 0.0 mins)\n",
      "\n",
      "DiffModel(KNeighbors)\n",
      "trig mae: 14.328   (took 0.2 mins)\n",
      "dumm mae: 14.316   (took 31.5 mins)\n",
      "\n",
      "DiffModel(LinearSVR)\n",
      "trig mae: 12.285   (took 0.0 mins)\n",
      "dumm mae: 12.296   (took 0.0 mins)\n",
      "\n",
      "DiffModel(DecisionTree)\n",
      "trig mae: 12.294   (took 5.5 mins)\n",
      "dumm mae: 12.284   (took 26.8 mins)\n",
      "\n",
      "DiffModel(SVRrbf)\n",
      "trig mae: 12.341   (took 14.5 mins)\n",
      "dumm mae: 12.452   (took 68.2 mins)\n",
      "\n",
      "DiffModel(SVRpoly)\n",
      "trig mae: 12.341   (took 12.1 mins)\n",
      "dumm mae: 12.513   (took 66.3 mins)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classes = [\n",
    "    SimpleMean,\n",
    "    LastWeek,\n",
    "    Linear,\n",
    "    KNeighbors,\n",
    "    LinearSVR,\n",
    "    DecisionTree,\n",
    "    RbfSVR,\n",
    "    PolySVR,\n",
    "]\n",
    "for modelclass in classes:\n",
    "    print(modelclass.classname)\n",
    "    for rv, rv_name in ((rv_trig, \"trig\"), (rv_dum, \"dumm\")):\n",
    "        start = timer()\n",
    "        try:\n",
    "            err = rv.test_modelclass(modelclass)\n",
    "        except Exception as e:\n",
    "            # We don't want to kill the whole script if something grows wrong\n",
    "            # with one model, so we just print the error and keep going.\n",
    "            print(e)\n",
    "        stop = timer()\n",
    "        time = (stop - start) / 60\n",
    "        print(\"{} mae: {:.3f}   (took {:.1f} mins)\".format(rv_name, err, time))\n",
    "        with open(\"latest_rv_dum.p\", \"wb\") as f:\n",
    "            pickle.dump(rv_dum, f)\n",
    "        with open(\"latest_rv_trig.p\", \"wb\") as f:\n",
    "            pickle.dump(rv_trig, f)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you need it, here's how you load the results from the disk if your kernel dies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"latest_rv_dum.p\", \"rb\") as f:\n",
    "    rv_dum = pickle.load(f)\n",
    "with open(\"latest_rv_trig.p\", \"rb\") as f:\n",
    "    rv_trig = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear model does no better than our simple `LastWeek` thing, which isn't too surprising: For instance with the trigonometrically encoded variables, it can only ever fit sinusoidal patterns in the data. All the non-linear ones do better, but none by a huge margin. In fact, their performance is strikingly similar. k-nearest-neighbors, which is essentially just a more sophisticad version of just predicting past values reoccurring, improves over `LastWeek` by an average of one bike, linear SVR and our choice of decision tree do no better. The SVRs with non-linear kernels still improve a bit on this, but nothing drastic.\n",
    "\n",
    "The intuitive picture I draw from this is that beyond saying \"this week will be like last week\", there's only so much we can do to predict variations with the data we have. This little bit extra isn't too hard to get, different choices of regressors and encodings can do it, but beyond that, we are still left with an average error of a bit more than 10 bikes per hour, that nothing we've tried is able to get rid of. Of course, there will be a hard bottom for how well our models can do, given that there are random fluctuations and external driving forces we have no way of predicting (road closure/vandalism of a station/a big shop opens nearby/etc.). Perhaps we are getting pretty close to hitting that limit.\n",
    "\n",
    "To get a handle on how little or much our models may be overfitting, here are, in addition to the test set MAEs, also the training set ones, when using dummy encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          Test     Training\n",
      "SimpleMean                25.587   24.211\n",
      "LastWeek                  12.285   11.369\n",
      "Linear                    12.615   11.821\n",
      "KNeighbors                11.491   1.104\n",
      "LinearSVR                 11.361   10.479\n",
      "DecisionTree              11.724   10.689\n",
      "SVRrbf                    10.915   9.282\n",
      "SVRpoly                   10.686   8.827\n",
      "DiffModel(SimpleMean)     12.300   11.384\n",
      "DiffModel(Linear)         13.029   11.942\n",
      "DiffModel(KNeighbors)     14.316   10.089\n",
      "DiffModel(LinearSVR)      12.296   11.367\n",
      "DiffModel(DecisionTree)   12.284   11.335\n",
      "DiffModel(SVRrbf)         12.452   11.083\n",
      "DiffModel(SVRpoly)        12.513   10.898\n"
     ]
    }
   ],
   "source": [
    "print(\"{:25} Test     Training\".format(\"\"))\n",
    "for k, v in rv_dum.models.items():\n",
    "    test_mae = v[\"test_mae\"].sum()\n",
    "    training_mae = v[\"training_mae\"].sum()\n",
    "    print(\"{:25} {:.3f}   {:.3f}\".format(k, test_mae, training_mae))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNeighbors of course does anomalously well since it's not really doing any fitting at all, but other than that, nothing too surprising or worrying here.\n",
    "\n",
    "Given that `LastWeek` already does almost as well as the fancier methods, one idea to try would be to apply all the above models to predicting differences in usage between now and a week ago, instead of predicting the usage directly. I tried this to some extent and found no great success. I suspect that when measuring variations instead of absolute values the yearly patterns for instance become quite hard to discern. Whatever the reason, it didn't work as well as I hoped.\n",
    "\n",
    "To get a more intuitive feel for how our above models too, let's see what the predictions from the winner of our above comparison, PolyRBF on trigonometricly encoded predictors, look like. Here's comparison between the predictions and the actual outcomes for the Hyde Park Corner station for one week in February 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:12: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d35972e8d6a849d0a0aaf925c0f170a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rv = rv_trig\n",
    "plot_column = (\"Hyde Park Corner, Hyde Park\", \"End\")\n",
    "timewindow = slice(\"2017-02-20\", \"2017-02-26\")\n",
    "# plot_columns = [(\"Wenlock Road , Hoxton\", \"End\")]\n",
    "# plot_columns = [(\"Waterloo Station 3, Waterloo\",  \"Start\")]\n",
    "modelclass = SVRpoly\n",
    "err = pd.concat(rv.models[modelclass.classname][\"test_errors\"]).sort_index()[timewindow]\n",
    "pred = pd.concat(\n",
    "    rv.models[modelclass.classname][\"test_predictions\"]\n",
    ").sort_index()[timewindow]\n",
    "truth = pd.concat([l[3] for l in rv.cv_batches]).sort_index()[timewindow]\n",
    "plt.figure()\n",
    "plt.plot(truth[plot_column])\n",
    "plt.plot(pred[plot_column])\n",
    "plt.ylabel(plot_column[1])\n",
    "plt.title(plot_column[0])\n",
    "plt.gcf().autofmt_xdate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Poisson regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZeroPredictor:\n",
    "    def predict(self, predictors):\n",
    "        prediction = pd.Series(0.0, index=predictors.index)\n",
    "        return prediction\n",
    "\n",
    "\n",
    "class PoissonGLMSingle:\n",
    "    classname = \"PoissonGLMSingle\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.column_name = None\n",
    "\n",
    "    def train(self, data, predictors):\n",
    "        # We assume predictors are normalized by this point.\n",
    "        glm_poisson = sm.GLM(data, predictors, family=sm.families.Poisson(),)\n",
    "        try:\n",
    "            model = glm_poisson.fit()\n",
    "        except ValueError as e:\n",
    "            # The GLM can't handle casese like data that is all zeros.\n",
    "            # In those cases, just make a predictor that always predicts zero.\n",
    "            model = ZeroPredictor()\n",
    "            if (data.max() > 0.0).any():\n",
    "                # The usual reason all all-zero is not why we errored this time.\n",
    "                # Print some diagnostics to figure out what went wrong.\n",
    "                print(\"glm_poisson.fit() raise a ValueError.\")\n",
    "                print(data.describe())\n",
    "                print(predictors.describe())\n",
    "        self.model = model\n",
    "        self.column_name = data.columns[0]\n",
    "\n",
    "    def predict(self, predictors):\n",
    "        name = self.column_name\n",
    "        times = predictors.index.to_series()\n",
    "        model = self.model\n",
    "        predictions = model.predict(predictors)\n",
    "        predictions = pd.DataFrame({name: predictions}, index=times)\n",
    "        return predictions\n",
    "\n",
    "\n",
    "class PoissonGLM:\n",
    "    classname = \"PoissonGLM\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "        self.stds = {}\n",
    "        self.means = {}\n",
    "        self.column_name = None\n",
    "\n",
    "    def normalize_predictors(self, predictors):\n",
    "        predictors = predictors.copy()\n",
    "        self.stds = {}\n",
    "        self.means = {}\n",
    "        for c in predictors.columns:\n",
    "            try:\n",
    "                is_stationcolumn = \"Start\" in c or \"End\" in c\n",
    "            except TypeError:\n",
    "                is_stationcolumn = False\n",
    "            if is_stationcolumn:\n",
    "                col = predictors[c]\n",
    "                mean = col.mean()\n",
    "                std = col.std()\n",
    "                predictors[c] = (col - mean) / std\n",
    "                self.stds[c] = std\n",
    "                self.means[c] = mean\n",
    "        return predictors\n",
    "\n",
    "    def renormalize_predictors(self, predictors):\n",
    "        predictors = predictors.copy()\n",
    "        for c in predictors.columns:\n",
    "            try:\n",
    "                is_stationcolumn = \"Start\" in c or \"End\" in c\n",
    "            except TypeError:\n",
    "                is_stationcolumn = False\n",
    "            if is_stationcolumn:\n",
    "                col = predictors[c]\n",
    "                mean = self.means[c]\n",
    "                std = self.stds[c]\n",
    "                predictors[c] = (col - mean) / std\n",
    "        return predictors\n",
    "\n",
    "    def train(self, data, predictors):\n",
    "        self.models = {}\n",
    "        predictors = self.normalize_predictors(predictors)\n",
    "\n",
    "        times = data.index.to_series()\n",
    "        groupers = [times.dt.weekday, times.dt.hour]\n",
    "        data_groups = data.groupby(groupers)\n",
    "        predictors_groups = predictors.groupby(groupers)\n",
    "        for group_label, data_group in data_groups:\n",
    "            predictors_group = predictors_groups.get_group(group_label)\n",
    "            model = PoissonGLMSingle()\n",
    "            model.train(data_group, predictors_group)\n",
    "            self.models[group_label] = model\n",
    "\n",
    "    def predict(self, predictors):\n",
    "        predictors = self.renormalize_predictors(predictors)\n",
    "        groupers = [times.dt.weekday, times.dt.hour]\n",
    "        predictors_groups = predictors.groupby(groupers)\n",
    "        predictions_groups = []\n",
    "        for (group_label, predictors_group,) in predictors_groups:\n",
    "            model = self.models[group_label]\n",
    "            predictions_group = model.predict(predictors_group)\n",
    "            predictions_groups.append(predictions_group)\n",
    "        predictions = pd.concat(predictions_groups, axis=0).sort_index()\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_rolling = (\n",
    "    events_by_station[test_columns].rolling(\"7d\").mean()[first_time:last_time]\n",
    ")\n",
    "weekly_rolling_normalized = (\n",
    "    weekly_rolling - weekly_rolling.mean()\n",
    ") / weekly_rolling.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created a RollingValidator with 6 cross-validation batches.\n",
      "Created a RollingValidator with 6 cross-validation batches.\n",
      "Created a RollingValidator with 6 cross-validation batches.\n"
     ]
    }
   ],
   "source": [
    "year_angles = 2 * np.pi * times.dt.week / 52\n",
    "# predictors_poiss = pd.DataFrame(\n",
    "#     {\"Year sine\": np.sin(year_angles), \"Year cosine\": np.cos(year_angles),}\n",
    "# )\n",
    "# predictors_poiss = pd.get_dummies(times.dt.month)\n",
    "\n",
    "specific_predictors_normalized = (\n",
    "    specific_predictors - specific_predictors.mean()\n",
    ") / specific_predictors.std()\n",
    "predictors_poiss = pd.concat(\n",
    "    [\n",
    "        #         pd.get_dummies(times.dt.month),\n",
    "        pd.get_dummies(times.dt.weekday),\n",
    "        pd.get_dummies(times.dt.hour),\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "rv_poiss = RollingValidator(\n",
    "    data,\n",
    "    predictors_poiss,\n",
    "    #     specific_predictors_normalized,\n",
    "    #     weekly_rolling,\n",
    "    None,\n",
    "    min_training_time,\n",
    "    prediction_time,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running RV on ('Waterloo Station 3, Waterloo', 'Start').\n",
      "Training for batch 0.\n",
      "Predicting for batch 0.\n",
      "Training for batch 1.\n",
      "Predicting for batch 1.\n",
      "Training for batch 2.\n",
      "Predicting for batch 2.\n",
      "Training for batch 3.\n",
      "Predicting for batch 3.\n",
      "Training for batch 4.\n",
      "Predicting for batch 4.\n",
      "Training for batch 5.\n",
      "Predicting for batch 5.\n",
      "Running RV on ('Hyde Park Corner, Hyde Park', 'End').\n",
      "Training for batch 0.\n",
      "Predicting for batch 0.\n",
      "Training for batch 1.\n",
      "Predicting for batch 1.\n",
      "Training for batch 2.\n",
      "Predicting for batch 2.\n",
      "Training for batch 3.\n",
      "Predicting for batch 3.\n",
      "Training for batch 4.\n",
      "Predicting for batch 4.\n",
      "Training for batch 5.\n",
      "Predicting for batch 5.\n",
      "Running RV on ('Wenlock Road , Hoxton', 'End').\n",
      "Training for batch 0.\n",
      "Predicting for batch 0.\n",
      "Training for batch 1.\n",
      "Predicting for batch 1.\n",
      "Training for batch 2.\n",
      "Predicting for batch 2.\n",
      "Training for batch 3.\n",
      "Predicting for batch 3.\n",
      "Training for batch 4.\n",
      "Predicting for batch 4.\n",
      "Training for batch 5.\n",
      "Predicting for batch 5.\n",
      "14.052302645661106\n",
      "13.70024922438926\n"
     ]
    }
   ],
   "source": [
    "mae = rv_poiss.test_modelclass(PoissonGLMSingle, print_progress=True)\n",
    "print(mae)\n",
    "print(rv_poiss.models[PoissonGLMSingle.classname][\"training_mae\"].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b406b5eb4a043cda6aa394d0fbf5c6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f9b94d6c210>"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure()\n",
    "pd.concat(\n",
    "    rv_poiss.models[PoissonGLMSingle.classname][\"test_predictions\"], axis=0\n",
    ").sort_index()[(\"Waterloo Station 3, Waterloo\", \"Start\")].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdf = pd.read_csv(\n",
    "    \"./data/weather/london_weather_data.csv\",\n",
    "    usecols=[\"DATE\", \"PRCP\", \"TAVG\"],\n",
    "    encoding=\"ISO-8859-2\",\n",
    ")\n",
    "wdf[\"DATE\"] = pd.to_datetime(wdf[\"DATE\"])\n",
    "wdf = wdf.set_index(\"DATE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdf = wdf[first_time:last_time]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There's only 6 NaNs in this time range, so how we fill them doesn't really matter much.\n",
    "wdf[\"PRCP\"] = wdf[\"PRCP\"].fillna(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07d01f4dd7a94e3f96c034bca805be21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f9e6f289d90>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wdf.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdf_resampled = wdf.resample(\"1h\", loffset=\"12h\").mean().reindex(times)\n",
    "# plt.figure()\n",
    "# wdf_resampled[\"TAVG\"].dropna().plot()\n",
    "wdf_resampled[\"TAVG\"] = (\n",
    "    wdf_resampled[\"TAVG\"]\n",
    "    .interpolate(method=\"polynomial\", order=2)\n",
    "    .bfill()\n",
    "    .ffill()\n",
    ")\n",
    "# wdf_resampled[\"TAVG\"].plot()\n",
    "# wdf_resampled[\"PRCP\"].dropna().plot()\n",
    "wdf_resampled[\"PRCP\"] = (\n",
    "    wdf_resampled[\"PRCP\"].interpolate(method=\"linear\", order=2).bfill().ffill()\n",
    ")\n",
    "# wdf_resampled[\"PRCP\"].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdfn = wdf_resampled.copy()\n",
    "wdfn[\"PRCP\"] /= wdfn[\"PRCP\"].std()\n",
    "wdfn[\"TAVG\"] = (wdfn[\"TAVG\"] - wdfn[\"TAVG\"].mean()) / wdfn[\"TAVG\"].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created a RollingValidator with 6 cross-validation batches.\n",
      "Created a RollingValidator with 6 cross-validation batches.\n",
      "Created a RollingValidator with 6 cross-validation batches.\n"
     ]
    }
   ],
   "source": [
    "year_angles = 2 * np.pi * times.dt.week / 52\n",
    "# predictors_poiss = pd.DataFrame(\n",
    "#     {\"Year sine\": np.sin(year_angles), \"Year cosine\": np.cos(year_angles),}\n",
    "# )\n",
    "# predictors_poiss = pd.get_dummies(times.dt.month)\n",
    "\n",
    "specific_predictors_normalized = (\n",
    "    specific_predictors - specific_predictors.mean()\n",
    ") / specific_predictors.std()\n",
    "predictors_poiss = pd.concat(\n",
    "    [\n",
    "        pd.get_dummies(times.dt.month),\n",
    "        pd.get_dummies(times.dt.weekday),\n",
    "        pd.get_dummies(times.dt.hour),\n",
    "        wdfn,\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "rv_poissw = RollingValidator(\n",
    "    data,\n",
    "    predictors_poiss,\n",
    "    specific_predictors_normalized,\n",
    "    #     weekly_rolling,\n",
    "    #     None,\n",
    "    min_training_time,\n",
    "    prediction_time,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running RV on ('Waterloo Station 3, Waterloo', 'Start').\n",
      "Training for batch 0.\n",
      "Predicting for batch 0.\n",
      "Training for batch 1.\n",
      "Predicting for batch 1.\n",
      "Training for batch 2.\n",
      "Predicting for batch 2.\n",
      "Training for batch 3.\n",
      "Predicting for batch 3.\n",
      "Training for batch 4.\n",
      "Predicting for batch 4.\n",
      "Training for batch 5.\n",
      "Predicting for batch 5.\n",
      "Running RV on ('Hyde Park Corner, Hyde Park', 'End').\n",
      "Training for batch 0.\n",
      "Predicting for batch 0.\n",
      "Training for batch 1.\n",
      "Predicting for batch 1.\n",
      "Training for batch 2.\n",
      "Predicting for batch 2.\n",
      "Training for batch 3.\n",
      "Predicting for batch 3.\n",
      "Training for batch 4.\n",
      "Predicting for batch 4.\n",
      "Training for batch 5.\n",
      "Predicting for batch 5.\n",
      "Running RV on ('Wenlock Road , Hoxton', 'End').\n",
      "Training for batch 0.\n",
      "Predicting for batch 0.\n",
      "Training for batch 1.\n",
      "Predicting for batch 1.\n",
      "Training for batch 2.\n",
      "Predicting for batch 2.\n",
      "Training for batch 3.\n",
      "Predicting for batch 3.\n",
      "Training for batch 4.\n",
      "Predicting for batch 4.\n",
      "Training for batch 5.\n",
      "Predicting for batch 5.\n",
      "12.97455776936832\n",
      "12.569187404098194\n"
     ]
    }
   ],
   "source": [
    "mae = rv_poissw.test_modelclass(PoissonGLMSingle, print_progress=True)\n",
    "print(mae)\n",
    "print(rv_poissw.models[PoissonGLMSingle.classname][\"training_mae\"].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Waterloo Station 3, Waterloo', 'Start')\n",
      "                             Generalized Linear Model Regression Results                             \n",
      "=====================================================================================================\n",
      "Dep. Variable:     ('Waterloo Station 3, Waterloo', 'Start')   No. Observations:                46711\n",
      "Model:                                                   GLM   Df Residuals:                    46668\n",
      "Model Family:                                        Poisson   Df Model:                           42\n",
      "Link Function:                                           log   Scale:                          1.0000\n",
      "Method:                                                 IRLS   Log-Likelihood:            -2.0486e+05\n",
      "Date:                                       Sun, 09 Feb 2020   Deviance:                   3.0826e+05\n",
      "Time:                                               21:49:21   Pearson chi2:                 4.37e+05\n",
      "No. Iterations:                                          100                                         \n",
      "Covariance Type:                                   nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "1              0.1264      0.007     19.301      0.000       0.114       0.139\n",
      "2              0.1917      0.006     30.015      0.000       0.179       0.204\n",
      "3              0.1789      0.006     30.558      0.000       0.167       0.190\n",
      "4              0.1227      0.006     21.932      0.000       0.112       0.134\n",
      "5              0.1589      0.006     28.566      0.000       0.148       0.170\n",
      "6              0.2094      0.006     33.673      0.000       0.197       0.222\n",
      "7              0.2061      0.007     30.091      0.000       0.193       0.219\n",
      "8              0.1406      0.007     20.774      0.000       0.127       0.154\n",
      "9              0.2106      0.006     34.584      0.000       0.199       0.223\n",
      "10             0.1968      0.006     34.707      0.000       0.186       0.208\n",
      "11             0.1931      0.006     31.842      0.000       0.181       0.205\n",
      "12            -0.0906      0.007    -13.267      0.000      -0.104      -0.077\n",
      "0              0.7323      0.005    135.722      0.000       0.722       0.743\n",
      "1              0.8556      0.005    162.246      0.000       0.845       0.866\n",
      "2              0.8603      0.005    163.741      0.000       0.850       0.871\n",
      "3              0.8436      0.005    159.893      0.000       0.833       0.854\n",
      "4              0.7978      0.005    150.267      0.000       0.787       0.808\n",
      "5             -0.9766      0.009   -107.358      0.000      -0.994      -0.959\n",
      "6             -1.2685      0.010   -123.991      0.000      -1.289      -1.248\n",
      "0             -0.8715      0.025    -35.004      0.000      -0.920      -0.823\n",
      "1             -1.5734      0.035    -45.264      0.000      -1.642      -1.505\n",
      "2             -3.1185      0.074    -42.023      0.000      -3.264      -2.973\n",
      "3             -3.6385      0.096    -37.824      0.000      -3.827      -3.450\n",
      "4             -3.8709      0.108    -35.846      0.000      -4.083      -3.659\n",
      "5             -0.4253      0.020    -20.960      0.000      -0.465      -0.386\n",
      "6              2.0321      0.009    237.109      0.000       2.015       2.049\n",
      "7              3.2295      0.007    450.899      0.000       3.215       3.244\n",
      "8              3.8105      0.007    555.373      0.000       3.797       3.824\n",
      "9              2.3525      0.008    292.298      0.000       2.337       2.368\n",
      "10             0.4415      0.014     31.418      0.000       0.414       0.469\n",
      "11             0.1775      0.016     11.350      0.000       0.147       0.208\n",
      "12             0.3610      0.015     24.882      0.000       0.333       0.389\n",
      "13             0.3996      0.014     27.975      0.000       0.372       0.428\n",
      "14             0.3694      0.014     25.545      0.000       0.341       0.398\n",
      "15             0.2782      0.015     18.544      0.000       0.249       0.308\n",
      "16             0.4199      0.014     29.630      0.000       0.392       0.448\n",
      "17             0.6615      0.013     51.238      0.000       0.636       0.687\n",
      "18             0.9536      0.012     82.066      0.000       0.931       0.976\n",
      "19             0.7476      0.013     59.787      0.000       0.723       0.772\n",
      "20             0.2454      0.015     16.143      0.000       0.216       0.275\n",
      "21            -0.2749      0.019    -14.477      0.000      -0.312      -0.238\n",
      "22            -0.3169      0.019    -16.382      0.000      -0.355      -0.279\n",
      "23            -0.5452      0.021    -25.438      0.000      -0.587      -0.503\n",
      "PRCP          -0.0327      0.002    -20.424      0.000      -0.036      -0.030\n",
      "TAVG           0.0369      0.003     12.560      0.000       0.031       0.043\n",
      "==============================================================================\n",
      "('Hyde Park Corner, Hyde Park', 'End')\n",
      "                           Generalized Linear Model Regression Results                            \n",
      "==================================================================================================\n",
      "Dep. Variable:     ('Hyde Park Corner, Hyde Park', 'End')   No. Observations:                46711\n",
      "Model:                                                GLM   Df Residuals:                    46668\n",
      "Model Family:                                     Poisson   Df Model:                           42\n",
      "Link Function:                                        log   Scale:                          1.0000\n",
      "Method:                                              IRLS   Log-Likelihood:            -1.9341e+05\n",
      "Date:                                    Sun, 09 Feb 2020   Deviance:                   2.6894e+05\n",
      "Time:                                            21:49:29   Pearson chi2:                 2.94e+05\n",
      "No. Iterations:                                       100                                         \n",
      "Covariance Type:                                nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "1             -0.0666      0.008     -8.083      0.000      -0.083      -0.050\n",
      "2              0.1358      0.008     17.799      0.000       0.121       0.151\n",
      "3              0.4218      0.006     68.985      0.000       0.410       0.434\n",
      "4              0.6998      0.005    138.723      0.000       0.690       0.710\n",
      "5              0.5483      0.005    107.073      0.000       0.538       0.558\n",
      "6              0.5254      0.006     93.397      0.000       0.514       0.536\n",
      "7              0.4700      0.006     73.971      0.000       0.458       0.482\n",
      "8              0.5421      0.006     90.063      0.000       0.530       0.554\n",
      "9              0.3285      0.006     55.858      0.000       0.317       0.340\n",
      "10             0.2176      0.006     35.821      0.000       0.206       0.229\n",
      "11            -0.0769      0.008     -9.882      0.000      -0.092      -0.062\n",
      "12            -0.0523      0.008     -6.729      0.000      -0.068      -0.037\n",
      "0              0.3884      0.006     65.687      0.000       0.377       0.400\n",
      "1              0.3897      0.006     65.818      0.000       0.378       0.401\n",
      "2              0.3381      0.006     56.658      0.000       0.326       0.350\n",
      "3              0.3548      0.006     59.684      0.000       0.343       0.366\n",
      "4              0.3999      0.006     68.129      0.000       0.388       0.411\n",
      "5              0.8470      0.005    157.491      0.000       0.836       0.858\n",
      "6              0.9756      0.005    185.851      0.000       0.965       0.986\n",
      "0             -1.3443      0.028    -48.723      0.000      -1.398      -1.290\n",
      "1             -2.6395      0.052    -51.203      0.000      -2.741      -2.538\n",
      "2             -3.7696      0.090    -41.808      0.000      -3.946      -3.593\n",
      "3             -4.1832      0.111    -37.754      0.000      -4.400      -3.966\n",
      "4             -3.5369      0.080    -44.015      0.000      -3.694      -3.379\n",
      "5             -2.6473      0.052    -51.140      0.000      -2.749      -2.546\n",
      "6             -1.2135      0.026    -46.749      0.000      -1.264      -1.163\n",
      "7              0.0070      0.015      0.459      0.646      -0.023       0.037\n",
      "8              0.9817      0.011     91.167      0.000       0.961       1.003\n",
      "9              1.1230      0.010    108.692      0.000       1.103       1.143\n",
      "10             1.1191      0.010    108.207      0.000       1.099       1.139\n",
      "11             1.4891      0.009    158.632      0.000       1.471       1.508\n",
      "12             1.8253      0.009    209.221      0.000       1.808       1.842\n",
      "13             1.9990      0.008    236.673      0.000       1.982       2.016\n",
      "14             2.1219      0.008    256.515      0.000       2.106       2.138\n",
      "15             2.1874      0.008    267.188      0.000       2.171       2.203\n",
      "16             2.3275      0.008    290.266      0.000       2.312       2.343\n",
      "17             2.3165      0.008    288.439      0.000       2.301       2.332\n",
      "18             2.1935      0.008    268.150      0.000       2.177       2.210\n",
      "19             1.8375      0.009    211.035      0.000       1.820       1.855\n",
      "20             1.3830      0.010    143.483      0.000       1.364       1.402\n",
      "21             0.7636      0.012     66.191      0.000       0.741       0.786\n",
      "22             0.0297      0.015      1.973      0.049       0.000       0.059\n",
      "23            -0.6771      0.020    -33.301      0.000      -0.717      -0.637\n",
      "PRCP          -0.0885      0.002    -50.558      0.000      -0.092      -0.085\n",
      "TAVG           0.2958      0.003     99.780      0.000       0.290       0.302\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "test_columns = [\n",
    "    (\"Waterloo Station 3, Waterloo\", \"Start\"),\n",
    "    (\"Hyde Park Corner, Hyde Park\", \"End\"),\n",
    "]\n",
    "for test_column in test_columns:\n",
    "    glm_poisson = sm.GLM(\n",
    "        data[test_column], predictors_poiss, family=sm.families.Poisson(),\n",
    "    )\n",
    "    print(test_column)\n",
    "    model = glm_poisson.fit()\n",
    "    print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Generalized Linear Model Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>   <td>('Hyde Park Corner, Hyde Park', 'End')</td> <th>  No. Observations:  </th>   <td> 46711</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                             <td>GLM</td>                  <th>  Df Residuals:      </th>   <td> 46668</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model Family:</th>                    <td>Poisson</td>                <th>  Df Model:          </th>   <td>    42</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Link Function:</th>                     <td>log</td>                  <th>  Scale:             </th>  <td>  1.0000</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                           <td>IRLS</td>                  <th>  Log-Likelihood:    </th> <td>-1.9341e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>                       <td>Sun, 09 Feb 2020</td>            <th>  Deviance:          </th> <td>2.6894e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                           <td>21:50:28</td>                <th>  Pearson chi2:      </th>  <td>2.94e+05</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Iterations:</th>                    <td>100</td>                  <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>                <td>nonrobust</td>               <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>      <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.005</th>    <th>0.995]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1</th>    <td>   -0.0666</td> <td>    0.008</td> <td>   -8.083</td> <td> 0.000</td> <td>   -0.088</td> <td>   -0.045</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2</th>    <td>    0.1358</td> <td>    0.008</td> <td>   17.799</td> <td> 0.000</td> <td>    0.116</td> <td>    0.155</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3</th>    <td>    0.4218</td> <td>    0.006</td> <td>   68.985</td> <td> 0.000</td> <td>    0.406</td> <td>    0.438</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4</th>    <td>    0.6998</td> <td>    0.005</td> <td>  138.723</td> <td> 0.000</td> <td>    0.687</td> <td>    0.713</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5</th>    <td>    0.5483</td> <td>    0.005</td> <td>  107.073</td> <td> 0.000</td> <td>    0.535</td> <td>    0.561</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6</th>    <td>    0.5254</td> <td>    0.006</td> <td>   93.397</td> <td> 0.000</td> <td>    0.511</td> <td>    0.540</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7</th>    <td>    0.4700</td> <td>    0.006</td> <td>   73.971</td> <td> 0.000</td> <td>    0.454</td> <td>    0.486</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8</th>    <td>    0.5421</td> <td>    0.006</td> <td>   90.063</td> <td> 0.000</td> <td>    0.527</td> <td>    0.558</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9</th>    <td>    0.3285</td> <td>    0.006</td> <td>   55.858</td> <td> 0.000</td> <td>    0.313</td> <td>    0.344</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10</th>   <td>    0.2176</td> <td>    0.006</td> <td>   35.821</td> <td> 0.000</td> <td>    0.202</td> <td>    0.233</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11</th>   <td>   -0.0769</td> <td>    0.008</td> <td>   -9.882</td> <td> 0.000</td> <td>   -0.097</td> <td>   -0.057</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12</th>   <td>   -0.0523</td> <td>    0.008</td> <td>   -6.729</td> <td> 0.000</td> <td>   -0.072</td> <td>   -0.032</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>0</th>    <td>    0.3884</td> <td>    0.006</td> <td>   65.687</td> <td> 0.000</td> <td>    0.373</td> <td>    0.404</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1</th>    <td>    0.3897</td> <td>    0.006</td> <td>   65.818</td> <td> 0.000</td> <td>    0.374</td> <td>    0.405</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2</th>    <td>    0.3381</td> <td>    0.006</td> <td>   56.658</td> <td> 0.000</td> <td>    0.323</td> <td>    0.353</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3</th>    <td>    0.3548</td> <td>    0.006</td> <td>   59.684</td> <td> 0.000</td> <td>    0.340</td> <td>    0.370</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4</th>    <td>    0.3999</td> <td>    0.006</td> <td>   68.129</td> <td> 0.000</td> <td>    0.385</td> <td>    0.415</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5</th>    <td>    0.8470</td> <td>    0.005</td> <td>  157.491</td> <td> 0.000</td> <td>    0.833</td> <td>    0.861</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6</th>    <td>    0.9756</td> <td>    0.005</td> <td>  185.851</td> <td> 0.000</td> <td>    0.962</td> <td>    0.989</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>0</th>    <td>   -1.3443</td> <td>    0.028</td> <td>  -48.723</td> <td> 0.000</td> <td>   -1.415</td> <td>   -1.273</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1</th>    <td>   -2.6395</td> <td>    0.052</td> <td>  -51.203</td> <td> 0.000</td> <td>   -2.772</td> <td>   -2.507</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2</th>    <td>   -3.7696</td> <td>    0.090</td> <td>  -41.808</td> <td> 0.000</td> <td>   -4.002</td> <td>   -3.537</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3</th>    <td>   -4.1832</td> <td>    0.111</td> <td>  -37.754</td> <td> 0.000</td> <td>   -4.469</td> <td>   -3.898</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4</th>    <td>   -3.5369</td> <td>    0.080</td> <td>  -44.015</td> <td> 0.000</td> <td>   -3.744</td> <td>   -3.330</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5</th>    <td>   -2.6473</td> <td>    0.052</td> <td>  -51.140</td> <td> 0.000</td> <td>   -2.781</td> <td>   -2.514</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6</th>    <td>   -1.2135</td> <td>    0.026</td> <td>  -46.749</td> <td> 0.000</td> <td>   -1.280</td> <td>   -1.147</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7</th>    <td>    0.0070</td> <td>    0.015</td> <td>    0.459</td> <td> 0.646</td> <td>   -0.032</td> <td>    0.046</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8</th>    <td>    0.9817</td> <td>    0.011</td> <td>   91.167</td> <td> 0.000</td> <td>    0.954</td> <td>    1.009</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9</th>    <td>    1.1230</td> <td>    0.010</td> <td>  108.692</td> <td> 0.000</td> <td>    1.096</td> <td>    1.150</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10</th>   <td>    1.1191</td> <td>    0.010</td> <td>  108.207</td> <td> 0.000</td> <td>    1.092</td> <td>    1.146</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11</th>   <td>    1.4891</td> <td>    0.009</td> <td>  158.632</td> <td> 0.000</td> <td>    1.465</td> <td>    1.513</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12</th>   <td>    1.8253</td> <td>    0.009</td> <td>  209.221</td> <td> 0.000</td> <td>    1.803</td> <td>    1.848</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13</th>   <td>    1.9990</td> <td>    0.008</td> <td>  236.673</td> <td> 0.000</td> <td>    1.977</td> <td>    2.021</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>14</th>   <td>    2.1219</td> <td>    0.008</td> <td>  256.515</td> <td> 0.000</td> <td>    2.101</td> <td>    2.143</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>15</th>   <td>    2.1874</td> <td>    0.008</td> <td>  267.188</td> <td> 0.000</td> <td>    2.166</td> <td>    2.208</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>16</th>   <td>    2.3275</td> <td>    0.008</td> <td>  290.266</td> <td> 0.000</td> <td>    2.307</td> <td>    2.348</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>17</th>   <td>    2.3165</td> <td>    0.008</td> <td>  288.439</td> <td> 0.000</td> <td>    2.296</td> <td>    2.337</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>18</th>   <td>    2.1935</td> <td>    0.008</td> <td>  268.150</td> <td> 0.000</td> <td>    2.172</td> <td>    2.215</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>19</th>   <td>    1.8375</td> <td>    0.009</td> <td>  211.035</td> <td> 0.000</td> <td>    1.815</td> <td>    1.860</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>20</th>   <td>    1.3830</td> <td>    0.010</td> <td>  143.483</td> <td> 0.000</td> <td>    1.358</td> <td>    1.408</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>21</th>   <td>    0.7636</td> <td>    0.012</td> <td>   66.191</td> <td> 0.000</td> <td>    0.734</td> <td>    0.793</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>22</th>   <td>    0.0297</td> <td>    0.015</td> <td>    1.973</td> <td> 0.049</td> <td>   -0.009</td> <td>    0.069</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>23</th>   <td>   -0.6771</td> <td>    0.020</td> <td>  -33.301</td> <td> 0.000</td> <td>   -0.729</td> <td>   -0.625</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PRCP</th> <td>   -0.0885</td> <td>    0.002</td> <td>  -50.558</td> <td> 0.000</td> <td>   -0.093</td> <td>   -0.084</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>TAVG</th> <td>    0.2958</td> <td>    0.003</td> <td>   99.780</td> <td> 0.000</td> <td>    0.288</td> <td>    0.303</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Generalized Linear Model Regression Results                            \n",
       "==================================================================================================\n",
       "Dep. Variable:     ('Hyde Park Corner, Hyde Park', 'End')   No. Observations:                46711\n",
       "Model:                                                GLM   Df Residuals:                    46668\n",
       "Model Family:                                     Poisson   Df Model:                           42\n",
       "Link Function:                                        log   Scale:                          1.0000\n",
       "Method:                                              IRLS   Log-Likelihood:            -1.9341e+05\n",
       "Date:                                    Sun, 09 Feb 2020   Deviance:                   2.6894e+05\n",
       "Time:                                            21:50:28   Pearson chi2:                 2.94e+05\n",
       "No. Iterations:                                       100                                         \n",
       "Covariance Type:                                nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.005      0.995]\n",
       "------------------------------------------------------------------------------\n",
       "1             -0.0666      0.008     -8.083      0.000      -0.088      -0.045\n",
       "2              0.1358      0.008     17.799      0.000       0.116       0.155\n",
       "3              0.4218      0.006     68.985      0.000       0.406       0.438\n",
       "4              0.6998      0.005    138.723      0.000       0.687       0.713\n",
       "5              0.5483      0.005    107.073      0.000       0.535       0.561\n",
       "6              0.5254      0.006     93.397      0.000       0.511       0.540\n",
       "7              0.4700      0.006     73.971      0.000       0.454       0.486\n",
       "8              0.5421      0.006     90.063      0.000       0.527       0.558\n",
       "9              0.3285      0.006     55.858      0.000       0.313       0.344\n",
       "10             0.2176      0.006     35.821      0.000       0.202       0.233\n",
       "11            -0.0769      0.008     -9.882      0.000      -0.097      -0.057\n",
       "12            -0.0523      0.008     -6.729      0.000      -0.072      -0.032\n",
       "0              0.3884      0.006     65.687      0.000       0.373       0.404\n",
       "1              0.3897      0.006     65.818      0.000       0.374       0.405\n",
       "2              0.3381      0.006     56.658      0.000       0.323       0.353\n",
       "3              0.3548      0.006     59.684      0.000       0.340       0.370\n",
       "4              0.3999      0.006     68.129      0.000       0.385       0.415\n",
       "5              0.8470      0.005    157.491      0.000       0.833       0.861\n",
       "6              0.9756      0.005    185.851      0.000       0.962       0.989\n",
       "0             -1.3443      0.028    -48.723      0.000      -1.415      -1.273\n",
       "1             -2.6395      0.052    -51.203      0.000      -2.772      -2.507\n",
       "2             -3.7696      0.090    -41.808      0.000      -4.002      -3.537\n",
       "3             -4.1832      0.111    -37.754      0.000      -4.469      -3.898\n",
       "4             -3.5369      0.080    -44.015      0.000      -3.744      -3.330\n",
       "5             -2.6473      0.052    -51.140      0.000      -2.781      -2.514\n",
       "6             -1.2135      0.026    -46.749      0.000      -1.280      -1.147\n",
       "7              0.0070      0.015      0.459      0.646      -0.032       0.046\n",
       "8              0.9817      0.011     91.167      0.000       0.954       1.009\n",
       "9              1.1230      0.010    108.692      0.000       1.096       1.150\n",
       "10             1.1191      0.010    108.207      0.000       1.092       1.146\n",
       "11             1.4891      0.009    158.632      0.000       1.465       1.513\n",
       "12             1.8253      0.009    209.221      0.000       1.803       1.848\n",
       "13             1.9990      0.008    236.673      0.000       1.977       2.021\n",
       "14             2.1219      0.008    256.515      0.000       2.101       2.143\n",
       "15             2.1874      0.008    267.188      0.000       2.166       2.208\n",
       "16             2.3275      0.008    290.266      0.000       2.307       2.348\n",
       "17             2.3165      0.008    288.439      0.000       2.296       2.337\n",
       "18             2.1935      0.008    268.150      0.000       2.172       2.215\n",
       "19             1.8375      0.009    211.035      0.000       1.815       1.860\n",
       "20             1.3830      0.010    143.483      0.000       1.358       1.408\n",
       "21             0.7636      0.012     66.191      0.000       0.734       0.793\n",
       "22             0.0297      0.015      1.973      0.049      -0.009       0.069\n",
       "23            -0.6771      0.020    -33.301      0.000      -0.729      -0.625\n",
       "PRCP          -0.0885      0.002    -50.558      0.000      -0.093      -0.084\n",
       "TAVG           0.2958      0.003     99.780      0.000       0.288       0.303\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.summary(alpha=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "00b2b3f8ebbf47ecb0ec2946243b38ee": {
      "model_module": "jupyter-matplotlib",
      "model_module_version": "^0.5.0",
      "model_name": "MPLCanvasModel",
      "state": {
       "layout": "IPY_MODEL_58ca06be6a63489187048ac270e65b80",
       "toolbar": "IPY_MODEL_966fd6dfbdab41e69d3d2c0efae4dbb6",
       "toolbar_position": "left"
      }
     },
     "01c6fc2720274d1a88ed78a79228dc21": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "02b26b76fc4e4a0ba074e3ebe0d5eb25": {
      "model_module": "jupyter-matplotlib",
      "model_module_version": "^0.5.0",
      "model_name": "MPLCanvasModel",
      "state": {
       "layout": "IPY_MODEL_5e025a6869084b1aa4507ea60f4931d5",
       "toolbar": "IPY_MODEL_bd5a951110f54175a7ae0de94e135702",
       "toolbar_position": "left"
      }
     },
     "02d4649be9b7445cace4ef9a8fc22a4c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "03ea8ae0f8e643bdadf93fdbabec526b": {
      "model_module": "jupyter-matplotlib",
      "model_module_version": "^0.5.0",
      "model_name": "ToolbarModel",
      "state": {
       "layout": "IPY_MODEL_601c3bf911334eeb8c047cfe7ec73103",
       "toolitems": [
        [
         "Home",
         "Reset original view",
         "home",
         "home"
        ],
        [
         "Back",
         "Back to previous view",
         "arrow-left",
         "back"
        ],
        [
         "Forward",
         "Forward to next view",
         "arrow-right",
         "forward"
        ],
        [
         "Pan",
         "Pan axes with left mouse, zoom with right",
         "arrows",
         "pan"
        ],
        [
         "Zoom",
         "Zoom to rectangle",
         "square-o",
         "zoom"
        ],
        [
         "Download",
         "Download plot",
         "floppy-o",
         "save_figure"
        ]
       ]
      }
     },
     "056b5375f2e743e3a64c6d6fb94aadb9": {
      "model_module": "jupyter-matplotlib",
      "model_module_version": "^0.5.0",
      "model_name": "MPLCanvasModel",
      "state": {
       "layout": "IPY_MODEL_30d200e0587a4c3ca1aa072f31723410",
       "toolbar": "IPY_MODEL_38b5d440d1144f1ab15d729355a9dbdd",
       "toolbar_position": "left"
      }
     },
     "08e32dfb96664b88863ed24c0847124a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "09df60191750438a9f8f5fd35dd28f9d": {
      "model_module": "jupyter-matplotlib",
      "model_module_version": "^0.5.0",
      "model_name": "MPLCanvasModel",
      "state": {
       "layout": "IPY_MODEL_8bd0d5cca841433295647e4a0603c75f",
       "toolbar": "IPY_MODEL_eb472f132d274dbaae9e15f1f5388142",
       "toolbar_position": "left"
      }
     },
     "0cdf160934124f15be9e3985615d127a": {
      "model_module": "jupyter-matplotlib",
      "model_module_version": "^0.5.0",
      "model_name": "ToolbarModel",
      "state": {
       "layout": "IPY_MODEL_440d022e1e0d4d5b948a919b5979f99c",
       "toolitems": [
        [
         "Home",
         "Reset original view",
         "home",
         "home"
        ],
        [
         "Back",
         "Back to previous view",
         "arrow-left",
         "back"
        ],
        [
         "Forward",
         "Forward to next view",
         "arrow-right",
         "forward"
        ],
        [
         "Pan",
         "Pan axes with left mouse, zoom with right",
         "arrows",
         "pan"
        ],
        [
         "Zoom",
         "Zoom to rectangle",
         "square-o",
         "zoom"
        ],
        [
         "Download",
         "Download plot",
         "floppy-o",
         "save_figure"
        ]
       ]
      }
     },
     "109fb7767e484a908ed5c73066faf8ec": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "1135574093af4509a65ee0a1588a6306": {
      "model_module": "jupyter-matplotlib",
      "model_module_version": "^0.5.0",
      "model_name": "MPLCanvasModel",
      "state": {
       "layout": "IPY_MODEL_21011b467512439f871d615943861eef",
       "toolbar": "IPY_MODEL_1b5ddfa227074fe3b4b42e366b79df6d",
       "toolbar_position": "left"
      }
     },
     "115ba6b1b07a444ba1c4f1839d73f34a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "11d023bbf9944469beab5cb237d2fbff": {
      "model_module": "jupyter-matplotlib",
      "model_module_version": "^0.5.0",
      "model_name": "ToolbarModel",
      "state": {
       "layout": "IPY_MODEL_fe759346f4fa40d59150c52a58bb0bb8",
       "toolitems": [
        [
         "Home",
         "Reset original view",
         "home",
         "home"
        ],
        [
         "Back",
         "Back to previous view",
         "arrow-left",
         "back"
        ],
        [
         "Forward",
         "Forward to next view",
         "arrow-right",
         "forward"
        ],
        [
         "Pan",
         "Pan axes with left mouse, zoom with right",
         "arrows",
         "pan"
        ],
        [
         "Zoom",
         "Zoom to rectangle",
         "square-o",
         "zoom"
        ],
        [
         "Download",
         "Download plot",
         "floppy-o",
         "save_figure"
        ]
       ]
      }
     },
     "13d8b797d0af4eb3a13b1bba4cf6f0c9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "17ab28624c4e4b0da91b3f25f787e5fe": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "1b5ddfa227074fe3b4b42e366b79df6d": {
      "model_module": "jupyter-matplotlib",
      "model_module_version": "^0.5.0",
      "model_name": "ToolbarModel",
      "state": {
       "layout": "IPY_MODEL_367ea27beb324a958f0ecf2781221f2c",
       "toolitems": [
        [
         "Home",
         "Reset original view",
         "home",
         "home"
        ],
        [
         "Back",
         "Back to previous view",
         "arrow-left",
         "back"
        ],
        [
         "Forward",
         "Forward to next view",
         "arrow-right",
         "forward"
        ],
        [
         "Pan",
         "Pan axes with left mouse, zoom with right",
         "arrows",
         "pan"
        ],
        [
         "Zoom",
         "Zoom to rectangle",
         "square-o",
         "zoom"
        ],
        [
         "Download",
         "Download plot",
         "floppy-o",
         "save_figure"
        ]
       ]
      }
     },
     "1cd815e92d05406a92d54b96f4535d13": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "1ea4e7bc9afe4c32b481b41f5a6d1bba": {
      "model_module": "jupyter-matplotlib",
      "model_module_version": "^0.5.0",
      "model_name": "MPLCanvasModel",
      "state": {
       "layout": "IPY_MODEL_68c5044e6aff445292424095e371b58e",
       "toolbar": "IPY_MODEL_0cdf160934124f15be9e3985615d127a",
       "toolbar_position": "left"
      }
     },
     "20b7b6ac7a5d4d7588cd2567464f9a5a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "21011b467512439f871d615943861eef": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "25e2c150de6d4b658895776db6bad3b8": {
      "model_module": "jupyter-matplotlib",
      "model_module_version": "^0.5.0",
      "model_name": "ToolbarModel",
      "state": {
       "layout": "IPY_MODEL_c735a345fde54edba6763da1d091001e",
       "toolitems": [
        [
         "Home",
         "Reset original view",
         "home",
         "home"
        ],
        [
         "Back",
         "Back to previous view",
         "arrow-left",
         "back"
        ],
        [
         "Forward",
         "Forward to next view",
         "arrow-right",
         "forward"
        ],
        [
         "Pan",
         "Pan axes with left mouse, zoom with right",
         "arrows",
         "pan"
        ],
        [
         "Zoom",
         "Zoom to rectangle",
         "square-o",
         "zoom"
        ],
        [
         "Download",
         "Download plot",
         "floppy-o",
         "save_figure"
        ]
       ]
      }
     },
     "2858c3e74be9476dabd9feb65f966ee9": {
      "model_module": "jupyter-matplotlib",
      "model_module_version": "^0.5.0",
      "model_name": "MPLCanvasModel",
      "state": {
       "layout": "IPY_MODEL_9b375a84324a4482a8e05d4469e11674",
       "toolbar": "IPY_MODEL_4e96aa2552b54654985c269a4fc32010",
       "toolbar_position": "left"
      }
     },
     "2dcdade1a1d8446689bc6e424bf0fe3d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "3027dfddbe7a47e7849d960e0533bc4a": {
      "model_module": "jupyter-matplotlib",
      "model_module_version": "^0.5.0",
      "model_name": "MPLCanvasModel",
      "state": {
       "layout": "IPY_MODEL_ceb9779498af46f0a13afe11dfe5ee13",
       "toolbar": "IPY_MODEL_11d023bbf9944469beab5cb237d2fbff",
       "toolbar_position": "left"
      }
     },
     "30d200e0587a4c3ca1aa072f31723410": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "30ea0e6d2b1541f39ee5567eb587b71d": {
      "model_module": "jupyter-matplotlib",
      "model_module_version": "^0.5.0",
      "model_name": "MPLCanvasModel",
      "state": {
       "layout": "IPY_MODEL_01c6fc2720274d1a88ed78a79228dc21",
       "toolbar": "IPY_MODEL_a7e844952f2c4d209c0719a0e1bbe913",
       "toolbar_position": "left"
      }
     },
     "356f38934bee48fd8c43ab6bb68ddf56": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "367ea27beb324a958f0ecf2781221f2c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "36d2d372883f43aabd970a3424c01a8c": {
      "model_module": "jupyter-matplotlib",
      "model_module_version": "^0.5.0",
      "model_name": "MPLCanvasModel",
      "state": {
       "layout": "IPY_MODEL_4a542e8f9bf14ea18fc53913449decd0",
       "toolbar": "IPY_MODEL_6696d786f82a4c42b63b5b541a82bdf0",
       "toolbar_position": "left"
      }
     },
     "38b5d440d1144f1ab15d729355a9dbdd": {
      "model_module": "jupyter-matplotlib",
      "model_module_version": "^0.5.0",
      "model_name": "ToolbarModel",
      "state": {
       "layout": "IPY_MODEL_5c89830e00c547268c7dac6379e31589",
       "toolitems": [
        [
         "Home",
         "Reset original view",
         "home",
         "home"
        ],
        [
         "Back",
         "Back to previous view",
         "arrow-left",
         "back"
        ],
        [
         "Forward",
         "Forward to next view",
         "arrow-right",
         "forward"
        ],
        [
         "Pan",
         "Pan axes with left mouse, zoom with right",
         "arrows",
         "pan"
        ],
        [
         "Zoom",
         "Zoom to rectangle",
         "square-o",
         "zoom"
        ],
        [
         "Download",
         "Download plot",
         "floppy-o",
         "save_figure"
        ]
       ]
      }
     },
     "391e668a94634818be4b0d9ff529daea": {
      "model_module": "jupyter-matplotlib",
      "model_module_version": "^0.5.0",
      "model_name": "MPLCanvasModel",
      "state": {
       "layout": "IPY_MODEL_9dcc44c54b7744a4a388d4e0acc6becd",
       "toolbar": "IPY_MODEL_03ea8ae0f8e643bdadf93fdbabec526b",
       "toolbar_position": "left"
      }
     },
     "40f1d191afcf451e97289088f47d6d52": {
      "model_module": "jupyter-matplotlib",
      "model_module_version": "^0.5.0",
      "model_name": "MPLCanvasModel",
      "state": {
       "layout": "IPY_MODEL_46326f6d1c8a4d87a4d6f8fec44104de",
       "toolbar": "IPY_MODEL_9ea991266d2346668eae8c0909934dfd",
       "toolbar_position": "left"
      }
     },
     "4284d29c7a3f430bb2e29fc9c89c05c6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "4373bf06ab9348ee9558615af323881c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "43dfd478e2e84d22aedb0deb566fe42a": {
      "model_module": "jupyter-matplotlib",
      "model_module_version": "^0.5.0",
      "model_name": "MPLCanvasModel",
      "state": {
       "layout": "IPY_MODEL_953a23821b874732931c588e0cedfcc6",
       "toolbar": "IPY_MODEL_c72cae9f073e4fcd8e7d30fd30f2b11a",
       "toolbar_position": "left"
      }
     },
     "440d022e1e0d4d5b948a919b5979f99c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "46326f6d1c8a4d87a4d6f8fec44104de": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "4716718d83ed4be5ab3f6cdffc625665": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "4a542e8f9bf14ea18fc53913449decd0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "4b71144ad50f4560bf18d8834d215158": {
      "model_module": "jupyter-matplotlib",
      "model_module_version": "^0.5.0",
      "model_name": "MPLCanvasModel",
      "state": {
       "layout": "IPY_MODEL_e140196faaf7478f938949d1d32f8f12",
       "toolbar": "IPY_MODEL_7b38cc9d78fc4f159efc08b6e160a0d7",
       "toolbar_position": "left"
      }
     },
     "4b7ca2ec9d8c4a958db5f8d0fe94ac26": {
      "model_module": "jupyter-matplotlib",
      "model_module_version": "^0.5.0",
      "model_name": "MPLCanvasModel",
      "state": {
       "layout": "IPY_MODEL_4c3134c4ce3c4fb28039c348b4914010",
       "toolbar": "IPY_MODEL_bd6b3734b6524333868e02cd35bb8d93",
       "toolbar_position": "left"
      }
     },
     "4c3134c4ce3c4fb28039c348b4914010": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "4e96aa2552b54654985c269a4fc32010": {
      "model_module": "jupyter-matplotlib",
      "model_module_version": "^0.5.0",
      "model_name": "ToolbarModel",
      "state": {
       "layout": "IPY_MODEL_a555a05885214b6589d545508d3f3b33",
       "toolitems": [
        [
         "Home",
         "Reset original view",
         "home",
         "home"
        ],
        [
         "Back",
         "Back to previous view",
         "arrow-left",
         "back"
        ],
        [
         "Forward",
         "Forward to next view",
         "arrow-right",
         "forward"
        ],
        [
         "Pan",
         "Pan axes with left mouse, zoom with right",
         "arrows",
         "pan"
        ],
        [
         "Zoom",
         "Zoom to rectangle",
         "square-o",
         "zoom"
        ],
        [
         "Download",
         "Download plot",
         "floppy-o",
         "save_figure"
        ]
       ]
      }
     },
     "51456f9b1c134aaa8daea5f2225b96f9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "51b43c374cfc47d69f400e2b80e428fb": {
      "model_module": "jupyter-matplotlib",
      "model_module_version": "^0.5.0",
      "model_name": "ToolbarModel",
      "state": {
       "layout": "IPY_MODEL_4284d29c7a3f430bb2e29fc9c89c05c6",
       "toolitems": [
        [
         "Home",
         "Reset original view",
         "home",
         "home"
        ],
        [
         "Back",
         "Back to previous view",
         "arrow-left",
         "back"
        ],
        [
         "Forward",
         "Forward to next view",
         "arrow-right",
         "forward"
        ],
        [
         "Pan",
         "Pan axes with left mouse, zoom with right",
         "arrows",
         "pan"
        ],
        [
         "Zoom",
         "Zoom to rectangle",
         "square-o",
         "zoom"
        ],
        [
         "Download",
         "Download plot",
         "floppy-o",
         "save_figure"
        ]
       ]
      }
     },
     "53deae4a7308486fb3d0a7e474c31bde": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "5810c4449bb446a9991b523ad05a44fd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "58ca06be6a63489187048ac270e65b80": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "5952e271a86b474aa50f3002c2f2d0e0": {
      "model_module": "jupyter-matplotlib",
      "model_module_version": "^0.5.0",
      "model_name": "ToolbarModel",
      "state": {
       "layout": "IPY_MODEL_dfa80b869bd74e80b5d3f6058117a0ac",
       "toolitems": [
        [
         "Home",
         "Reset original view",
         "home",
         "home"
        ],
        [
         "Back",
         "Back to previous view",
         "arrow-left",
         "back"
        ],
        [
         "Forward",
         "Forward to next view",
         "arrow-right",
         "forward"
        ],
        [
         "Pan",
         "Pan axes with left mouse, zoom with right",
         "arrows",
         "pan"
        ],
        [
         "Zoom",
         "Zoom to rectangle",
         "square-o",
         "zoom"
        ],
        [
         "Download",
         "Download plot",
         "floppy-o",
         "save_figure"
        ]
       ]
      }
     },
     "5c89830e00c547268c7dac6379e31589": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "5e025a6869084b1aa4507ea60f4931d5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "5e571f77fdc54a0fa964a850c5634bf5": {
      "model_module": "jupyter-matplotlib",
      "model_module_version": "^0.5.0",
      "model_name": "ToolbarModel",
      "state": {
       "layout": "IPY_MODEL_356f38934bee48fd8c43ab6bb68ddf56",
       "toolitems": [
        [
         "Home",
         "Reset original view",
         "home",
         "home"
        ],
        [
         "Back",
         "Back to previous view",
         "arrow-left",
         "back"
        ],
        [
         "Forward",
         "Forward to next view",
         "arrow-right",
         "forward"
        ],
        [
         "Pan",
         "Pan axes with left mouse, zoom with right",
         "arrows",
         "pan"
        ],
        [
         "Zoom",
         "Zoom to rectangle",
         "square-o",
         "zoom"
        ],
        [
         "Download",
         "Download plot",
         "floppy-o",
         "save_figure"
        ]
       ]
      }
     },
     "601c3bf911334eeb8c047cfe7ec73103": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "60e424fa893440de843a7e0517179a9e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "6696d786f82a4c42b63b5b541a82bdf0": {
      "model_module": "jupyter-matplotlib",
      "model_module_version": "^0.5.0",
      "model_name": "ToolbarModel",
      "state": {
       "layout": "IPY_MODEL_17ab28624c4e4b0da91b3f25f787e5fe",
       "toolitems": [
        [
         "Home",
         "Reset original view",
         "home",
         "home"
        ],
        [
         "Back",
         "Back to previous view",
         "arrow-left",
         "back"
        ],
        [
         "Forward",
         "Forward to next view",
         "arrow-right",
         "forward"
        ],
        [
         "Pan",
         "Pan axes with left mouse, zoom with right",
         "arrows",
         "pan"
        ],
        [
         "Zoom",
         "Zoom to rectangle",
         "square-o",
         "zoom"
        ],
        [
         "Download",
         "Download plot",
         "floppy-o",
         "save_figure"
        ]
       ]
      }
     },
     "68c5044e6aff445292424095e371b58e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "692ca4ec4c59423e961404b8a8c2bd4d": {
      "model_module": "jupyter-matplotlib",
      "model_module_version": "^0.5.0",
      "model_name": "MPLCanvasModel",
      "state": {
       "layout": "IPY_MODEL_dc2719cd078f464ca3cdd3a1e2826838",
       "toolbar": "IPY_MODEL_73eb85e9184847e98bda246ff36f11ad",
       "toolbar_position": "left"
      }
     },
     "69927a38ee26444abf62f0c045c7beba": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "6e16719391d64827b615f154a739f9ac": {
      "model_module": "jupyter-matplotlib",
      "model_module_version": "^0.5.0",
      "model_name": "MPLCanvasModel",
      "state": {
       "layout": "IPY_MODEL_2dcdade1a1d8446689bc6e424bf0fe3d",
       "toolbar": "IPY_MODEL_75dad3450ff842aab3e7ef90fefffe32",
       "toolbar_position": "left"
      }
     },
     "6e66120c889647d6bc5247790785a0f2": {
      "model_module": "jupyter-matplotlib",
      "model_module_version": "^0.5.0",
      "model_name": "MPLCanvasModel",
      "state": {
       "layout": "IPY_MODEL_13d8b797d0af4eb3a13b1bba4cf6f0c9",
       "toolbar": "IPY_MODEL_d4b9558f68074559b6822edd3531f3c4",
       "toolbar_position": "left"
      }
     },
     "6fae64031db04d00afbf24767ec6892a": {
      "model_module": "jupyter-matplotlib",
      "model_module_version": "^0.5.0",
      "model_name": "MPLCanvasModel",
      "state": {
       "layout": "IPY_MODEL_b73e626f4a9a4e6a9087680b167506bb",
       "toolbar": "IPY_MODEL_51b43c374cfc47d69f400e2b80e428fb",
       "toolbar_position": "left"
      }
     },
     "70ed30bfc68f4432b932a389ebcecc21": {
      "model_module": "jupyter-matplotlib",
      "model_module_version": "^0.5.0",
      "model_name": "MPLCanvasModel",
      "state": {
       "layout": "IPY_MODEL_9a5e0cfbbcde40d9bd26ec99b753bd24",
       "toolbar": "IPY_MODEL_5e571f77fdc54a0fa964a850c5634bf5",
       "toolbar_position": "left"
      }
     },
     "725a9ad986b7483497849e9357de75b2": {
      "model_module": "jupyter-matplotlib",
      "model_module_version": "^0.5.0",
      "model_name": "ToolbarModel",
      "state": {
       "layout": "IPY_MODEL_53deae4a7308486fb3d0a7e474c31bde",
       "toolitems": [
        [
         "Home",
         "Reset original view",
         "home",
         "home"
        ],
        [
         "Back",
         "Back to previous view",
         "arrow-left",
         "back"
        ],
        [
         "Forward",
         "Forward to next view",
         "arrow-right",
         "forward"
        ],
        [
         "Pan",
         "Pan axes with left mouse, zoom with right",
         "arrows",
         "pan"
        ],
        [
         "Zoom",
         "Zoom to rectangle",
         "square-o",
         "zoom"
        ],
        [
         "Download",
         "Download plot",
         "floppy-o",
         "save_figure"
        ]
       ]
      }
     },
     "73eb85e9184847e98bda246ff36f11ad": {
      "model_module": "jupyter-matplotlib",
      "model_module_version": "^0.5.0",
      "model_name": "ToolbarModel",
      "state": {
       "layout": "IPY_MODEL_109fb7767e484a908ed5c73066faf8ec",
       "toolitems": [
        [
         "Home",
         "Reset original view",
         "home",
         "home"
        ],
        [
         "Back",
         "Back to previous view",
         "arrow-left",
         "back"
        ],
        [
         "Forward",
         "Forward to next view",
         "arrow-right",
         "forward"
        ],
        [
         "Pan",
         "Pan axes with left mouse, zoom with right",
         "arrows",
         "pan"
        ],
        [
         "Zoom",
         "Zoom to rectangle",
         "square-o",
         "zoom"
        ],
        [
         "Download",
         "Download plot",
         "floppy-o",
         "save_figure"
        ]
       ]
      }
     },
     "75dad3450ff842aab3e7ef90fefffe32": {
      "model_module": "jupyter-matplotlib",
      "model_module_version": "^0.5.0",
      "model_name": "ToolbarModel",
      "state": {
       "layout": "IPY_MODEL_b536763162fe41c9bbdcd2233d7302d3",
       "toolitems": [
        [
         "Home",
         "Reset original view",
         "home",
         "home"
        ],
        [
         "Back",
         "Back to previous view",
         "arrow-left",
         "back"
        ],
        [
         "Forward",
         "Forward to next view",
         "arrow-right",
         "forward"
        ],
        [
         "Pan",
         "Pan axes with left mouse, zoom with right",
         "arrows",
         "pan"
        ],
        [
         "Zoom",
         "Zoom to rectangle",
         "square-o",
         "zoom"
        ],
        [
         "Download",
         "Download plot",
         "floppy-o",
         "save_figure"
        ]
       ]
      }
     },
     "77dc76185358407cbc5e472124f72140": {
      "model_module": "jupyter-matplotlib",
      "model_module_version": "^0.5.0",
      "model_name": "ToolbarModel",
      "state": {
       "layout": "IPY_MODEL_c1d61b0ecec84b549f23b2d37ff066fe",
       "toolitems": [
        [
         "Home",
         "Reset original view",
         "home",
         "home"
        ],
        [
         "Back",
         "Back to previous view",
         "arrow-left",
         "back"
        ],
        [
         "Forward",
         "Forward to next view",
         "arrow-right",
         "forward"
        ],
        [
         "Pan",
         "Pan axes with left mouse, zoom with right",
         "arrows",
         "pan"
        ],
        [
         "Zoom",
         "Zoom to rectangle",
         "square-o",
         "zoom"
        ],
        [
         "Download",
         "Download plot",
         "floppy-o",
         "save_figure"
        ]
       ]
      }
     },
     "7b38cc9d78fc4f159efc08b6e160a0d7": {
      "model_module": "jupyter-matplotlib",
      "model_module_version": "^0.5.0",
      "model_name": "ToolbarModel",
      "state": {
       "layout": "IPY_MODEL_8ccad5a8d9a9422e84a8207964c5ec36",
       "toolitems": [
        [
         "Home",
         "Reset original view",
         "home",
         "home"
        ],
        [
         "Back",
         "Back to previous view",
         "arrow-left",
         "back"
        ],
        [
         "Forward",
         "Forward to next view",
         "arrow-right",
         "forward"
        ],
        [
         "Pan",
         "Pan axes with left mouse, zoom with right",
         "arrows",
         "pan"
        ],
        [
         "Zoom",
         "Zoom to rectangle",
         "square-o",
         "zoom"
        ],
        [
         "Download",
         "Download plot",
         "floppy-o",
         "save_figure"
        ]
       ]
      }
     },
     "7d20af643b784e059d95876edab61a1c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "7d93309648774111bd06dc08e4ed9f64": {
      "model_module": "jupyter-matplotlib",
      "model_module_version": "^0.5.0",
      "model_name": "MPLCanvasModel",
      "state": {
       "layout": "IPY_MODEL_d3e3168360e340698e377825c7e7ede0",
       "toolbar": "IPY_MODEL_725a9ad986b7483497849e9357de75b2",
       "toolbar_position": "left"
      }
     },
     "80a5f2d79a7e474d9c039713454ce06d": {
      "model_module": "jupyter-matplotlib",
      "model_module_version": "^0.5.0",
      "model_name": "MPLCanvasModel",
      "state": {
       "layout": "IPY_MODEL_115ba6b1b07a444ba1c4f1839d73f34a",
       "toolbar": "IPY_MODEL_ec34d84a51844f4e82a1e931f47e4853",
       "toolbar_position": "left"
      }
     },
     "8bd0d5cca841433295647e4a0603c75f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "8ccad5a8d9a9422e84a8207964c5ec36": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "90ba96e6ff3f4117b7af1c76d9eaa0b3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "953a23821b874732931c588e0cedfcc6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "96158488b9784f50b26c571465d99a64": {
      "model_module": "jupyter-matplotlib",
      "model_module_version": "^0.5.0",
      "model_name": "MPLCanvasModel",
      "state": {
       "layout": "IPY_MODEL_90ba96e6ff3f4117b7af1c76d9eaa0b3",
       "toolbar": "IPY_MODEL_dfbfc7d699494e998cb6efe0d34aa879",
       "toolbar_position": "left"
      }
     },
     "966fd6dfbdab41e69d3d2c0efae4dbb6": {
      "model_module": "jupyter-matplotlib",
      "model_module_version": "^0.5.0",
      "model_name": "ToolbarModel",
      "state": {
       "layout": "IPY_MODEL_f3ac5644352e42809cd3ab174cb1377a",
       "toolitems": [
        [
         "Home",
         "Reset original view",
         "home",
         "home"
        ],
        [
         "Back",
         "Back to previous view",
         "arrow-left",
         "back"
        ],
        [
         "Forward",
         "Forward to next view",
         "arrow-right",
         "forward"
        ],
        [
         "Pan",
         "Pan axes with left mouse, zoom with right",
         "arrows",
         "pan"
        ],
        [
         "Zoom",
         "Zoom to rectangle",
         "square-o",
         "zoom"
        ],
        [
         "Download",
         "Download plot",
         "floppy-o",
         "save_figure"
        ]
       ]
      }
     },
     "9919357da0ba45ae964d865f490018c5": {
      "model_module": "jupyter-matplotlib",
      "model_module_version": "^0.5.0",
      "model_name": "ToolbarModel",
      "state": {
       "layout": "IPY_MODEL_b02cfca608a94e04be81dcee5feb80eb",
       "toolitems": [
        [
         "Home",
         "Reset original view",
         "home",
         "home"
        ],
        [
         "Back",
         "Back to previous view",
         "arrow-left",
         "back"
        ],
        [
         "Forward",
         "Forward to next view",
         "arrow-right",
         "forward"
        ],
        [
         "Pan",
         "Pan axes with left mouse, zoom with right",
         "arrows",
         "pan"
        ],
        [
         "Zoom",
         "Zoom to rectangle",
         "square-o",
         "zoom"
        ],
        [
         "Download",
         "Download plot",
         "floppy-o",
         "save_figure"
        ]
       ]
      }
     },
     "9a5e0cfbbcde40d9bd26ec99b753bd24": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "9b375a84324a4482a8e05d4469e11674": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "9dcc44c54b7744a4a388d4e0acc6becd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "9ea991266d2346668eae8c0909934dfd": {
      "model_module": "jupyter-matplotlib",
      "model_module_version": "^0.5.0",
      "model_name": "ToolbarModel",
      "state": {
       "layout": "IPY_MODEL_f49c454e36e54d7f97f15399153052c2",
       "toolitems": [
        [
         "Home",
         "Reset original view",
         "home",
         "home"
        ],
        [
         "Back",
         "Back to previous view",
         "arrow-left",
         "back"
        ],
        [
         "Forward",
         "Forward to next view",
         "arrow-right",
         "forward"
        ],
        [
         "Pan",
         "Pan axes with left mouse, zoom with right",
         "arrows",
         "pan"
        ],
        [
         "Zoom",
         "Zoom to rectangle",
         "square-o",
         "zoom"
        ],
        [
         "Download",
         "Download plot",
         "floppy-o",
         "save_figure"
        ]
       ]
      }
     },
     "9ec7e08c544744dfa01b72cb740e71ed": {
      "model_module": "jupyter-matplotlib",
      "model_module_version": "^0.5.0",
      "model_name": "ToolbarModel",
      "state": {
       "layout": "IPY_MODEL_f4b4305e107a4ef1a170f6dac71347c5",
       "toolitems": [
        [
         "Home",
         "Reset original view",
         "home",
         "home"
        ],
        [
         "Back",
         "Back to previous view",
         "arrow-left",
         "back"
        ],
        [
         "Forward",
         "Forward to next view",
         "arrow-right",
         "forward"
        ],
        [
         "Pan",
         "Pan axes with left mouse, zoom with right",
         "arrows",
         "pan"
        ],
        [
         "Zoom",
         "Zoom to rectangle",
         "square-o",
         "zoom"
        ],
        [
         "Download",
         "Download plot",
         "floppy-o",
         "save_figure"
        ]
       ]
      }
     },
     "a2fe98777d4d425e963070715adba992": {
      "model_module": "jupyter-matplotlib",
      "model_module_version": "^0.5.0",
      "model_name": "MPLCanvasModel",
      "state": {
       "layout": "IPY_MODEL_69927a38ee26444abf62f0c045c7beba",
       "toolbar": "IPY_MODEL_e5f4c72f86da44ef900fbfa4fbf06dda",
       "toolbar_position": "left"
      }
     },
     "a555a05885214b6589d545508d3f3b33": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "a7e844952f2c4d209c0719a0e1bbe913": {
      "model_module": "jupyter-matplotlib",
      "model_module_version": "^0.5.0",
      "model_name": "ToolbarModel",
      "state": {
       "layout": "IPY_MODEL_1cd815e92d05406a92d54b96f4535d13",
       "toolitems": [
        [
         "Home",
         "Reset original view",
         "home",
         "home"
        ],
        [
         "Back",
         "Back to previous view",
         "arrow-left",
         "back"
        ],
        [
         "Forward",
         "Forward to next view",
         "arrow-right",
         "forward"
        ],
        [
         "Pan",
         "Pan axes with left mouse, zoom with right",
         "arrows",
         "pan"
        ],
        [
         "Zoom",
         "Zoom to rectangle",
         "square-o",
         "zoom"
        ],
        [
         "Download",
         "Download plot",
         "floppy-o",
         "save_figure"
        ]
       ]
      }
     },
     "b02cfca608a94e04be81dcee5feb80eb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "b536763162fe41c9bbdcd2233d7302d3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "b73e626f4a9a4e6a9087680b167506bb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "bd19952207ab43b4af6e8ce354574bc1": {
      "model_module": "jupyter-matplotlib",
      "model_module_version": "^0.5.0",
      "model_name": "MPLCanvasModel",
      "state": {
       "layout": "IPY_MODEL_4716718d83ed4be5ab3f6cdffc625665",
       "toolbar": "IPY_MODEL_9919357da0ba45ae964d865f490018c5",
       "toolbar_position": "left"
      }
     },
     "bd5a951110f54175a7ae0de94e135702": {
      "model_module": "jupyter-matplotlib",
      "model_module_version": "^0.5.0",
      "model_name": "ToolbarModel",
      "state": {
       "layout": "IPY_MODEL_cfd6ad63ff9f4ed8ad016ab54613dbf2",
       "toolitems": [
        [
         "Home",
         "Reset original view",
         "home",
         "home"
        ],
        [
         "Back",
         "Back to previous view",
         "arrow-left",
         "back"
        ],
        [
         "Forward",
         "Forward to next view",
         "arrow-right",
         "forward"
        ],
        [
         "Pan",
         "Pan axes with left mouse, zoom with right",
         "arrows",
         "pan"
        ],
        [
         "Zoom",
         "Zoom to rectangle",
         "square-o",
         "zoom"
        ],
        [
         "Download",
         "Download plot",
         "floppy-o",
         "save_figure"
        ]
       ]
      }
     },
     "bd6b3734b6524333868e02cd35bb8d93": {
      "model_module": "jupyter-matplotlib",
      "model_module_version": "^0.5.0",
      "model_name": "ToolbarModel",
      "state": {
       "layout": "IPY_MODEL_7d20af643b784e059d95876edab61a1c",
       "toolitems": [
        [
         "Home",
         "Reset original view",
         "home",
         "home"
        ],
        [
         "Back",
         "Back to previous view",
         "arrow-left",
         "back"
        ],
        [
         "Forward",
         "Forward to next view",
         "arrow-right",
         "forward"
        ],
        [
         "Pan",
         "Pan axes with left mouse, zoom with right",
         "arrows",
         "pan"
        ],
        [
         "Zoom",
         "Zoom to rectangle",
         "square-o",
         "zoom"
        ],
        [
         "Download",
         "Download plot",
         "floppy-o",
         "save_figure"
        ]
       ]
      }
     },
     "bf2c763f93044077809e040ff3b23225": {
      "model_module": "jupyter-matplotlib",
      "model_module_version": "^0.5.0",
      "model_name": "MPLCanvasModel",
      "state": {
       "layout": "IPY_MODEL_60e424fa893440de843a7e0517179a9e",
       "toolbar": "IPY_MODEL_25e2c150de6d4b658895776db6bad3b8",
       "toolbar_position": "left"
      }
     },
     "c0b66279aa7e40f5ba4c3801ccd473f8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "c1d61b0ecec84b549f23b2d37ff066fe": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "c3ef843866744d0fa894b321771a831e": {
      "model_module": "jupyter-matplotlib",
      "model_module_version": "^0.5.0",
      "model_name": "MPLCanvasModel",
      "state": {
       "layout": "IPY_MODEL_f1b4363b1b044aa7acee0200609be2dd",
       "toolbar": "IPY_MODEL_9ec7e08c544744dfa01b72cb740e71ed",
       "toolbar_position": "left"
      }
     },
     "c5009225f1bd4134a4455b61d85f034d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "c72cae9f073e4fcd8e7d30fd30f2b11a": {
      "model_module": "jupyter-matplotlib",
      "model_module_version": "^0.5.0",
      "model_name": "ToolbarModel",
      "state": {
       "layout": "IPY_MODEL_c0b66279aa7e40f5ba4c3801ccd473f8",
       "toolitems": [
        [
         "Home",
         "Reset original view",
         "home",
         "home"
        ],
        [
         "Back",
         "Back to previous view",
         "arrow-left",
         "back"
        ],
        [
         "Forward",
         "Forward to next view",
         "arrow-right",
         "forward"
        ],
        [
         "Pan",
         "Pan axes with left mouse, zoom with right",
         "arrows",
         "pan"
        ],
        [
         "Zoom",
         "Zoom to rectangle",
         "square-o",
         "zoom"
        ],
        [
         "Download",
         "Download plot",
         "floppy-o",
         "save_figure"
        ]
       ]
      }
     },
     "c735a345fde54edba6763da1d091001e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "ceb9779498af46f0a13afe11dfe5ee13": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "cfd6ad63ff9f4ed8ad016ab54613dbf2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "d206e922a04b4f2abf147bf9f0d7c444": {
      "model_module": "jupyter-matplotlib",
      "model_module_version": "^0.5.0",
      "model_name": "ToolbarModel",
      "state": {
       "layout": "IPY_MODEL_c5009225f1bd4134a4455b61d85f034d",
       "toolitems": [
        [
         "Home",
         "Reset original view",
         "home",
         "home"
        ],
        [
         "Back",
         "Back to previous view",
         "arrow-left",
         "back"
        ],
        [
         "Forward",
         "Forward to next view",
         "arrow-right",
         "forward"
        ],
        [
         "Pan",
         "Pan axes with left mouse, zoom with right",
         "arrows",
         "pan"
        ],
        [
         "Zoom",
         "Zoom to rectangle",
         "square-o",
         "zoom"
        ],
        [
         "Download",
         "Download plot",
         "floppy-o",
         "save_figure"
        ]
       ]
      }
     },
     "d35972e8d6a849d0a0aaf925c0f170a8": {
      "model_module": "jupyter-matplotlib",
      "model_module_version": "^0.5.0",
      "model_name": "MPLCanvasModel",
      "state": {
       "layout": "IPY_MODEL_02d4649be9b7445cace4ef9a8fc22a4c",
       "toolbar": "IPY_MODEL_77dc76185358407cbc5e472124f72140",
       "toolbar_position": "left"
      }
     },
     "d3e3168360e340698e377825c7e7ede0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "d4b9558f68074559b6822edd3531f3c4": {
      "model_module": "jupyter-matplotlib",
      "model_module_version": "^0.5.0",
      "model_name": "ToolbarModel",
      "state": {
       "layout": "IPY_MODEL_5810c4449bb446a9991b523ad05a44fd",
       "toolitems": [
        [
         "Home",
         "Reset original view",
         "home",
         "home"
        ],
        [
         "Back",
         "Back to previous view",
         "arrow-left",
         "back"
        ],
        [
         "Forward",
         "Forward to next view",
         "arrow-right",
         "forward"
        ],
        [
         "Pan",
         "Pan axes with left mouse, zoom with right",
         "arrows",
         "pan"
        ],
        [
         "Zoom",
         "Zoom to rectangle",
         "square-o",
         "zoom"
        ],
        [
         "Download",
         "Download plot",
         "floppy-o",
         "save_figure"
        ]
       ]
      }
     },
     "dc2719cd078f464ca3cdd3a1e2826838": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "dfa80b869bd74e80b5d3f6058117a0ac": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "dfbfc7d699494e998cb6efe0d34aa879": {
      "model_module": "jupyter-matplotlib",
      "model_module_version": "^0.5.0",
      "model_name": "ToolbarModel",
      "state": {
       "layout": "IPY_MODEL_51456f9b1c134aaa8daea5f2225b96f9",
       "toolitems": [
        [
         "Home",
         "Reset original view",
         "home",
         "home"
        ],
        [
         "Back",
         "Back to previous view",
         "arrow-left",
         "back"
        ],
        [
         "Forward",
         "Forward to next view",
         "arrow-right",
         "forward"
        ],
        [
         "Pan",
         "Pan axes with left mouse, zoom with right",
         "arrows",
         "pan"
        ],
        [
         "Zoom",
         "Zoom to rectangle",
         "square-o",
         "zoom"
        ],
        [
         "Download",
         "Download plot",
         "floppy-o",
         "save_figure"
        ]
       ]
      }
     },
     "e140196faaf7478f938949d1d32f8f12": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "e1e945efa73144dda16d469d5cdd6815": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "e5f4c72f86da44ef900fbfa4fbf06dda": {
      "model_module": "jupyter-matplotlib",
      "model_module_version": "^0.5.0",
      "model_name": "ToolbarModel",
      "state": {
       "layout": "IPY_MODEL_08e32dfb96664b88863ed24c0847124a",
       "toolitems": [
        [
         "Home",
         "Reset original view",
         "home",
         "home"
        ],
        [
         "Back",
         "Back to previous view",
         "arrow-left",
         "back"
        ],
        [
         "Forward",
         "Forward to next view",
         "arrow-right",
         "forward"
        ],
        [
         "Pan",
         "Pan axes with left mouse, zoom with right",
         "arrows",
         "pan"
        ],
        [
         "Zoom",
         "Zoom to rectangle",
         "square-o",
         "zoom"
        ],
        [
         "Download",
         "Download plot",
         "floppy-o",
         "save_figure"
        ]
       ]
      }
     },
     "e9200f6608c34f349258b5b11723b5a6": {
      "model_module": "jupyter-matplotlib",
      "model_module_version": "^0.5.0",
      "model_name": "MPLCanvasModel",
      "state": {
       "layout": "IPY_MODEL_f8e09e2cab834588bcef1b84c023a9fc",
       "toolbar": "IPY_MODEL_d206e922a04b4f2abf147bf9f0d7c444",
       "toolbar_position": "left"
      }
     },
     "ea6a1688cc254d40b254e4b5af03f212": {
      "model_module": "jupyter-matplotlib",
      "model_module_version": "^0.5.0",
      "model_name": "MPLCanvasModel",
      "state": {
       "layout": "IPY_MODEL_20b7b6ac7a5d4d7588cd2567464f9a5a",
       "toolbar": "IPY_MODEL_5952e271a86b474aa50f3002c2f2d0e0",
       "toolbar_position": "left"
      }
     },
     "eb472f132d274dbaae9e15f1f5388142": {
      "model_module": "jupyter-matplotlib",
      "model_module_version": "^0.5.0",
      "model_name": "ToolbarModel",
      "state": {
       "layout": "IPY_MODEL_4373bf06ab9348ee9558615af323881c",
       "toolitems": [
        [
         "Home",
         "Reset original view",
         "home",
         "home"
        ],
        [
         "Back",
         "Back to previous view",
         "arrow-left",
         "back"
        ],
        [
         "Forward",
         "Forward to next view",
         "arrow-right",
         "forward"
        ],
        [
         "Pan",
         "Pan axes with left mouse, zoom with right",
         "arrows",
         "pan"
        ],
        [
         "Zoom",
         "Zoom to rectangle",
         "square-o",
         "zoom"
        ],
        [
         "Download",
         "Download plot",
         "floppy-o",
         "save_figure"
        ]
       ]
      }
     },
     "ec34d84a51844f4e82a1e931f47e4853": {
      "model_module": "jupyter-matplotlib",
      "model_module_version": "^0.5.0",
      "model_name": "ToolbarModel",
      "state": {
       "layout": "IPY_MODEL_e1e945efa73144dda16d469d5cdd6815",
       "toolitems": [
        [
         "Home",
         "Reset original view",
         "home",
         "home"
        ],
        [
         "Back",
         "Back to previous view",
         "arrow-left",
         "back"
        ],
        [
         "Forward",
         "Forward to next view",
         "arrow-right",
         "forward"
        ],
        [
         "Pan",
         "Pan axes with left mouse, zoom with right",
         "arrows",
         "pan"
        ],
        [
         "Zoom",
         "Zoom to rectangle",
         "square-o",
         "zoom"
        ],
        [
         "Download",
         "Download plot",
         "floppy-o",
         "save_figure"
        ]
       ]
      }
     },
     "f1b4363b1b044aa7acee0200609be2dd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "f3ac5644352e42809cd3ab174cb1377a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "f49c454e36e54d7f97f15399153052c2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "f4b4305e107a4ef1a170f6dac71347c5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "f8e09e2cab834588bcef1b84c023a9fc": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "fe759346f4fa40d59150c52a58bb0bb8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
